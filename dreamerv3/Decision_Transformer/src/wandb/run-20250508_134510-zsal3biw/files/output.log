Class Weights: [1.08333333 0.86666667 1.08333333]
âœ… Will update: action_embedding.0.weight
âœ… Will update: time_embedding.weight
âœ… Will update: state_embedding.weight
âœ… Will update: transformer.pos_embed.W_pos
âœ… Will update: transformer.blocks.0.attn.W_Q
âœ… Will update: transformer.blocks.0.attn.W_K
âœ… Will update: transformer.blocks.0.attn.W_V
âœ… Will update: transformer.blocks.0.attn.W_O
âœ… Will update: transformer.blocks.0.attn.b_Q
âœ… Will update: transformer.blocks.0.attn.b_K
âœ… Will update: transformer.blocks.0.attn.b_V
âœ… Will update: transformer.blocks.0.attn.b_O
âœ… Will update: transformer.blocks.0.mlp.W_in
âœ… Will update: transformer.blocks.0.mlp.b_in
âœ… Will update: transformer.blocks.0.mlp.W_out
âœ… Will update: transformer.blocks.0.mlp.b_out
âœ… Will update: transformer.blocks.1.attn.W_Q
âœ… Will update: transformer.blocks.1.attn.W_K
âœ… Will update: transformer.blocks.1.attn.W_V
âœ… Will update: transformer.blocks.1.attn.W_O
âœ… Will update: transformer.blocks.1.attn.b_Q
âœ… Will update: transformer.blocks.1.attn.b_K
âœ… Will update: transformer.blocks.1.attn.b_V
âœ… Will update: transformer.blocks.1.attn.b_O
âœ… Will update: transformer.blocks.1.mlp.W_in
âœ… Will update: transformer.blocks.1.mlp.b_in
âœ… Will update: transformer.blocks.1.mlp.W_out
âœ… Will update: transformer.blocks.1.mlp.b_out
âœ… Will update: action_predictor.weight
âœ… Will update: action_predictor.bias
âœ… Will update: state_predictor.weight
âœ… Will update: state_predictor.bias
âœ… Will update: reward_embedding.0.weight
âœ… Will update: reward_predictor.weight
âœ… Will update: reward_predictor.bias
âœ… Will update: penultimate_layer.0.weight
âœ… Will update: penultimate_layer.0.bias
âœ… Will update: output_layer.0.weight
âœ… Will update: output_layer.0.bias
===== íƒœìŠ¤í¬ë³„ ë°ì´í„°ì…‹ í¬ê¸° =====
Task task_0: 133 ìƒ˜í”Œ
Task task_1: 165 ìƒ˜í”Œ
Task task_2: 131 ìƒ˜í”Œ
===== ì›ë³¸ ë°ì´í„°ì…‹ íƒœìŠ¤í¬ë³„ ë¶„í¬ =====
ì „ì²´ ë°ì´í„° ìˆ˜: 429
Task 0: 133 ìƒ˜í”Œ (31.00%)
Task 1: 165 ìƒ˜í”Œ (38.46%)
Task 2: 131 ìƒ˜í”Œ (30.54%)
===== í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (íƒœìŠ¤í¬ ê· í˜• ìœ ì§€) =====
í•™ìŠµ ë°ì´í„°: 299 ìƒ˜í”Œ
í…ŒìŠ¤íŠ¸ ë°ì´í„°: 130 ìƒ˜í”Œ
===== ì²« ë°°ì¹˜ì—ì„œì˜ íƒœìŠ¤í¬ ë¶„í¬ í™•ì¸ =====
Task 0: 42 ìƒ˜í”Œ (32.81%)
Task 1: 44 ìƒ˜í”Œ (34.38%)
Task 2: 42 ìƒ˜í”Œ (32.81%)
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
Training DT: 0.0001:   0%|          | 0/100 [00:01<?, ?it/s]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
Training DT: 0.0001:   0%|          | 0/100 [00:02<?, ?it/s]/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning:
[33mWARN: Overwriting existing videos at /home/hail/DT/src/videos/dt_eval_videos_0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)
Evaluating DT:   0%|          | 0/10 [00:00<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [41, 41, 41, 41, 41, 41, 41, 41] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:02<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [87, 87, 87, 87, 87, 87, 87, 87] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:05<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [133, 133, 133, 133, 133, 133, 133, 133] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:07<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [179, 179, 179, 179, 179, 179, 179, 179] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:09<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [200, 200, 200, 200, 200, 200, 200, 200] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:10<?, ?it/s]
Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [8, 8, 8, 8, 8, 8, 8, 8] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|â–ˆ         | 1/10 [00:11<01:32, 10.33s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [59, 59, 59, 59, 59, 59, 59, 59] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|â–ˆ         | 1/10 [00:13<01:32, 10.33s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [124, 124, 124, 124, 124, 124, 124, 124] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|â–ˆ         | 1/10 [00:15<01:32, 10.33s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [191, 191, 191, 191, 191, 191, 191, 191] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|â–ˆ         | 1/10 [00:17<01:32, 10.33s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:   1%|          | 1/100 [00:21<33:36, 20.37s/it]timestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:   2%|â–         | 2/100 [00:22<15:56,  9.76s/it]timestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:   3%|â–         | 3/100 [00:25<10:17,  6.37s/it]timestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])


Training DT: 0.0001:   5%|â–Œ         | 5/100 [00:29<06:08,  3.88s/it]timestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:   6%|â–Œ         | 6/100 [00:31<05:14,  3.35s/it]timestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:   6%|â–Œ         | 6/100 [00:33<05:14,  3.35s/it]timestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:   7%|â–‹         | 7/100 [00:35<04:39,  3.01s/it]timestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])

Training DT: 0.0001:   8%|â–Š         | 8/100 [00:37<04:16,  2.79s/it]timestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:   9%|â–‰         | 9/100 [00:40<04:01,  2.65s/it]timestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  10%|â–ˆ         | 10/100 [00:41<03:51,  2.57s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  11%|â–ˆ         | 11/100 [00:43<03:41,  2.49s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  12%|â–ˆâ–        | 12/100 [00:45<03:34,  2.44s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  12%|â–ˆâ–        | 12/100 [00:47<03:34,  2.44s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  13%|â–ˆâ–        | 13/100 [00:49<03:28,  2.40s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  14%|â–ˆâ–        | 14/100 [00:51<03:24,  2.38s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  15%|â–ˆâ–Œ        | 15/100 [00:54<03:20,  2.36s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  16%|â–ˆâ–Œ        | 16/100 [00:55<03:17,  2.35s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  17%|â–ˆâ–‹        | 17/100 [00:57<03:14,  2.34s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  18%|â–ˆâ–Š        | 18/100 [00:59<03:10,  2.33s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  19%|â–ˆâ–‰        | 19/100 [01:02<03:08,  2.32s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])


Training DT: 0.0001:  20%|â–ˆâ–ˆ        | 20/100 [01:05<03:05,  2.32s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  21%|â–ˆâ–ˆ        | 21/100 [01:07<03:03,  2.32s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  22%|â–ˆâ–ˆâ–       | 22/100 [01:10<03:00,  2.32s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])


Training DT: 0.0001:  24%|â–ˆâ–ˆâ–       | 24/100 [01:13<02:56,  2.32s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [01:16<02:53,  2.32s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])


Training DT: 0.0001:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [01:19<02:51,  2.32s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])

state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  28%|â–ˆâ–ˆâ–Š       | 28/100 [01:24<02:47,  2.32s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])


Training DT: 0.0001:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [01:27<02:42,  2.31s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [01:30<02:40,  2.32s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [01:32<02:37,  2.32s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])


Training DT: 0.0001:  33%|â–ˆâ–ˆâ–ˆâ–      | 33/100 [01:35<02:35,  2.32s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [01:38<02:35,  2.36s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [01:39<02:35,  2.40s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [01:42<02:35,  2.44s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [01:43<02:35,  2.44s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [01:45<02:31,  2.40s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [01:47<02:27,  2.38s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [01:50<02:23,  2.36s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [01:51<02:20,  2.35s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [01:53<02:21,  2.40s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])


Training DT: 0.0001:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [01:57<02:18,  2.39s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/100 [01:59<02:15,  2.37s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [02:02<02:11,  2.35s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [02:04<02:09,  2.35s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [02:05<02:07,  2.35s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [02:08<02:07,  2.40s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [02:09<02:07,  2.40s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])


Training DT: 0.0001:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [02:14<02:05,  2.46s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [02:15<02:00,  2.41s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [02:17<01:56,  2.38s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [02:20<01:53,  2.36s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/100 [02:22<01:50,  2.34s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])


Training DT: 0.0000:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [02:26<01:44,  2.33s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0001:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [02:28<01:44,  2.33s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [02:30<01:41,  2.32s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [02:32<01:39,  2.31s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [02:33<01:37,  2.31s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])


Training DT: 0.0000:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [02:37<01:35,  2.34s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [02:40<01:41,  2.53s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [02:41<01:39,  2.56s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [02:44<01:39,  2.63s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])


Training DT: 0.0000:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/100 [02:48<01:41,  2.74s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [02:50<01:34,  2.63s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [02:52<01:29,  2.56s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [02:53<01:29,  2.56s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [02:56<01:30,  2.68s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [02:58<01:28,  2.69s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [03:00<01:26,  2.72s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [03:02<01:26,  2.72s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])


Training DT: 0.0000:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [03:06<01:19,  2.66s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [03:08<01:19,  2.73s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [03:10<01:19,  2.73s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [03:12<01:12,  2.61s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/100 [03:14<01:07,  2.52s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [03:17<01:03,  2.45s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])

Training DT: 0.0000:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [03:18<01:00,  2.41s/it]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.91s/it]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])


Class Weights: [1.08333333 0.86666667 1.08333333]
âœ… Will update: action_embedding.0.weight
âœ… Will update: time_embedding.weight
âœ… Will update: state_embedding.weight
âœ… Will update: transformer.pos_embed.W_pos
âœ… Will update: transformer.blocks.0.attn.W_Q
âœ… Will update: transformer.blocks.0.attn.W_K
âœ… Will update: transformer.blocks.0.attn.W_V
âœ… Will update: transformer.blocks.0.attn.W_O
âœ… Will update: transformer.blocks.0.attn.b_Q
âœ… Will update: transformer.blocks.0.attn.b_K
âœ… Will update: transformer.blocks.0.attn.b_V
âœ… Will update: transformer.blocks.0.attn.b_O
âœ… Will update: transformer.blocks.0.mlp.W_in
âœ… Will update: transformer.blocks.0.mlp.b_in
âœ… Will update: transformer.blocks.0.mlp.W_out
âœ… Will update: transformer.blocks.0.mlp.b_out
âœ… Will update: transformer.blocks.1.attn.W_Q
âœ… Will update: transformer.blocks.1.attn.W_K
âœ… Will update: transformer.blocks.1.attn.W_V
âœ… Will update: transformer.blocks.1.attn.W_O
âœ… Will update: transformer.blocks.1.attn.b_Q
âœ… Will update: transformer.blocks.1.attn.b_K
âœ… Will update: transformer.blocks.1.attn.b_V
âœ… Will update: transformer.blocks.1.attn.b_O
âœ… Will update: transformer.blocks.1.mlp.W_in
âœ… Will update: transformer.blocks.1.mlp.b_in
âœ… Will update: transformer.blocks.1.mlp.W_out
âœ… Will update: transformer.blocks.1.mlp.b_out
âœ… Will update: action_predictor.weight
âœ… Will update: action_predictor.bias
âœ… Will update: state_predictor.weight
âœ… Will update: state_predictor.bias
âœ… Will update: reward_embedding.0.weight
âœ… Will update: reward_predictor.weight
âœ… Will update: reward_predictor.bias
âœ… Will update: penultimate_layer.0.weight
âœ… Will update: penultimate_layer.0.bias
âœ… Will update: output_layer.0.weight
âœ… Will update: output_layer.0.bias
===== íƒœìŠ¤í¬ë³„ ë°ì´í„°ì…‹ í¬ê¸° =====
Task task_0: 133 ìƒ˜í”Œ
Task task_1: 165 ìƒ˜í”Œ
Task task_2: 131 ìƒ˜í”Œ
===== ì›ë³¸ ë°ì´í„°ì…‹ íƒœìŠ¤í¬ë³„ ë¶„í¬ =====
ì „ì²´ ë°ì´í„° ìˆ˜: 429
Task 0: 133 ìƒ˜í”Œ (31.00%)
Task 1: 165 ìƒ˜í”Œ (38.46%)
Task 2: 131 ìƒ˜í”Œ (30.54%)
===== í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (íƒœìŠ¤í¬ ê· í˜• ìœ ì§€) =====
í•™ìŠµ ë°ì´í„°: 299 ìƒ˜í”Œ
í…ŒìŠ¤íŠ¸ ë°ì´í„°: 130 ìƒ˜í”Œ
===== ì²« ë°°ì¹˜ì—ì„œì˜ íƒœìŠ¤í¬ ë¶„í¬ í™•ì¸ =====
Task 0: 44 ìƒ˜í”Œ (34.38%)
Task 1: 45 ìƒ˜í”Œ (35.16%)
Task 2: 39 ìƒ˜í”Œ (30.47%)
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
Training DT: 0.0002:   0%|          | 0/100 [00:00<?, ?it/s]/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning:
[33mWARN: Overwriting existing videos at /home/hail/DT/src/videos/dt_eval_videos_0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)
Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [12, 12, 12, 12, 12, 12, 12, 12] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:01<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [92, 92, 92, 92, 92, 92, 92, 92] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:03<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [171, 171, 171, 171, 171, 171, 171, 171] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:05<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [200, 200, 200, 200, 200, 200, 200, 200] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:06<?, ?it/s]
Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [56, 56, 56, 56, 56, 56, 56, 56] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|â–ˆ         | 1/10 [00:07<00:56,  6.25s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False


Training DT: 0.0001:  13%|â–ˆâ–        | 13/100 [00:11<00:18,  4.59it/s]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:09<00:01,  1.02it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])

Training DT: 0.0002:  24%|â–ˆâ–ˆâ–       | 24/100 [00:13<00:14,  5.34it/s]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:09<00:01,  1.02it/s]
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])


Training DT: 0.0001:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:18<00:09,  5.59it/s]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:09<00:01,  1.02it/s]
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])

Training DT: 0.0001:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:20<00:06,  5.88it/s]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:09<00:01,  1.02it/s]
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])

Training DT: 0.0001:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:22<00:04,  5.81it/s]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:09<00:01,  1.02it/s]
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])

Training DT: 0.0000:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:24<00:03,  5.68it/s]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:09<00:01,  1.02it/s]
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])

Training DT: 0.0001:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:26<00:01,  5.82it/s]imestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:09<00:01,  1.02it/s]
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
Training DT: 0.0001: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:27<00:00,  3.66it/s]mestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:09<00:01,  1.02it/s]
MLP Fine-Tuning: task_loss = 1.1063:  33%|â–ˆâ–ˆâ–ˆâ–      | 5/15 [00:00<00:01,  7.03it/s]
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
[correct] no_actions is False
r.shape: torch.Size([128, 10, 1])
reward_preds.shape: torch.Size([1280, 1])
s[1:].shape (GT): torch.Size([128, 10, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 11, 980])
r[1:].shape (GT): torch.Size([128, 9, 1])
reward_preds.shape (pred): torch.Size([1280, 1])
ğŸ”’ Freezing all layers except MLP (penultimate_layer, output_layer)
âŒ action_embedding.0.weight is frozen.
âŒ time_embedding.weight is frozen.
âŒ state_embedding.weight is frozen.
âŒ transformer.pos_embed.W_pos is frozen.
âŒ transformer.blocks.0.attn.W_Q is frozen.
âŒ transformer.blocks.0.attn.W_K is frozen.
âŒ transformer.blocks.0.attn.W_V is frozen.
âŒ transformer.blocks.0.attn.W_O is frozen.
âŒ transformer.blocks.0.attn.b_Q is frozen.
âŒ transformer.blocks.0.attn.b_K is frozen.
âŒ transformer.blocks.0.attn.b_V is frozen.
âŒ transformer.blocks.0.attn.b_O is frozen.
âŒ transformer.blocks.0.mlp.W_in is frozen.
âŒ transformer.blocks.0.mlp.b_in is frozen.
âŒ transformer.blocks.0.mlp.W_out is frozen.
âŒ transformer.blocks.0.mlp.b_out is frozen.
âŒ transformer.blocks.1.attn.W_Q is frozen.
âŒ transformer.blocks.1.attn.W_K is frozen.
âŒ transformer.blocks.1.attn.W_V is frozen.
âŒ transformer.blocks.1.attn.W_O is frozen.
âŒ transformer.blocks.1.attn.b_Q is frozen.
âŒ transformer.blocks.1.attn.b_K is frozen.
âŒ transformer.blocks.1.attn.b_V is frozen.
âŒ transformer.blocks.1.attn.b_O is frozen.
âŒ transformer.blocks.1.mlp.W_in is frozen.
âŒ transformer.blocks.1.mlp.b_in is frozen.
âŒ transformer.blocks.1.mlp.W_out is frozen.
âŒ transformer.blocks.1.mlp.b_out is frozen.
âŒ action_predictor.weight is frozen.
âŒ action_predictor.bias is frozen.
âŒ state_predictor.weight is frozen.
âŒ state_predictor.bias is frozen.
âŒ reward_embedding.0.weight is frozen.
âŒ reward_predictor.weight is frozen.
âŒ reward_predictor.bias is frozen.
âœ… penultimate_layer.0.weight will be updated.
âœ… penultimate_layer.0.bias will be updated.
âœ… output_layer.0.weight will be updated.
âœ… output_layer.0.bias will be updated.
[correct] no_actions is False
task_labels:  tensor([1, 2, 1, 1, 0, 0, 0, 0, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 1, 1, 1, 0, 1, 2,
        0, 2, 1, 0, 0, 1, 2, 2, 0, 1, 2, 2, 2, 0, 2, 0, 1, 1, 1, 1, 0, 1, 1, 2,
        2, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 2, 1, 1, 2, 0, 1, 2, 2,
        2, 0, 0, 1, 0, 2, 1, 0, 2, 0, 2, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 2,
        1, 0, 2, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 0,
        1, 2, 0, 1, 1, 1, 0, 1]) , task_preds: tensor([[-5.5020e-03, -4.8163e-03, -6.2590e-03],
        [ 1.6112e-02,  8.0895e-03, -1.3380e-02],
        [-1.0859e-02, -3.0981e-03, -5.1311e-03],
        [ 1.0528e-02, -1.2954e-03, -1.2631e-02],
        [-6.1607e-04,  1.2733e-02, -6.6365e-03],
        [-5.8398e-03, -7.0920e-03,  7.6542e-03],
        [ 8.0373e-03, -3.4306e-03, -6.1436e-03],
        [ 7.5455e-03, -4.7679e-03,  4.5080e-04],
        [-3.1555e-03, -8.6052e-05, -7.6905e-03],
        [ 2.3343e-04,  3.6125e-03, -4.3232e-03],
        [-2.7126e-03,  4.1815e-03,  1.4129e-03],
        [-3.2723e-03,  5.5670e-03, -3.9426e-03],
        [-7.9409e-04,  1.3579e-02, -8.1337e-03],
        [ 1.2453e-02,  1.6368e-02, -3.9496e-03],
        [ 1.0999e-02,  4.6804e-03, -7.0570e-03],
        [-3.5134e-03,  1.4360e-02,  5.3741e-03],
        [ 9.1529e-03,  1.3819e-03, -6.0118e-03],
        [-1.2781e-02,  1.0182e-04, -2.0226e-03],
        [ 6.0562e-03,  9.6301e-03,  1.2122e-02],
        [ 7.0173e-04, -6.4984e-03, -1.3359e-02],
        [ 1.5109e-02,  1.4744e-02, -7.8372e-03],
        [ 6.3383e-03, -4.9845e-03,  2.5970e-03],
        [-1.1371e-02,  6.9635e-04, -5.8668e-03],
        [ 5.1983e-03, -5.7371e-03, -1.8110e-02],
        [ 6.7981e-03, -2.1114e-03, -1.6184e-02],
        [-3.1748e-03, -1.5084e-03, -9.9232e-03],
        [-2.8347e-03,  1.0893e-02, -1.7495e-03],
        [ 4.2912e-03,  1.4934e-03,  6.5737e-03],
        [ 1.3121e-02,  1.2810e-02, -1.0838e-02],
        [-3.2692e-03,  3.1684e-03,  2.1436e-03],
        [ 4.2240e-04, -7.6761e-04, -8.8481e-03],
        [-1.3731e-03, -1.0647e-03, -1.0593e-02],
        [ 1.4752e-02,  9.3146e-03, -4.5244e-04],
        [-1.5267e-04,  7.2127e-03, -7.5413e-03],
        [-5.9097e-03, -1.3198e-03,  7.1983e-04],
        [ 9.8064e-04,  4.2125e-03,  1.0374e-03],
        [-7.4229e-03,  2.7462e-03,  4.0311e-03],
        [-2.7793e-03,  2.4104e-03, -1.9052e-03],
        [-1.3672e-02,  2.9910e-03, -4.0967e-03],
        [ 5.7634e-03,  6.9086e-03, -3.1953e-03],
        [-1.0478e-02, -6.2412e-03,  3.6487e-03],
        [ 7.1090e-03,  5.3015e-03, -1.6728e-02],
        [-6.2497e-04,  4.5495e-03,  7.5813e-03],
        [-6.1589e-03, -9.4248e-05, -8.3092e-03],
        [-5.1456e-03,  6.0869e-03, -4.6177e-03],
        [ 4.2831e-03,  4.3796e-03, -2.3046e-03],
        [-3.5867e-03,  5.4595e-03, -9.7423e-03],
        [ 5.3157e-03, -3.2606e-04, -1.5503e-03],
        [ 9.8686e-03,  2.4885e-03, -6.6752e-03],
        [ 1.9565e-03, -5.0831e-03, -1.5631e-02],
        [ 1.1088e-02,  1.2487e-02, -9.3320e-03],
        [-2.4777e-03, -1.3049e-02, -1.2210e-02],
        [-3.5582e-03, -2.7232e-04, -4.2326e-03],
        [-1.8337e-02, -1.8480e-06, -5.9407e-03],
        [ 6.2612e-03,  9.9913e-03,  7.2145e-04],
        [ 1.0639e-02,  9.8949e-03, -1.1820e-02],
        [-1.7041e-02,  1.5925e-02, -7.2850e-03],
        [ 1.1714e-03,  3.4853e-03,  7.5037e-03],
        [ 9.4487e-04,  4.3598e-03, -1.3104e-02],
        [-1.8374e-03,  5.9636e-03, -3.9767e-03],
        [ 3.7463e-03,  1.5092e-02, -1.0771e-02],
        [ 1.4783e-03, -3.5288e-03, -3.9641e-03],
        [-7.9594e-04, -4.3253e-03,  1.9610e-03],
        [ 6.4159e-03,  6.5730e-03,  6.6641e-04],
        [ 2.5838e-03,  1.6902e-03, -1.0442e-03],
        [ 2.7632e-03,  2.4545e-03, -6.3516e-04],
        [-1.3048e-04,  4.3001e-03, -1.8912e-03],
        [ 2.7723e-04,  6.0355e-03,  3.3436e-03],
        [ 5.0722e-03, -4.4935e-03,  1.8441e-03],
        [ 1.1013e-02,  1.1673e-02, -2.7658e-03],
        [ 2.9535e-03,  8.9085e-03, -1.1256e-02],
        [ 4.7219e-03,  9.9779e-03, -3.5315e-03],
        [-2.8441e-03,  4.5997e-03, -1.7690e-02],
        [-8.9524e-04,  9.7890e-03, -1.1049e-03],
        [-8.8699e-03,  3.8323e-03, -1.0435e-02],
        [-4.6747e-03,  3.1015e-03, -2.1969e-04],
        [ 3.5051e-03, -7.0926e-03, -1.4203e-02],
        [-1.4343e-02, -6.1908e-03, -9.3174e-03],
        [ 4.3091e-03,  6.0864e-03, -3.0708e-03],
        [ 6.2090e-03,  9.0549e-03, -2.7027e-03],
        [ 6.4034e-04,  6.7939e-03, -2.1111e-03],
        [-7.0569e-03,  1.0608e-02, -4.2024e-03],
        [-9.4416e-03,  1.8495e-03,  1.3515e-03],
        [-5.1867e-03,  2.7237e-03,  6.7165e-03],
        [ 4.6468e-03,  1.6209e-03, -1.0025e-02],
        [-3.0425e-03,  1.0056e-02, -4.1075e-03],
        [ 1.8196e-03,  1.1770e-02, -8.7794e-03],
        [-1.5980e-03,  5.3280e-03, -1.6461e-02],
        [-6.9218e-03,  1.8778e-04, -1.1995e-02],
        [-2.0169e-03,  1.6430e-03,  4.0408e-03],
        [-1.0903e-03, -4.3120e-04, -7.7194e-03],
        [ 5.9751e-03,  3.4501e-06, -9.3133e-03],
        [ 1.1896e-03,  2.0699e-03, -2.7827e-03],
        [ 1.1663e-02,  4.3737e-03,  1.8046e-03],
        [ 7.9977e-03,  1.3469e-04, -7.9366e-03],
        [ 1.8825e-02,  1.3420e-02, -1.4280e-02],
        [ 1.6756e-03,  6.0340e-03, -6.6105e-03],
        [ 2.0976e-03,  9.6349e-03, -3.2197e-03],
        [ 4.1491e-03,  8.9181e-03,  3.1705e-03],
        [ 8.1788e-03, -4.9420e-03, -1.4416e-03],
        [ 1.1012e-02,  2.7341e-03, -5.6065e-03],
        [-8.0740e-03,  1.5440e-02,  6.6249e-03],
        [ 8.9772e-03,  3.1806e-03, -1.0259e-02],
        [ 8.6994e-03,  1.0455e-02, -1.0639e-02],
        [ 5.6463e-03,  3.2065e-03, -8.0034e-03],
        [ 7.2281e-04,  6.1124e-03,  2.0410e-03],
        [-1.4229e-02, -6.9780e-03, -1.0882e-02],
        [-8.4278e-03,  4.7414e-03,  1.8668e-03],
        [ 1.3289e-03,  8.8579e-03, -1.2378e-02],
        [ 3.9743e-03,  1.0735e-03,  3.4759e-03],
        [ 1.3265e-04,  5.5304e-03,  3.1409e-04],
        [ 2.7553e-03,  9.4017e-03, -4.0958e-04],
        [ 1.5974e-03,  5.6538e-03,  3.1429e-03],
        [ 6.0036e-03,  5.9900e-03,  7.1491e-03],
        [ 4.3232e-03,  1.7015e-03, -3.4198e-03],
        [ 3.6806e-03,  1.0868e-02,  1.5433e-03],
        [ 8.5783e-03,  9.9866e-04, -1.0673e-02],
        [-9.3798e-03,  1.3520e-03, -5.3085e-03],
        [-4.4136e-03,  8.0197e-03,  1.4329e-04],
        [-5.9867e-03,  3.7688e-03, -6.8958e-03],
        [-5.8994e-03, -1.1526e-04, -2.9964e-03],
        [ 3.5062e-03,  5.4778e-03, -1.0827e-02],
        [ 3.2699e-03,  5.4455e-03, -2.7547e-03],
        [ 4.4704e-03, -1.5436e-05, -4.4543e-03],
        [-7.0021e-03,  2.7640e-03,  1.3557e-02],
        [-1.4224e-03,  2.3621e-04, -1.0090e-02],
        [-4.7385e-03,  1.4123e-03, -3.2929e-03],
        [ 6.6541e-03,  4.1018e-03, -7.4944e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
[correct] no_actions is False
task_labels:  tensor([0, 1, 0, 0, 2, 0, 1, 1, 0, 2, 0, 1, 2, 2, 0, 0, 2, 0, 0, 0, 1, 2, 1, 1,
        2, 1, 1, 2, 2, 2, 0, 0, 2, 2, 1, 0, 2, 0, 1, 1, 1, 2, 2, 0, 2, 1, 1, 2,
        2, 2, 2, 2, 0, 2, 2, 1, 2, 0, 1, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0, 1, 0, 0,
        1, 1, 1, 1, 2, 0, 1, 1, 2, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 1, 0, 1, 1, 1,
        2, 1, 1, 1, 0, 1, 0, 2, 1, 2, 1, 2, 2, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 2,
        0, 0, 0, 0, 1, 1, 1, 2]) , task_preds: tensor([[ 9.0502e-03, -2.6326e-03, -1.4829e-02],
        [ 2.3205e-03, -6.7499e-04, -2.1439e-03],
        [-1.2412e-02,  1.5858e-02, -1.1822e-02],
        [-2.9643e-03,  3.3281e-03, -3.8914e-05],
        [-6.1655e-03,  3.4752e-03, -6.5900e-03],
        [ 1.7364e-03, -1.0702e-03,  3.2921e-04],
        [-5.6594e-03,  1.6214e-03,  4.5181e-03],
        [ 1.2732e-02, -7.9615e-03,  6.2098e-03],
        [ 8.3158e-03,  2.0100e-03, -1.4006e-02],
        [ 7.1265e-03, -5.1491e-05, -2.8419e-03],
        [-9.8656e-04,  3.3554e-03, -1.0109e-02],
        [ 9.1269e-03,  1.8321e-02, -4.8696e-03],
        [ 3.6397e-03,  4.0900e-03, -1.0095e-03],
        [ 4.1669e-03,  3.7317e-03, -9.4057e-03],
        [-4.2463e-03,  3.8231e-03, -3.9416e-03],
        [-9.2778e-03, -5.4355e-03, -9.5616e-03],
        [ 1.6378e-02,  1.2792e-02, -1.6375e-02],
        [ 8.5388e-03,  5.3763e-03, -2.4849e-03],
        [ 3.2369e-03, -7.3443e-03, -9.8697e-03],
        [ 1.4216e-02, -2.4043e-03, -2.8499e-03],
        [-1.2612e-02,  8.2654e-03, -8.5247e-04],
        [ 3.6397e-03,  4.0900e-03, -1.0095e-03],
        [ 7.3156e-03,  1.1059e-02, -8.8556e-03],
        [-7.3934e-03, -3.1469e-04, -3.2817e-03],
        [ 8.1137e-03,  1.0964e-02, -6.2570e-03],
        [ 1.5480e-02,  1.5630e-02, -6.7945e-03],
        [ 1.0587e-02,  1.9273e-04,  1.4448e-03],
        [ 1.6106e-04, -4.0002e-03, -1.4162e-03],
        [-2.4876e-03, -4.0506e-03, -1.3747e-02],
        [ 3.6397e-03,  4.0900e-03, -1.0095e-03],
        [-4.9989e-03,  1.5009e-02,  4.6380e-04],
        [ 8.0585e-03,  4.8994e-03,  5.1345e-03],
        [-2.8871e-03,  1.1066e-02, -1.2751e-02],
        [ 6.0157e-03,  5.8885e-03, -3.4253e-04],
        [ 7.2371e-03, -7.6329e-03, -1.6052e-02],
        [ 5.9582e-04,  8.0151e-03, -3.1143e-03],
        [ 1.6231e-02, -2.3274e-03, -1.5698e-02],
        [ 1.3523e-02,  4.5802e-03, -1.4371e-02],
        [ 2.0707e-03,  7.9164e-03, -8.0688e-03],
        [-1.1273e-03, -1.1625e-02, -2.1841e-02],
        [ 4.2155e-03,  8.0463e-03, -1.3140e-02],
        [ 7.0490e-03,  7.7051e-03,  1.1455e-03],
        [-2.8122e-03,  5.3918e-05, -3.5240e-03],
        [ 6.9798e-03, -1.0251e-02, -6.5569e-03],
        [ 7.5327e-04, -2.3810e-04, -4.7706e-03],
        [-4.5169e-03,  2.7136e-03, -2.5500e-03],
        [ 7.1302e-03,  5.1499e-03,  2.3796e-03],
        [ 7.0515e-03,  1.6766e-03, -2.1960e-02],
        [ 6.5418e-03,  9.9770e-03, -5.0480e-03],
        [-4.3892e-03,  5.4425e-03, -6.6368e-03],
        [-3.4437e-03, -1.6589e-02,  3.3584e-03],
        [ 9.5455e-03, -5.2710e-03,  1.9266e-03],
        [-7.5382e-04,  7.9219e-03, -1.2467e-02],
        [ 1.1677e-02, -5.5046e-03, -6.4194e-04],
        [-3.2012e-03, -5.5845e-03, -1.5633e-03],
        [-9.5747e-03, -6.4743e-03,  4.9108e-03],
        [ 8.5546e-03,  2.1643e-03,  8.8368e-04],
        [ 1.7001e-03,  4.5965e-03,  2.0110e-03],
        [ 3.7640e-03,  5.2973e-03, -4.1534e-03],
        [ 2.9753e-03,  4.5180e-03,  4.7230e-03],
        [ 6.3687e-03,  7.6452e-03, -2.2838e-03],
        [ 1.3734e-02,  1.1445e-02, -4.8822e-03],
        [ 5.1564e-04,  6.9975e-03, -1.0617e-02],
        [ 5.7319e-03,  5.8605e-03,  1.8682e-03],
        [ 1.1832e-02,  7.8908e-03, -1.0875e-02],
        [ 9.8368e-03, -2.5435e-03, -2.1890e-02],
        [ 4.6557e-03, -2.9845e-04, -1.1649e-02],
        [ 5.3906e-03,  1.4275e-03, -2.4086e-03],
        [-3.9582e-03, -3.0923e-03, -6.4647e-03],
        [-7.0554e-03,  1.2448e-02, -9.7875e-04],
        [ 9.6742e-04,  2.7999e-03, -1.9436e-02],
        [ 3.9228e-03,  7.8268e-03,  3.0107e-03],
        [ 7.2611e-03,  9.0177e-03,  6.3503e-03],
        [ 2.8220e-03, -4.4029e-03, -9.0904e-03],
        [ 7.6557e-03,  6.4651e-03, -4.5461e-03],
        [-4.1201e-03, -1.9683e-03, -1.2469e-02],
        [ 1.2881e-02,  1.8091e-02, -4.4680e-03],
        [ 3.3322e-03, -7.1302e-03,  1.0131e-02],
        [-1.8093e-03,  3.2138e-04,  8.9452e-03],
        [-8.0847e-04,  3.2643e-03, -1.1838e-02],
        [ 3.5203e-03,  4.0052e-03, -1.9198e-02],
        [ 3.6397e-03,  4.0900e-03, -1.0095e-03],
        [ 1.0268e-03, -8.1165e-03, -7.3788e-03],
        [ 1.0135e-02,  2.4857e-03, -1.3079e-02],
        [ 9.8234e-03,  4.2174e-03, -8.8830e-03],
        [ 1.2818e-02,  1.7113e-02, -1.3488e-02],
        [ 1.7984e-03,  3.2345e-04, -6.7404e-03],
        [ 6.6550e-03,  1.0894e-02, -7.3439e-03],
        [ 3.8478e-04,  3.2245e-03, -1.3666e-02],
        [ 6.4660e-03,  5.1248e-03, -6.9601e-03],
        [ 5.1968e-03,  1.3555e-02,  1.8385e-03],
        [ 2.6364e-03,  7.2083e-03, -7.6093e-03],
        [-4.9385e-03,  1.3201e-03, -3.7914e-03],
        [-1.4270e-03,  3.0229e-03, -1.1144e-02],
        [ 3.5957e-03,  5.3932e-03,  2.9403e-03],
        [-1.4402e-02,  1.0931e-02, -3.2809e-03],
        [ 8.7579e-05, -5.7363e-03, -1.6858e-02],
        [-9.9937e-03, -3.4041e-03, -7.7877e-03],
        [-4.9689e-03, -9.0617e-04, -1.4304e-02],
        [ 1.5355e-03,  7.6640e-03, -9.3441e-03],
        [ 8.2140e-03, -2.6093e-04, -7.2073e-03],
        [ 7.8704e-03,  2.0141e-03, -1.8662e-02],
        [-2.8173e-03,  3.5501e-03, -7.2332e-03],
        [ 1.0514e-02,  2.3458e-03, -1.5025e-02],
        [ 3.2775e-03,  2.8265e-04, -1.2160e-02],
        [ 2.0061e-02,  1.0319e-02, -9.9810e-03],
        [-7.7996e-04, -4.3830e-03, -9.2931e-03],
        [ 8.7061e-03,  7.2019e-03, -1.1163e-02],
        [ 1.4648e-02,  4.7000e-03, -1.9110e-03],
        [ 5.8607e-03,  1.6886e-03, -8.6047e-03],
        [-3.2693e-03,  3.2338e-03, -6.9806e-03],
        [ 3.6376e-03,  5.8411e-03, -1.1838e-02],
        [-6.1108e-03,  9.6699e-03,  1.1136e-03],
        [ 9.2415e-03,  7.1856e-04, -2.3125e-03],
        [ 1.1596e-02, -2.2484e-03,  8.1381e-04],
        [ 6.5549e-04, -5.1670e-04, -4.6912e-03],
        [ 8.4473e-03,  7.2215e-04, -4.6120e-03],
        [-9.3868e-03,  6.3613e-03, -1.6395e-02],
        [-5.0005e-03,  2.2819e-03,  8.4873e-03],
        [ 6.8749e-03, -2.1017e-03, -2.3807e-03],
        [-4.1073e-03, -3.7001e-03, -8.1985e-03],
        [ 5.2449e-03,  8.3018e-03, -9.7020e-03],
        [ 2.7230e-03,  8.0064e-03, -8.3754e-04],
        [ 1.4914e-02,  1.4755e-02, -1.7326e-02],
        [ 1.7170e-02,  1.5669e-03, -5.0841e-03],
        [-1.3496e-03,  8.5895e-03, -7.8118e-03],
        [-1.6862e-04, -9.5053e-03, -6.0189e-03],
        [ 4.5602e-03,  1.5159e-03, -6.8345e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000029
penultimate_layer.0.bias: grad mean = 0.000765
output_layer.0.weight: grad mean = 0.000801
output_layer.0.bias: grad mean = 0.019809
[correct] no_actions is False
task_labels:  tensor([0, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 0, 2, 0, 1, 2, 1, 2, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 2, 1, 1, 0, 1, 0, 1, 0, 1, 2, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 2, 0, 2, 0, 0, 1, 1, 2, 1, 1, 0, 0, 1, 2, 0, 2, 2, 1, 0,
        0, 1, 1, 2, 1, 1, 1, 2, 1, 0, 2, 0, 2, 2, 2, 0, 0, 1, 2, 2, 1, 2, 0, 2,
        1, 0, 1, 1, 1, 0, 2, 2, 2, 2, 0, 1, 0, 2, 1, 2, 1, 1, 2, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 2, 1, 0, 0]) , task_preds: tensor([[ 1.8012e-02, -4.0427e-03, -5.0756e-03],
        [ 2.4391e-03,  2.5348e-03,  1.9250e-03],
        [ 1.1463e-02, -2.4456e-03, -9.3160e-03],
        [ 1.9555e-02,  1.5729e-03, -1.2611e-02],
        [-1.4416e-03,  4.3071e-04, -3.3935e-03],
        [ 2.4115e-03,  5.9748e-03, -6.6466e-03],
        [-4.8823e-03,  6.0695e-03, -1.0839e-02],
        [ 3.1959e-03,  2.6857e-03, -4.1374e-03],
        [ 1.5971e-02,  4.0870e-03, -5.5626e-03],
        [ 5.1907e-03,  9.2796e-03,  3.4415e-03],
        [ 1.8225e-02,  1.4683e-02, -1.0287e-02],
        [ 3.9681e-03, -3.1550e-03, -9.9833e-03],
        [-9.3121e-03,  3.3881e-03, -6.2078e-03],
        [ 9.0841e-03,  2.0450e-03, -2.2420e-03],
        [-7.7147e-03, -6.4902e-03,  1.6290e-03],
        [-8.2357e-03, -6.3457e-03, -1.2469e-02],
        [-2.2401e-03,  8.7491e-04, -2.4249e-03],
        [ 6.0241e-03,  4.9563e-03, -4.9729e-04],
        [ 1.3637e-02, -3.6478e-03, -8.0690e-03],
        [ 2.9536e-03,  1.1918e-02, -9.0328e-03],
        [ 5.5902e-03, -3.0976e-03, -1.3900e-02],
        [ 3.0005e-03, -5.6518e-03,  2.5318e-04],
        [-5.2908e-03, -3.1544e-03, -2.0831e-02],
        [ 9.7005e-03,  5.6469e-03,  8.7349e-03],
        [ 5.2680e-03,  4.1149e-03, -5.1630e-03],
        [ 7.0413e-03,  5.8424e-03,  6.5268e-03],
        [ 9.8379e-03,  3.6986e-03,  8.2226e-04],
        [ 9.8873e-03, -3.9129e-03, -1.4184e-02],
        [ 8.1036e-03, -1.1596e-02, -6.0677e-03],
        [ 7.5363e-03,  8.6451e-03, -3.1922e-03],
        [ 6.5250e-03,  1.0907e-02, -8.4266e-04],
        [ 9.0073e-03, -3.4989e-03, -1.3261e-02],
        [ 7.2647e-03,  5.8496e-03, -6.2315e-03],
        [ 1.4020e-02,  1.0789e-02, -4.9133e-03],
        [ 3.4832e-03,  7.1672e-04, -9.4726e-04],
        [-5.1033e-04,  7.3100e-03, -7.1767e-03],
        [ 2.7441e-03, -1.0440e-03, -2.7703e-05],
        [-2.0532e-05, -6.4528e-03, -6.8619e-03],
        [ 5.0682e-03,  7.6397e-03, -1.4183e-02],
        [ 5.4316e-03,  3.6520e-03, -6.1504e-03],
        [ 3.5338e-03,  3.1033e-03,  7.0848e-05],
        [-8.0867e-03, -1.0418e-02, -5.6289e-04],
        [ 9.0453e-03,  1.0964e-02, -5.2523e-03],
        [ 8.1591e-03, -4.4855e-03,  6.1822e-04],
        [ 1.1504e-02,  9.6505e-03, -5.1249e-03],
        [ 1.4166e-02,  5.9323e-03, -5.0503e-03],
        [ 1.7499e-03,  1.0163e-02, -6.1412e-03],
        [ 1.1776e-02,  1.2096e-02, -7.9363e-04],
        [ 1.0268e-02,  4.1928e-03, -1.0218e-02],
        [ 7.7693e-03, -8.3143e-03, -1.5781e-02],
        [ 1.5174e-02, -3.8475e-03, -5.2996e-03],
        [ 7.0822e-03,  9.2935e-03, -1.0503e-02],
        [ 4.3494e-04, -3.8921e-05, -1.0768e-02],
        [-1.3205e-03, -1.6921e-03, -1.1889e-02],
        [ 1.2710e-02,  1.1981e-02,  4.1522e-03],
        [-1.4999e-03, -8.9735e-03, -5.9878e-03],
        [ 3.7790e-04,  7.3667e-03, -1.2810e-02],
        [ 2.5789e-03,  1.0009e-03, -7.5644e-03],
        [ 6.4812e-03, -5.2272e-03, -6.1559e-03],
        [ 2.5969e-03,  4.6217e-03, -3.8084e-03],
        [-5.7662e-03,  3.3783e-03, -3.6110e-03],
        [ 7.6369e-03,  8.5646e-03,  6.5170e-03],
        [ 7.4138e-03,  4.5586e-03, -5.6041e-03],
        [ 3.6210e-03, -3.7612e-03, -3.5920e-03],
        [ 1.3000e-02,  6.4960e-03,  4.0391e-03],
        [ 4.1458e-03,  4.1579e-03, -1.3982e-02],
        [-2.4996e-03, -1.0054e-03, -2.6904e-03],
        [ 3.5338e-03,  3.1033e-03,  7.0848e-05],
        [ 5.3398e-03,  6.9662e-03, -7.5392e-03],
        [ 2.1229e-02,  6.6550e-03, -1.1312e-02],
        [ 7.6333e-03, -2.9515e-03, -2.4475e-03],
        [-4.5323e-03,  2.2654e-03, -5.0320e-03],
        [ 1.3896e-02, -6.3165e-03, -9.6033e-03],
        [ 1.3911e-02,  2.4161e-03, -7.6785e-03],
        [-6.2380e-03,  1.6339e-03, -1.3938e-03],
        [ 1.3283e-03,  3.3533e-03,  4.0925e-03],
        [ 5.6797e-03,  1.7626e-03, -7.2005e-03],
        [ 6.5823e-03,  2.9341e-03, -1.1444e-02],
        [ 4.3188e-03, -3.2996e-03, -6.5424e-03],
        [-4.8763e-03,  5.5391e-03,  5.6466e-03],
        [ 9.1287e-03,  2.9237e-04, -1.7883e-03],
        [ 6.5653e-03,  9.0954e-03, -1.0977e-02],
        [ 1.3481e-02,  6.4999e-03,  1.5651e-03],
        [ 9.0546e-03,  9.8248e-03, -1.7547e-02],
        [ 2.1316e-03, -1.0055e-03, -5.6363e-03],
        [ 1.6563e-02, -3.6006e-03, -1.4659e-02],
        [ 5.2980e-03,  8.5217e-03, -7.7001e-03],
        [ 1.5089e-02,  7.5948e-03, -7.2509e-03],
        [ 6.6133e-03, -2.3010e-03, -1.8690e-03],
        [ 4.9915e-03, -7.4993e-03, -6.3646e-03],
        [-2.0671e-04, -1.0117e-03, -1.7156e-02],
        [ 7.6284e-03, -4.4674e-03, -1.9898e-03],
        [ 1.5071e-02,  1.6553e-03, -6.8671e-03],
        [ 5.2101e-03,  1.1751e-02, -7.1481e-03],
        [ 1.3225e-03, -3.9560e-03,  1.8761e-03],
        [ 1.2747e-02,  7.9418e-03, -1.4386e-02],
        [ 1.2153e-02,  1.3819e-02, -6.8242e-03],
        [ 1.5445e-02,  1.0856e-02, -1.1201e-02],
        [-9.3058e-03,  2.3309e-04, -3.0196e-03],
        [ 1.9226e-02,  4.4535e-03, -1.2275e-04],
        [ 9.3482e-03, -5.0587e-03, -6.0666e-03],
        [-5.2469e-03,  3.2398e-03,  3.0882e-03],
        [ 6.3944e-03,  1.2751e-03, -5.7303e-03],
        [-1.6089e-02,  2.8777e-03, -1.4582e-02],
        [ 1.0616e-02,  1.1870e-03, -1.3924e-02],
        [ 8.2888e-03,  1.6928e-03, -1.0929e-02],
        [ 8.1731e-03,  2.0029e-03, -4.6328e-03],
        [ 1.2550e-02,  4.2449e-03,  1.8597e-03],
        [-5.3614e-03,  6.9319e-05, -1.2821e-02],
        [ 9.3455e-03,  7.3088e-03, -2.3346e-03],
        [ 9.6546e-04, -7.6167e-03, -1.0941e-02],
        [-1.4265e-02, -4.7779e-03, -2.2500e-03],
        [ 1.7258e-02, -1.7597e-03, -1.6461e-02],
        [ 4.0690e-03, -3.3110e-04, -1.2438e-02],
        [ 1.0767e-02,  1.3032e-03, -2.0649e-02],
        [ 8.5322e-03,  5.0314e-03, -2.0240e-03],
        [-3.7114e-03, -4.6720e-03, -7.8965e-03],
        [ 1.2057e-03, -8.4763e-03, -8.0927e-03],
        [ 9.2013e-04,  5.4806e-03, -1.0490e-02],
        [ 1.3214e-02, -1.7534e-03, -1.5739e-02],
        [-4.7430e-03,  8.0555e-03, -5.2215e-03],
        [ 5.7358e-03,  6.3676e-03, -1.2422e-02],
        [ 1.3694e-02,  6.1366e-03, -1.0954e-02],
        [ 5.4301e-03,  5.0949e-03, -4.9055e-03],
        [-5.2405e-03,  8.7002e-03,  1.4075e-03],
        [-3.1733e-03,  3.0795e-03, -9.2788e-03],
        [-2.7718e-03,  3.4549e-03, -1.7037e-02],
        [ 7.4066e-04,  1.1928e-03,  7.1122e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000029
penultimate_layer.0.bias: grad mean = 0.000722
output_layer.0.weight: grad mean = 0.000917
output_layer.0.bias: grad mean = 0.020737
[correct] no_actions is False
task_labels:  tensor([2, 1, 1, 0, 1, 2, 2, 0, 1, 2, 2, 2, 1, 2, 1, 0, 1, 1, 1, 1, 0, 2, 0, 1,
        1, 0, 2, 0, 1, 0, 1, 0, 2, 2, 0, 1, 1, 1, 0, 2, 0, 1, 2, 2, 2, 2, 2, 1,
        1, 1, 2, 0, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 0,
        2, 0, 2, 1, 1, 2, 2, 0, 2, 2, 1, 2, 0, 1, 1, 2, 2, 2, 2, 1, 1, 1, 0, 2,
        0, 1, 2, 0, 1, 0, 2, 0, 0, 1, 2, 1, 0, 0, 1, 0, 1, 0, 2, 2, 0, 2, 0, 1,
        0, 1, 1, 2, 0, 2, 2, 2]) , task_preds: tensor([[-0.0015, -0.0011, -0.0054],
        [ 0.0149, -0.0025, -0.0149],
        [ 0.0077, -0.0001, -0.0091],
        [ 0.0049,  0.0035, -0.0068],
        [-0.0039, -0.0093,  0.0117],
        [ 0.0046,  0.0029, -0.0005],
        [ 0.0030, -0.0083, -0.0101],
        [-0.0123,  0.0023,  0.0075],
        [ 0.0159, -0.0002,  0.0021],
        [ 0.0046,  0.0029, -0.0005],
        [ 0.0102,  0.0012, -0.0096],
        [-0.0023,  0.0096, -0.0038],
        [ 0.0051,  0.0010, -0.0048],
        [-0.0006, -0.0180,  0.0025],
        [ 0.0068,  0.0058, -0.0042],
        [ 0.0061,  0.0066, -0.0032],
        [ 0.0004,  0.0024, -0.0120],
        [ 0.0052,  0.0049, -0.0062],
        [ 0.0076,  0.0095, -0.0042],
        [ 0.0153,  0.0059, -0.0033],
        [ 0.0037,  0.0012, -0.0090],
        [ 0.0079,  0.0092, -0.0066],
        [ 0.0030,  0.0038,  0.0046],
        [ 0.0052,  0.0041, -0.0112],
        [ 0.0157, -0.0066, -0.0016],
        [ 0.0138, -0.0052, -0.0089],
        [ 0.0057,  0.0124, -0.0076],
        [ 0.0028,  0.0024,  0.0018],
        [ 0.0036,  0.0033,  0.0044],
        [ 0.0121, -0.0015, -0.0072],
        [-0.0007,  0.0030, -0.0034],
        [ 0.0126, -0.0037, -0.0151],
        [ 0.0100,  0.0023,  0.0042],
        [ 0.0017,  0.0049,  0.0029],
        [ 0.0174,  0.0034, -0.0018],
        [ 0.0038,  0.0027,  0.0039],
        [ 0.0123,  0.0038, -0.0050],
        [ 0.0021,  0.0010, -0.0161],
        [ 0.0064, -0.0021, -0.0049],
        [-0.0001,  0.0044,  0.0023],
        [ 0.0032, -0.0014, -0.0049],
        [ 0.0072,  0.0046, -0.0084],
        [ 0.0008,  0.0027, -0.0187],
        [-0.0046,  0.0031, -0.0037],
        [ 0.0040,  0.0063, -0.0102],
        [ 0.0024, -0.0153, -0.0139],
        [ 0.0081,  0.0028, -0.0118],
        [ 0.0095,  0.0095, -0.0172],
        [ 0.0108,  0.0039, -0.0183],
        [ 0.0042,  0.0052, -0.0109],
        [ 0.0050,  0.0020, -0.0052],
        [ 0.0065,  0.0097,  0.0016],
        [ 0.0023,  0.0077, -0.0060],
        [-0.0066, -0.0021,  0.0033],
        [ 0.0028,  0.0048, -0.0106],
        [ 0.0070,  0.0109, -0.0211],
        [-0.0018,  0.0012,  0.0010],
        [ 0.0049,  0.0121, -0.0049],
        [ 0.0087,  0.0029, -0.0036],
        [-0.0014,  0.0006, -0.0015],
        [ 0.0068,  0.0081, -0.0156],
        [ 0.0177,  0.0065, -0.0109],
        [-0.0022, -0.0031, -0.0155],
        [-0.0033, -0.0042, -0.0189],
        [ 0.0056,  0.0017, -0.0186],
        [-0.0027,  0.0022, -0.0036],
        [-0.0057,  0.0033, -0.0089],
        [ 0.0062, -0.0025, -0.0035],
        [ 0.0138,  0.0076, -0.0097],
        [ 0.0077,  0.0016, -0.0111],
        [ 0.0084,  0.0011, -0.0100],
        [ 0.0094,  0.0006, -0.0054],
        [-0.0075, -0.0044, -0.0205],
        [ 0.0049,  0.0022, -0.0053],
        [ 0.0102,  0.0055,  0.0021],
        [ 0.0046,  0.0029, -0.0005],
        [ 0.0106,  0.0019, -0.0119],
        [ 0.0144,  0.0071, -0.0182],
        [ 0.0067,  0.0013, -0.0041],
        [-0.0015, -0.0087, -0.0089],
        [ 0.0070,  0.0048, -0.0010],
        [ 0.0037,  0.0010, -0.0031],
        [-0.0087, -0.0054,  0.0031],
        [ 0.0126, -0.0003, -0.0127],
        [ 0.0070, -0.0023, -0.0013],
        [ 0.0136, -0.0023,  0.0018],
        [ 0.0202, -0.0122, -0.0015],
        [ 0.0080,  0.0071, -0.0134],
        [-0.0025, -0.0121, -0.0040],
        [ 0.0057,  0.0117, -0.0101],
        [ 0.0111,  0.0009, -0.0085],
        [ 0.0087,  0.0013, -0.0130],
        [ 0.0029,  0.0078, -0.0046],
        [-0.0002,  0.0083, -0.0174],
        [ 0.0071,  0.0060, -0.0011],
        [ 0.0113,  0.0057, -0.0056],
        [ 0.0124,  0.0054, -0.0095],
        [-0.0065, -0.0051, -0.0039],
        [ 0.0046,  0.0029, -0.0005],
        [ 0.0148, -0.0005, -0.0086],
        [ 0.0027, -0.0029, -0.0062],
        [ 0.0085,  0.0079, -0.0111],
        [ 0.0121,  0.0175, -0.0088],
        [ 0.0048, -0.0002, -0.0079],
        [-0.0183,  0.0012,  0.0007],
        [-0.0039, -0.0067,  0.0031],
        [ 0.0034, -0.0032, -0.0064],
        [ 0.0082,  0.0100, -0.0055],
        [ 0.0074,  0.0023,  0.0006],
        [ 0.0168, -0.0034, -0.0039],
        [ 0.0074,  0.0113, -0.0143],
        [ 0.0068,  0.0018,  0.0004],
        [ 0.0119,  0.0125,  0.0014],
        [ 0.0014,  0.0014, -0.0100],
        [ 0.0127, -0.0045, -0.0125],
        [ 0.0156,  0.0133, -0.0108],
        [ 0.0093,  0.0083, -0.0079],
        [ 0.0134,  0.0012, -0.0027],
        [ 0.0032, -0.0001, -0.0110],
        [ 0.0088, -0.0005, -0.0074],
        [ 0.0060,  0.0037,  0.0032],
        [ 0.0089,  0.0042, -0.0066],
        [ 0.0197,  0.0077, -0.0164],
        [ 0.0079,  0.0035,  0.0023],
        [ 0.0111,  0.0086, -0.0098],
        [ 0.0005, -0.0123, -0.0103],
        [ 0.0136,  0.0010, -0.0163],
        [-0.0017,  0.0008, -0.0059]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000034
penultimate_layer.0.bias: grad mean = 0.001044
output_layer.0.weight: grad mean = 0.000980
output_layer.0.bias: grad mean = 0.026195
[correct] no_actions is False
task_labels:  tensor([1, 0, 0, 2, 1, 1, 0, 1, 0, 2, 1, 2, 1, 1, 2, 1, 2, 0, 1, 0, 1, 0, 2, 0,
        2, 2, 0, 2, 1, 1, 0, 2, 2, 2, 1, 0, 0, 0, 1, 0, 2, 0, 2, 2, 2, 2, 1, 2,
        1, 2, 1, 0, 2, 0, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1, 0, 2,
        1, 0, 2, 0, 2, 0, 0, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1,
        0, 0, 2, 1, 1, 2, 1, 1, 0, 1, 2, 1, 0, 2, 1, 0, 0, 2, 1, 0, 2, 2, 0, 0,
        1, 2, 0, 1, 1, 1, 0, 0]) , task_preds: tensor([[-2.8025e-03,  1.6706e-04,  3.6983e-03],
        [ 9.9989e-03,  5.6020e-03, -5.2981e-03],
        [ 2.4679e-03,  6.1010e-03, -9.5814e-03],
        [ 1.6973e-02,  3.7881e-03, -6.0767e-03],
        [ 5.2211e-03,  3.9549e-03, -3.9637e-03],
        [-9.0696e-03, -5.6468e-03,  3.6186e-03],
        [ 5.0435e-03,  5.4491e-03, -4.0230e-03],
        [ 1.5935e-02,  5.2019e-04, -6.4067e-03],
        [ 2.2289e-02,  4.9838e-03, -1.0461e-02],
        [-1.0699e-02, -9.3736e-03, -1.1303e-02],
        [ 1.1804e-02,  2.0049e-03, -7.9443e-03],
        [ 1.0257e-02,  5.2871e-03, -5.3925e-03],
        [-3.6318e-03, -4.0038e-04, -1.6151e-02],
        [ 1.3574e-02,  2.4773e-03, -9.1168e-03],
        [-1.4650e-03,  6.8954e-03, -6.6634e-03],
        [ 1.3426e-02,  2.5310e-03, -1.2752e-02],
        [ 1.5125e-02, -6.3246e-03, -2.9186e-03],
        [ 3.8991e-03,  8.6917e-03, -2.6869e-03],
        [-2.9553e-03, -4.9155e-03, -8.2670e-03],
        [ 2.0111e-02, -4.8676e-03, -5.8980e-03],
        [ 1.5700e-02,  8.1562e-03, -6.9452e-03],
        [-9.0281e-04,  2.1210e-03, -4.9078e-03],
        [-4.9637e-03, -4.9517e-03, -2.2114e-03],
        [ 1.6057e-02,  4.7748e-03, -5.3731e-03],
        [ 1.0515e-02, -4.4725e-03, -1.3487e-02],
        [ 4.4953e-03,  4.0886e-03,  1.8689e-03],
        [ 4.1454e-03,  2.2315e-03,  4.5069e-04],
        [-1.6251e-03,  6.2533e-04,  1.5065e-03],
        [ 6.0966e-03,  9.2860e-03,  2.6878e-03],
        [ 5.7645e-03,  8.9335e-03,  1.0622e-02],
        [ 5.8550e-03,  2.5322e-03, -4.0068e-03],
        [ 1.2971e-02, -9.7865e-03, -1.2111e-02],
        [ 4.1454e-03,  2.2315e-03,  4.5069e-04],
        [ 5.6894e-03,  8.2691e-03,  1.1888e-03],
        [ 7.5383e-03, -5.5289e-03, -1.4234e-02],
        [ 1.1392e-02,  9.2686e-04,  2.1918e-03],
        [ 6.0978e-03,  3.1982e-03,  3.5401e-03],
        [ 3.3403e-03,  3.2396e-03,  4.9363e-03],
        [ 6.1378e-03,  1.8388e-03,  1.5088e-03],
        [ 4.1454e-03,  2.2315e-03,  4.5069e-04],
        [ 4.1454e-03,  2.2315e-03,  4.5069e-04],
        [ 4.8066e-03,  2.1026e-03, -2.5999e-03],
        [ 8.4205e-03,  2.3806e-03, -2.3944e-03],
        [ 1.2420e-02,  6.4636e-04, -6.9194e-03],
        [ 1.0618e-02,  3.2045e-04, -7.6168e-03],
        [ 1.0002e-02,  4.1876e-03, -1.0974e-02],
        [ 1.4756e-02,  9.8170e-03, -6.5306e-03],
        [-1.5534e-03, -6.8046e-03, -1.6995e-03],
        [ 3.1147e-03, -4.9167e-03, -7.8955e-03],
        [ 8.7434e-03,  7.4007e-03,  1.4703e-03],
        [ 1.3097e-02, -3.2592e-03, -9.7963e-03],
        [ 1.5354e-02, -4.6451e-03, -8.4209e-03],
        [-4.9010e-03,  2.5257e-03, -2.8414e-03],
        [ 1.7827e-02,  9.7268e-03, -1.1944e-02],
        [-3.6201e-03,  8.7929e-03, -1.0115e-02],
        [-2.2347e-03,  1.9803e-03,  1.9590e-03],
        [-2.4372e-03,  1.3012e-02, -9.9537e-03],
        [ 8.7142e-03,  3.6682e-03,  6.0943e-03],
        [ 1.1081e-04,  1.9836e-03, -1.7424e-02],
        [ 1.3831e-02,  4.7168e-03, -5.5138e-03],
        [ 6.4218e-03,  7.5798e-03, -7.8747e-03],
        [-1.6268e-04,  5.2433e-04, -4.1591e-03],
        [ 1.1534e-02, -3.7933e-03, -1.3516e-02],
        [ 7.7205e-03,  7.8969e-03, -1.3910e-03],
        [ 1.1974e-02, -2.2772e-03, -8.6842e-03],
        [ 5.5702e-03,  4.8361e-04,  7.6321e-04],
        [ 8.8624e-03,  4.1723e-04,  1.8357e-03],
        [-6.5739e-03, -7.1109e-03, -1.0025e-02],
        [ 8.5815e-03,  7.2196e-03, -1.4712e-02],
        [-5.2455e-03, -1.0944e-02, -1.4234e-02],
        [ 1.1636e-02,  1.7126e-03, -1.4225e-02],
        [ 2.1566e-02,  7.5956e-03, -8.3831e-03],
        [ 6.8712e-03, -2.7882e-03, -1.0640e-02],
        [ 1.1546e-02, -7.6631e-03, -8.1780e-03],
        [ 9.1537e-05, -6.0326e-03,  1.1923e-03],
        [-1.2126e-02, -2.8579e-03, -1.3625e-02],
        [ 2.5583e-03, -1.1879e-03, -5.2806e-03],
        [-2.9749e-03,  2.8615e-04, -4.3134e-03],
        [-1.2402e-03,  1.8209e-03, -2.5688e-03],
        [-3.6260e-03,  1.3638e-02,  4.8207e-03],
        [ 1.5100e-02,  7.5419e-03, -1.4092e-02],
        [ 1.1121e-02, -8.6913e-03, -6.6195e-03],
        [-3.9510e-03,  5.9444e-03,  2.9369e-03],
        [ 3.7863e-03,  1.1968e-02, -9.0388e-04],
        [ 7.9913e-03, -5.4672e-03, -3.4375e-03],
        [ 1.2324e-02,  9.2397e-03, -1.4897e-02],
        [ 1.0646e-02,  7.6043e-04, -1.9618e-02],
        [ 1.0185e-02,  1.6281e-03, -1.0147e-02],
        [ 8.5001e-03, -7.1166e-04, -7.0275e-03],
        [-1.7045e-03,  1.1578e-02, -7.2428e-03],
        [ 1.5110e-02, -1.0379e-02,  1.9456e-04],
        [ 2.3907e-02, -6.5762e-03, -1.0916e-02],
        [ 5.6632e-05,  2.0842e-03, -1.1428e-02],
        [ 8.4255e-03,  8.5819e-03,  5.8382e-03],
        [ 8.3807e-03,  3.0153e-03, -2.6908e-03],
        [ 1.5672e-02, -4.2849e-03, -2.8662e-03],
        [ 1.6592e-02,  1.2770e-02, -7.1496e-03],
        [ 1.9802e-02,  8.5433e-03, -6.3376e-03],
        [ 4.5968e-03,  2.8855e-03, -3.6192e-03],
        [-7.4282e-04,  4.1291e-03, -1.3000e-02],
        [ 6.6103e-03,  9.5697e-03,  5.7072e-03],
        [-4.9041e-05,  3.5962e-03, -7.4137e-03],
        [ 6.8857e-03,  4.2200e-03,  4.6056e-04],
        [ 8.4303e-03,  8.0341e-03, -3.3025e-03],
        [ 2.2243e-02,  5.6260e-03, -2.1333e-03],
        [ 4.9113e-03,  1.3095e-02,  2.4791e-03],
        [ 2.2562e-03, -1.6103e-02, -1.2933e-02],
        [ 1.4789e-02, -1.9543e-03, -1.6799e-02],
        [ 5.6088e-03,  2.6045e-03, -6.5348e-03],
        [ 1.1425e-02, -2.4994e-03,  5.0001e-05],
        [ 4.5828e-03,  4.6346e-03, -7.2574e-03],
        [ 1.2948e-02,  3.1448e-03, -5.5241e-03],
        [-2.0139e-03, -3.5202e-03, -1.5180e-02],
        [-3.8249e-03,  7.3378e-03,  1.6837e-03],
        [ 3.5544e-03, -9.3298e-04, -8.3848e-04],
        [ 2.7637e-03,  4.3087e-03, -3.0010e-03],
        [ 1.4479e-02, -1.4909e-03, -8.8256e-03],
        [ 6.7437e-04, -5.4731e-03, -1.4304e-02],
        [ 2.2047e-04,  2.4474e-03, -3.7416e-03],
        [ 1.6095e-02,  4.7522e-03, -4.8209e-03],
        [ 6.2107e-03,  6.5501e-03, -2.4611e-03],
        [ 1.8877e-02,  5.5596e-03, -1.3058e-02],
        [ 1.7478e-02,  1.1203e-02,  4.6108e-03],
        [ 8.1306e-03,  8.4231e-03, -1.7771e-02],
        [ 6.0952e-03, -7.9005e-03, -6.8732e-03],
        [ 9.5238e-04, -4.9504e-03, -1.0136e-02],
        [ 8.7449e-03,  7.3727e-03, -1.2606e-02],
        [ 2.1816e-04,  1.3770e-03, -1.0086e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000038
penultimate_layer.0.bias: grad mean = 0.001364
output_layer.0.weight: grad mean = 0.001000
output_layer.0.bias: grad mean = 0.035573
[correct] no_actions is False
task_labels:  tensor([0, 0, 2, 2, 2, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 2,
        1, 1, 1, 1, 2, 1, 0, 1, 0, 2, 0, 0, 0, 2, 1, 1, 2, 1, 1, 2, 0, 1, 2, 1,
        2, 2, 0, 2, 0, 1, 0, 0, 2, 0, 2, 1, 0, 1, 0, 1, 1, 1, 2, 2, 2, 1, 1, 0,
        0, 2, 2, 0, 2, 1, 1, 0, 2, 1, 2, 2, 2, 0, 1, 2, 1, 2, 2, 0, 0, 1, 2, 0,
        2, 1, 1, 2, 1, 0, 1, 2, 0, 0, 2, 2, 0, 1, 2, 1, 0, 2, 2, 0, 2, 1, 0, 1,
        1, 0, 0, 1, 2, 2, 2, 0]) , task_preds: tensor([[ 1.1865e-02,  3.7764e-04, -8.4285e-03],
        [ 1.0270e-02,  1.6334e-03, -1.5458e-03],
        [ 2.8050e-03,  1.0839e-02, -2.3950e-03],
        [ 2.1006e-02,  3.9300e-04, -1.2580e-02],
        [ 1.0839e-02,  3.4670e-03, -3.5351e-03],
        [ 9.8520e-03,  1.3888e-03, -3.5429e-03],
        [ 6.9010e-03,  1.2962e-02, -8.2791e-03],
        [ 3.9126e-03, -1.4220e-03, -1.2750e-02],
        [ 5.1213e-03,  4.4258e-03,  1.5924e-02],
        [ 2.3530e-03, -2.6603e-04, -1.2697e-02],
        [ 6.4406e-03,  7.5337e-03, -5.0916e-03],
        [ 1.8627e-02,  1.2183e-02, -7.7037e-03],
        [ 3.3257e-03,  4.0112e-03, -9.0262e-03],
        [ 5.0566e-03,  1.2694e-02, -5.4233e-03],
        [ 5.5138e-03,  4.8341e-03,  1.9148e-03],
        [ 1.2995e-02, -6.8176e-03, -3.8301e-04],
        [ 8.8781e-03, -1.8127e-03,  7.4660e-04],
        [-4.5856e-04, -2.2740e-03, -2.9806e-03],
        [ 2.7380e-03,  7.0698e-03, -3.7828e-03],
        [ 8.7677e-03, -2.7764e-03, -1.2114e-02],
        [ 9.2413e-03,  3.0348e-04, -1.9171e-03],
        [ 4.5528e-03, -1.2051e-04, -3.3074e-03],
        [ 1.2976e-02,  1.1783e-03, -2.6213e-04],
        [ 8.2760e-03, -1.4025e-02, -1.1393e-03],
        [-1.5150e-02, -9.7128e-03,  7.4123e-04],
        [-9.6555e-04,  4.0780e-03, -8.8353e-03],
        [ 9.6944e-03, -1.8130e-03, -1.2623e-03],
        [ 8.9644e-03,  9.4933e-03, -4.8240e-05],
        [ 4.9258e-03,  1.1777e-03,  5.0942e-03],
        [ 4.3971e-03,  8.3471e-03, -5.3386e-03],
        [ 9.8104e-03, -5.1019e-03, -1.2028e-02],
        [ 1.4958e-02,  2.3198e-03, -8.4470e-03],
        [-3.7082e-03,  5.8763e-03,  3.6141e-03],
        [ 3.4844e-03, -6.6361e-03, -7.0102e-03],
        [-6.9426e-04,  2.7248e-03, -1.1507e-02],
        [ 1.3758e-02, -3.4536e-03, -1.5192e-03],
        [ 7.4566e-03,  1.5855e-02, -6.2481e-03],
        [ 6.2140e-03,  1.7571e-04, -1.6047e-02],
        [ 6.5976e-03,  6.1404e-03, -3.6372e-03],
        [ 1.4365e-02,  1.0198e-02, -1.4992e-02],
        [ 6.4936e-03,  6.1947e-03, -7.7136e-03],
        [ 1.1331e-03,  7.9745e-04, -1.3004e-02],
        [ 1.3412e-02, -4.1434e-03, -8.5976e-03],
        [ 6.9731e-03,  1.1058e-02,  7.4399e-03],
        [ 6.4021e-03,  5.3744e-03, -1.2935e-02],
        [ 5.2259e-03,  1.0352e-02, -8.3276e-03],
        [ 1.1998e-02, -7.4483e-03, -7.9203e-03],
        [ 4.3760e-03, -5.8940e-04, -1.2491e-02],
        [-5.6856e-03,  4.2874e-03,  2.5797e-03],
        [ 1.4133e-02,  6.5532e-03, -1.7475e-02],
        [ 4.4135e-03,  1.2420e-02, -1.0373e-02],
        [ 1.5178e-03, -5.0249e-03, -4.8455e-03],
        [ 3.6424e-03, -9.5761e-03, -8.9338e-03],
        [ 7.0561e-03,  5.3064e-03, -3.3554e-03],
        [ 1.7260e-03, -4.5391e-03,  1.9795e-03],
        [ 4.2097e-03, -5.3523e-03, -7.9243e-03],
        [ 1.3871e-03,  1.0875e-03, -9.5840e-03],
        [-5.0299e-03, -9.2835e-03, -7.3548e-03],
        [ 9.9564e-03,  4.6600e-03,  3.0689e-03],
        [ 9.3542e-03,  2.0921e-03, -1.6182e-02],
        [ 1.7125e-03,  6.4008e-04, -9.4291e-03],
        [-2.1634e-03,  3.5207e-03, -1.4860e-03],
        [ 7.9625e-03, -6.1721e-03, -3.0525e-03],
        [-1.5246e-03,  1.0294e-04, -2.2445e-03],
        [ 3.7420e-03,  4.0715e-03, -4.2054e-03],
        [ 9.6085e-03,  3.2357e-03, -1.6698e-02],
        [ 2.0289e-03,  5.9684e-04, -6.3528e-04],
        [ 5.8902e-03, -1.8924e-03, -5.7099e-03],
        [ 1.7449e-03, -2.0796e-03, -5.7522e-04],
        [ 1.9473e-02,  1.4432e-02, -1.1047e-02],
        [-3.0930e-03,  1.3281e-03, -2.4984e-03],
        [-6.0913e-03,  7.3735e-03,  4.2801e-03],
        [ 1.7435e-03,  4.5058e-03,  4.0389e-03],
        [ 8.2646e-03, -4.9086e-05, -5.8735e-03],
        [ 1.2839e-03,  6.3081e-03, -5.3467e-03],
        [ 6.8573e-03,  2.6526e-03, -6.6907e-03],
        [ 7.9157e-03,  1.4638e-03, -1.0287e-02],
        [ 7.1467e-03, -2.3516e-04,  9.2549e-04],
        [-2.9508e-03,  5.4184e-03, -1.1751e-02],
        [ 4.0922e-03,  1.4799e-03,  1.2563e-03],
        [-1.2632e-03,  1.3020e-02, -5.3276e-03],
        [ 1.0903e-02,  8.9247e-03, -1.0589e-02],
        [-1.4652e-02,  8.4462e-04, -1.3663e-02],
        [-5.9708e-04,  2.5196e-03, -1.3051e-02],
        [ 2.4757e-02,  9.5754e-03, -1.5127e-02],
        [ 1.4780e-03, -2.4592e-03, -8.6899e-03],
        [ 3.2577e-03,  4.1452e-03, -2.1046e-02],
        [ 5.4526e-03,  1.1848e-02, -1.6579e-02],
        [ 1.4512e-03,  6.2233e-04, -1.2143e-02],
        [ 9.5985e-03, -2.0074e-03,  5.5754e-04],
        [ 8.0219e-03,  6.6271e-04, -9.6913e-03],
        [-3.3623e-03,  5.2719e-04,  9.1915e-04],
        [ 1.5085e-03,  3.4894e-03, -1.0175e-02],
        [ 3.7877e-03, -9.0461e-03, -6.0789e-03],
        [ 7.4153e-03,  7.9744e-03, -4.9604e-03],
        [ 9.7182e-03,  5.6540e-04, -6.5226e-03],
        [-3.3913e-03, -4.9517e-03, -1.8052e-02],
        [ 1.0871e-02, -5.7422e-03, -6.5936e-03],
        [ 1.5818e-02,  9.9924e-03, -1.1010e-02],
        [ 3.5956e-03, -4.7935e-03,  2.3218e-03],
        [ 1.0133e-02, -9.3334e-04, -1.0414e-02],
        [-5.8141e-03, -1.1179e-02, -1.6419e-03],
        [ 2.4217e-03,  4.3332e-03, -1.8623e-02],
        [ 2.1458e-02, -3.7260e-03, -1.0369e-02],
        [ 8.4852e-03,  6.0102e-03, -2.9582e-03],
        [ 9.6440e-03,  9.5590e-03, -6.6694e-03],
        [-4.8205e-03,  1.0657e-02, -2.0523e-03],
        [ 1.3144e-02,  1.1203e-02, -8.2222e-03],
        [ 8.8746e-03, -1.2347e-03, -1.1137e-02],
        [ 1.6553e-02, -4.8813e-03, -5.3891e-03],
        [ 8.8554e-03, -6.1282e-04, -5.4598e-03],
        [-7.6888e-03, -7.3537e-03,  4.2388e-03],
        [-2.2143e-03,  6.6227e-03, -5.7844e-03],
        [ 3.8868e-04,  1.5884e-02, -1.1118e-02],
        [ 1.1062e-02,  5.2442e-04, -1.1838e-02],
        [ 1.0788e-02, -1.3786e-02, -5.9715e-03],
        [ 2.2628e-02,  1.8970e-02, -4.7722e-03],
        [ 8.2200e-03,  2.7569e-04, -6.3222e-04],
        [ 1.0914e-02,  5.9266e-03, -3.3704e-03],
        [ 4.7337e-03,  1.3152e-03, -3.5370e-03],
        [ 6.3744e-05, -1.1907e-03, -6.3212e-03],
        [ 2.2029e-03, -3.5079e-04, -7.0852e-03],
        [ 1.6359e-02,  1.1017e-02, -1.4662e-02],
        [-1.9021e-03,  5.3160e-04, -4.9105e-03],
        [ 6.3965e-03,  1.0698e-02, -2.0477e-02],
        [ 6.5211e-03,  2.4127e-03, -4.2031e-03],
        [ 1.1656e-02, -5.2586e-03, -1.0961e-02],
        [ 3.1433e-03,  6.4290e-03, -1.4093e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000027
penultimate_layer.0.bias: grad mean = 0.000254
output_layer.0.weight: grad mean = 0.000793
output_layer.0.bias: grad mean = 0.007014
[correct] no_actions is False
task_labels:  tensor([1, 2, 2, 1, 0, 2, 1, 2, 2, 0, 0, 0, 1, 2, 2, 1, 0, 0, 0, 2, 0, 0, 2, 1,
        1, 0, 1, 1, 1, 2, 0, 0, 1, 2, 2, 1, 2, 0, 1, 1, 0, 1, 1, 0, 2, 1, 2, 2,
        2, 1, 0, 2, 1, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 1, 2, 0, 2, 2,
        2, 1, 2, 2, 1, 0, 2, 0, 0, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 0, 1, 2,
        1, 0, 2, 1, 2, 2, 0, 2, 0, 1, 2, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 2, 1, 0, 1, 2, 2, 2]) , task_preds: tensor([[ 4.4364e-03,  6.8416e-04,  1.7933e-03],
        [ 5.6709e-03,  4.5866e-04,  2.7900e-04],
        [-2.4761e-03, -3.3919e-03, -1.0974e-05],
        [ 4.0977e-03,  2.3821e-03, -3.3768e-03],
        [ 8.2870e-03,  5.5835e-04,  2.5854e-06],
        [ 1.2630e-02, -8.8879e-03, -1.0088e-03],
        [ 1.4105e-02,  1.0018e-02, -1.4617e-02],
        [ 6.7110e-04,  3.8738e-05, -2.2908e-03],
        [ 4.4364e-03,  6.8416e-04,  1.7933e-03],
        [ 5.1514e-03, -6.3529e-03, -1.1821e-02],
        [ 3.8021e-03, -6.8192e-03, -2.5779e-03],
        [ 2.8375e-03,  1.7015e-03, -6.8415e-03],
        [ 1.0946e-02, -6.1317e-03,  3.8138e-05],
        [ 8.1745e-03,  5.1955e-03, -1.1657e-02],
        [ 9.6272e-03,  7.0345e-03, -1.0780e-02],
        [ 9.5102e-03, -3.7485e-03, -1.1869e-02],
        [-3.2180e-03,  1.5456e-03,  1.7067e-03],
        [ 1.7339e-03, -2.4435e-03, -5.4746e-03],
        [ 6.3020e-03, -1.0123e-03,  3.1664e-03],
        [-6.8352e-05,  2.6186e-03, -6.4168e-03],
        [ 1.4898e-02,  1.0005e-03, -1.1823e-02],
        [-2.3278e-04,  1.1214e-02,  4.1960e-03],
        [ 1.3765e-02, -5.3497e-04, -1.4851e-02],
        [ 6.2501e-03,  3.4252e-03, -4.5163e-03],
        [-8.0743e-04,  3.7199e-03, -8.6055e-03],
        [ 1.0044e-02,  2.8018e-03,  4.6432e-03],
        [ 4.6374e-03,  3.2826e-03, -2.8298e-03],
        [ 2.6176e-03,  2.8300e-03,  4.7048e-03],
        [ 5.4280e-04, -7.5093e-03, -9.7556e-03],
        [ 1.3288e-02,  1.5954e-02, -2.6492e-03],
        [-6.1161e-03, -7.9662e-03, -9.5270e-03],
        [ 1.5527e-02, -7.9378e-03, -1.2469e-02],
        [-2.4476e-03, -8.5130e-04,  3.7827e-04],
        [-5.4719e-03, -1.4953e-02, -7.2619e-03],
        [-2.1901e-03, -8.1934e-03, -1.0775e-02],
        [ 1.1560e-02, -1.9083e-04,  6.4352e-04],
        [ 1.3481e-02,  4.3880e-03, -8.9782e-03],
        [ 1.2120e-02,  4.2549e-04, -2.3051e-03],
        [ 5.2323e-03, -4.5571e-03, -1.9754e-03],
        [-5.3283e-03, -7.7390e-03,  3.9484e-03],
        [ 7.3721e-03,  3.8695e-03,  8.2972e-04],
        [-2.5472e-03, -7.4475e-03,  2.6210e-03],
        [ 3.5453e-03, -8.8550e-03, -1.3306e-02],
        [ 6.5897e-03,  9.3464e-03, -9.3145e-03],
        [ 6.2931e-03,  1.0917e-02, -7.3583e-03],
        [ 3.9263e-03, -2.4457e-04,  1.5394e-03],
        [ 7.1376e-03,  7.1964e-03, -3.9483e-03],
        [ 7.0260e-03,  2.1278e-03, -4.4147e-03],
        [ 2.7114e-03,  1.9638e-03,  1.7115e-03],
        [ 8.1910e-03, -3.6313e-03,  2.7249e-04],
        [ 1.0983e-02,  5.3357e-03, -2.8359e-03],
        [ 1.1258e-03,  5.7391e-03, -4.6410e-03],
        [ 9.6959e-03,  6.0821e-03, -1.7536e-02],
        [ 2.1470e-02,  6.3970e-03, -7.0911e-03],
        [ 2.0676e-02,  6.2126e-03, -1.1708e-02],
        [ 1.3221e-03, -5.4887e-03, -8.6975e-04],
        [ 6.6892e-03,  1.6297e-04, -2.8942e-03],
        [-1.3214e-03, -3.9993e-03, -1.5269e-02],
        [ 7.6047e-03, -2.1173e-03, -4.5303e-03],
        [ 5.8214e-03,  1.0461e-02, -1.9786e-02],
        [ 1.1941e-02,  1.8920e-04, -5.9384e-03],
        [ 9.6349e-03, -2.1363e-03, -4.2034e-03],
        [ 1.3445e-02, -4.7117e-03, -1.4708e-02],
        [ 3.9466e-03, -2.6202e-03, -4.2762e-03],
        [ 6.3736e-03,  2.8018e-03,  3.6899e-03],
        [-4.3431e-04,  5.7452e-03, -9.6207e-03],
        [ 1.8245e-02, -5.6355e-03, -4.4582e-03],
        [-9.7187e-03, -5.8373e-03,  4.2972e-03],
        [ 4.4364e-03,  6.8416e-04,  1.7933e-03],
        [ 1.6515e-02,  3.7070e-03, -4.6594e-03],
        [ 9.6432e-03,  9.1519e-04, -6.3567e-03],
        [ 1.4962e-02, -7.8568e-03, -1.0050e-02],
        [ 1.1819e-02, -2.6584e-04, -1.9927e-02],
        [-4.6552e-04, -9.0545e-04, -2.5401e-03],
        [-7.5979e-03, -5.8504e-03, -1.8931e-02],
        [ 1.4257e-02,  4.4813e-03, -3.2515e-03],
        [ 5.2399e-03,  9.5217e-03,  3.0917e-03],
        [ 1.1768e-02, -6.2381e-05, -2.4510e-03],
        [ 1.1866e-02, -2.0668e-03, -1.0238e-02],
        [ 3.0683e-03,  4.1769e-03, -1.3962e-03],
        [ 1.4797e-02,  1.2438e-02, -6.0516e-03],
        [ 1.2013e-02, -2.0136e-03, -1.1789e-02],
        [ 4.8210e-03,  9.7458e-04, -1.5174e-02],
        [ 7.3375e-03, -6.2764e-03, -2.1090e-03],
        [-7.6614e-04,  2.2540e-03, -9.6673e-03],
        [ 9.6811e-03, -5.9745e-03, -1.1035e-02],
        [ 1.7274e-02,  1.1867e-02, -5.0973e-03],
        [ 1.2088e-02,  3.2725e-03, -1.5608e-02],
        [ 1.1178e-02,  7.4134e-03, -1.0422e-03],
        [ 2.0463e-02, -1.3281e-02, -5.9524e-04],
        [-3.0111e-03,  3.8988e-04, -1.4402e-03],
        [ 6.7205e-03,  4.1394e-03, -7.6375e-03],
        [ 1.6675e-03, -1.8597e-03,  3.9869e-05],
        [ 1.0469e-02,  4.5172e-03, -4.5720e-03],
        [ 1.3868e-02,  1.9352e-03, -7.6547e-03],
        [-5.4496e-03,  5.4511e-03,  3.4052e-03],
        [-8.1853e-03, -4.9097e-03, -1.0508e-02],
        [-2.2131e-04,  1.2866e-03, -4.6098e-03],
        [ 1.2966e-02,  7.8214e-03, -5.3190e-03],
        [ 2.0807e-03,  5.0963e-03, -1.0337e-02],
        [ 4.0449e-03,  3.3382e-03,  2.9741e-03],
        [ 2.5392e-03, -4.3014e-03, -2.5948e-03],
        [-4.8265e-04,  2.1801e-03, -1.1122e-02],
        [ 8.1618e-03, -3.4639e-03, -2.4716e-03],
        [ 1.0263e-02,  1.0657e-03, -1.8185e-02],
        [ 6.0728e-03, -9.9310e-03, -2.7735e-04],
        [ 8.0098e-03,  1.8501e-03,  3.9689e-03],
        [ 3.2662e-03,  5.3284e-03,  3.9901e-03],
        [ 7.1438e-03, -9.0706e-03,  9.0146e-03],
        [ 1.2030e-02,  2.4917e-03,  2.7514e-04],
        [ 1.4366e-02, -2.7103e-03, -1.1474e-02],
        [-7.1164e-03, -6.9141e-03,  1.5046e-03],
        [ 1.2885e-02, -3.4788e-03, -6.4005e-03],
        [ 1.5054e-02, -4.4081e-03, -2.2822e-03],
        [ 7.6378e-03,  4.0067e-04, -1.8797e-03],
        [ 7.8794e-03,  4.0185e-04, -9.5208e-04],
        [ 2.2937e-02,  3.9535e-03, -9.9263e-03],
        [-5.5132e-03,  2.2684e-03, -7.9843e-03],
        [ 3.9804e-03, -6.2931e-03, -6.7792e-03],
        [ 7.8030e-04, -4.8896e-03, -1.0092e-02],
        [ 1.3126e-02,  1.4590e-02, -1.2263e-02],
        [ 4.2237e-03, -1.6829e-03,  4.4948e-04],
        [ 1.4771e-02, -2.1454e-03, -1.6607e-02],
        [ 1.5493e-02, -1.1635e-03, -1.3124e-02],
        [ 1.4072e-02, -3.3078e-03, -1.3513e-02],
        [ 7.5084e-03,  1.4756e-04, -8.7641e-03],
        [ 4.4364e-03,  6.8416e-04,  1.7933e-03],
        [ 4.3480e-03,  1.4162e-03, -8.7284e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000032
penultimate_layer.0.bias: grad mean = 0.000564
output_layer.0.weight: grad mean = 0.000944
output_layer.0.bias: grad mean = 0.015773
[correct] no_actions is False
task_labels:  tensor([2, 2, 1, 1, 1, 0, 2, 1, 2, 2, 0, 1, 2, 0, 1, 1, 0, 1, 1, 2, 1, 0, 2, 2,
        0, 2, 1, 2, 2, 1, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 0, 2, 1, 2, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 1, 1, 2, 0,
        1, 0, 1, 1, 0, 1, 1, 2, 1, 1, 2, 0, 1, 2, 1, 1, 1, 0, 0, 1, 0, 2, 0, 2,
        0, 1, 0, 0, 1, 0, 2, 1, 2, 2, 1, 1, 2, 0, 0, 1, 2, 2, 0, 1, 0, 0, 2, 0,
        0, 0, 1, 2, 2, 2, 1, 0]) , task_preds: tensor([[-5.4293e-03,  3.5274e-04, -2.1103e-04],
        [ 4.6857e-03, -4.1814e-04,  2.7284e-03],
        [ 4.6857e-03, -4.1814e-04,  2.7284e-03],
        [-2.7199e-03, -6.7785e-03, -9.1388e-03],
        [-4.1678e-03, -5.9866e-03, -6.2250e-03],
        [ 1.7777e-02,  3.9901e-03, -1.0106e-02],
        [-5.1873e-04, -2.0016e-02,  4.4943e-03],
        [ 1.3504e-02, -4.9431e-03, -8.4076e-03],
        [ 5.4955e-03,  5.8300e-03, -5.1325e-03],
        [-1.1660e-03, -1.4088e-03,  3.2194e-03],
        [ 3.9030e-03, -1.9997e-03,  2.9536e-03],
        [ 1.6075e-02, -9.7615e-03, -2.0092e-03],
        [-3.2984e-03, -4.4910e-03,  1.5101e-03],
        [ 1.8683e-03, -9.3775e-04, -7.9407e-03],
        [ 5.3051e-03,  1.6807e-03, -6.5146e-03],
        [ 3.9086e-04,  2.5823e-04, -1.2892e-02],
        [-9.2067e-03, -3.1519e-03, -8.5655e-03],
        [ 2.6902e-03, -5.7586e-04,  1.8538e-03],
        [ 1.3237e-02, -4.8440e-03, -1.9978e-02],
        [ 4.5212e-03,  5.6575e-03, -1.0148e-02],
        [ 1.2971e-02,  6.3058e-04,  5.0615e-03],
        [ 9.3023e-03,  1.5846e-03, -8.0546e-03],
        [ 2.0995e-02,  2.9690e-03, -7.3320e-03],
        [ 1.3302e-02, -6.7875e-05, -2.5345e-03],
        [ 1.5207e-02, -7.7135e-03, -7.5704e-03],
        [ 7.8892e-03, -1.5241e-03, -4.0704e-03],
        [ 1.2944e-02,  4.8382e-03,  3.2388e-03],
        [ 1.1637e-02, -2.2508e-03, -8.5367e-03],
        [ 5.1594e-03,  8.5358e-04,  7.2872e-03],
        [ 1.4197e-02,  3.1016e-03, -6.3996e-03],
        [ 8.5580e-03,  1.0899e-02, -1.2717e-02],
        [ 1.9882e-03, -3.5237e-03, -2.7457e-03],
        [ 1.6438e-02,  8.7270e-03, -1.2375e-02],
        [ 6.4508e-03,  8.0984e-03,  7.3228e-03],
        [ 1.7478e-02, -7.5484e-03, -1.1338e-02],
        [-5.6645e-03, -4.3404e-03, -1.4742e-02],
        [ 8.4306e-03,  6.1741e-04, -2.4077e-03],
        [ 1.6263e-02, -6.2170e-03, -3.8045e-03],
        [ 5.7863e-03,  1.2270e-02, -6.2786e-03],
        [ 6.9541e-03, -7.5320e-03, -3.5303e-03],
        [-2.7323e-03,  2.8971e-03,  7.3243e-03],
        [ 1.3019e-02,  4.2757e-03, -1.0867e-02],
        [-4.7720e-04,  4.8113e-03, -1.1064e-02],
        [ 7.5477e-03,  1.3633e-03,  2.1409e-03],
        [ 2.1851e-02,  5.1944e-03, -4.6114e-03],
        [ 3.4987e-03,  5.8008e-03, -9.0735e-03],
        [ 2.1320e-03, -4.1952e-03, -7.4331e-03],
        [ 2.4760e-03,  2.9322e-03, -1.9191e-02],
        [ 3.7827e-03,  8.2411e-03, -1.3717e-02],
        [ 2.7515e-03,  2.7956e-03, -1.9664e-03],
        [-9.8266e-03, -8.8239e-03,  5.3443e-03],
        [ 1.8516e-02,  7.9161e-03, -1.0642e-02],
        [ 7.4171e-03,  8.7312e-03, -1.4305e-02],
        [ 2.3310e-03, -3.8000e-03,  1.0176e-03],
        [ 9.6869e-03,  8.1346e-03, -5.2467e-03],
        [ 6.3013e-03, -9.4700e-03, -3.2340e-03],
        [ 1.5735e-02, -1.4335e-03, -4.2669e-03],
        [ 1.2448e-02, -5.5065e-03, -2.0954e-02],
        [-8.4094e-03, -8.0819e-03,  5.5351e-03],
        [-2.8462e-03,  1.0240e-02,  4.6803e-03],
        [ 9.1831e-03,  2.9305e-03, -2.1921e-03],
        [ 2.0434e-03, -3.5804e-03, -1.2607e-02],
        [ 4.2540e-03,  9.8959e-03, -8.3571e-03],
        [ 1.3568e-02, -7.5439e-04, -1.0060e-02],
        [-3.6188e-04, -3.9545e-03, -3.5496e-03],
        [-5.9336e-03, -6.5864e-03, -1.4128e-02],
        [ 1.1089e-02, -7.3193e-03, -1.1659e-02],
        [ 1.0603e-03,  4.3396e-03, -6.3695e-03],
        [ 7.4487e-03, -4.3026e-03, -6.1085e-03],
        [-4.0577e-04, -4.9328e-03, -9.5006e-03],
        [ 7.5656e-03, -3.7115e-03,  2.4083e-03],
        [ 8.3747e-03, -1.1312e-03, -5.7068e-03],
        [-4.3519e-03, -3.8814e-03, -9.6589e-03],
        [ 5.0509e-03,  3.4444e-03, -3.3538e-03],
        [ 1.4309e-04,  1.3357e-03, -5.3337e-03],
        [ 3.1747e-03,  2.1679e-03,  2.5221e-03],
        [ 1.4141e-02, -4.4952e-03, -1.3753e-02],
        [ 4.7787e-03,  3.6130e-03, -4.6348e-03],
        [-2.2229e-03, -9.7647e-03, -9.3250e-03],
        [ 5.1669e-03,  9.1684e-04,  4.7615e-03],
        [ 6.7920e-03,  2.9419e-03,  1.8142e-03],
        [ 7.5140e-03,  1.1009e-02,  3.5142e-03],
        [ 8.4264e-03, -4.4023e-03, -9.7224e-06],
        [ 9.4129e-03, -2.6803e-03,  6.6733e-03],
        [ 2.7562e-03,  1.8945e-03, -6.3954e-03],
        [-2.9752e-03,  8.0148e-03, -1.6831e-03],
        [-4.1739e-03,  6.4349e-03,  4.7996e-03],
        [ 1.4239e-02,  8.3849e-03, -4.6871e-03],
        [ 8.1826e-03,  9.3217e-03, -4.8875e-03],
        [ 2.9585e-03,  8.4630e-03, -5.8153e-03],
        [ 7.2382e-03,  1.4139e-02, -4.3142e-03],
        [ 1.2717e-02,  4.3160e-03, -2.5366e-04],
        [ 6.8516e-03, -9.0605e-03, -1.6010e-02],
        [ 1.8184e-02,  3.1758e-03, -1.0069e-02],
        [ 7.2722e-03,  3.4315e-04, -5.5470e-03],
        [ 1.2097e-03,  1.9398e-05,  4.2221e-03],
        [-3.3007e-03, -1.2019e-02, -5.6651e-03],
        [ 1.5732e-02,  4.8882e-03, -1.0484e-02],
        [ 1.7963e-02,  9.1902e-03,  6.2810e-03],
        [ 1.1460e-02,  2.8640e-03, -1.3416e-02],
        [ 1.6086e-02,  3.7179e-05, -6.6906e-03],
        [ 8.5500e-03,  1.7806e-03, -1.1273e-02],
        [ 1.5161e-03, -1.8609e-02, -9.7842e-03],
        [ 5.8921e-03,  6.3079e-03, -1.1893e-02],
        [-3.3365e-03,  1.7954e-03,  8.2753e-03],
        [ 1.8568e-02,  1.5986e-02,  4.1132e-03],
        [ 8.0229e-03,  1.5431e-02,  5.9492e-04],
        [ 5.0852e-04, -8.7175e-03, -9.5163e-03],
        [ 6.3089e-03,  4.1815e-03,  3.2202e-04],
        [ 1.1296e-02,  9.1359e-03, -7.6795e-03],
        [ 1.1343e-02,  3.2590e-03,  1.4309e-04],
        [-6.6630e-03, -4.9857e-03, -6.7012e-03],
        [ 1.1353e-02, -7.3715e-03, -1.1824e-02],
        [ 1.3596e-02, -5.8683e-03, -1.5019e-03],
        [ 7.5927e-03,  8.5740e-03, -1.1183e-02],
        [ 8.2321e-03,  1.5434e-04, -8.9835e-03],
        [-3.5618e-03,  1.3220e-04,  3.2363e-03],
        [ 3.9079e-03, -6.0343e-03,  1.8853e-03],
        [ 4.5899e-03,  4.3941e-03,  1.4262e-03],
        [-4.2343e-03, -1.9217e-03, -7.5492e-03],
        [ 1.5060e-02,  3.0622e-03, -2.2433e-03],
        [-1.5267e-04, -1.5020e-03, -6.8270e-03],
        [ 8.2113e-03, -5.4267e-04, -1.0742e-02],
        [ 1.9290e-02,  1.0708e-02, -6.1536e-03],
        [-1.1435e-02, -1.1769e-02, -8.2744e-03],
        [ 1.4988e-02,  1.9562e-03, -9.4907e-03],
        [ 5.6514e-03,  7.5600e-03, -1.4158e-02],
        [ 1.9673e-03,  2.4814e-03,  1.6140e-04]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000034
penultimate_layer.0.bias: grad mean = 0.001071
output_layer.0.weight: grad mean = 0.000992
output_layer.0.bias: grad mean = 0.030653
[correct] no_actions is False
task_labels:  tensor([1, 0, 0, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0,
        2, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 2, 2, 1, 0, 1, 1, 0, 1, 1, 2, 1,
        0, 1, 1, 1, 0, 2, 2, 1, 1, 2, 0, 0, 2, 1, 2, 0, 0, 1, 1, 2, 1, 2, 2, 1,
        1, 0, 2, 1, 2, 2, 1, 0, 2, 0, 0, 1, 2, 2, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2,
        0, 0, 0, 2, 2, 0, 1, 1, 2, 1, 1, 0, 2, 1, 1, 2, 0, 2, 1, 2, 0, 0, 0, 2,
        2, 1, 2, 0, 1, 0, 1, 0]) , task_preds: tensor([[ 5.9194e-03, -1.1151e-03, -6.7746e-03],
        [ 9.8584e-03, -1.4622e-03, -5.1922e-03],
        [ 7.9663e-03, -3.1584e-03,  4.1932e-03],
        [ 5.8218e-03, -3.4691e-03, -1.4510e-02],
        [ 2.3207e-03,  1.7956e-03, -4.9563e-03],
        [ 1.0421e-02, -2.5982e-03, -1.1158e-03],
        [-5.7023e-03, -1.6531e-02, -5.4615e-03],
        [ 5.2495e-03,  6.2374e-03, -7.7235e-03],
        [ 5.1894e-03, -2.2839e-03, -1.6041e-03],
        [ 1.9999e-03, -8.2882e-03, -1.1278e-02],
        [ 1.0699e-02,  3.6052e-03, -1.2188e-02],
        [-4.0229e-03, -7.1275e-03, -1.5330e-02],
        [ 9.8957e-04,  1.1312e-03,  2.2109e-03],
        [ 2.7912e-04, -2.4998e-05, -9.6735e-03],
        [ 4.0867e-03,  7.3313e-04,  6.9092e-03],
        [-1.4173e-03,  3.9711e-03, -3.6785e-03],
        [ 1.5728e-02, -7.4139e-03, -5.8852e-03],
        [ 4.9311e-03, -1.1409e-03,  3.2747e-03],
        [ 1.3361e-02,  6.5551e-03, -8.8784e-03],
        [ 1.0287e-02, -8.8984e-03,  1.1060e-03],
        [ 8.4843e-03,  6.9379e-03,  1.9957e-04],
        [ 1.1941e-02, -9.7697e-04,  1.9702e-03],
        [ 1.8141e-02,  2.0118e-03, -3.0423e-03],
        [ 1.0308e-02,  4.2755e-04,  8.3910e-03],
        [ 1.1872e-02, -8.7568e-03,  2.4819e-03],
        [ 1.6524e-02,  7.6288e-03, -1.1311e-02],
        [ 1.6879e-02,  2.1224e-02, -1.5550e-02],
        [ 3.6566e-03,  2.1936e-03, -2.2269e-03],
        [-7.3335e-04,  9.7711e-03, -5.5947e-03],
        [-3.9105e-03,  2.0524e-03, -3.1034e-03],
        [ 2.0838e-02, -7.3759e-03, -3.9017e-03],
        [ 1.2653e-02,  3.4497e-03, -4.3544e-03],
        [ 2.4947e-02, -9.2262e-03, -9.0380e-03],
        [ 8.9238e-03,  7.5036e-03,  1.0973e-02],
        [ 1.1968e-02, -5.6989e-03, -5.1673e-03],
        [ 4.9311e-03, -1.1409e-03,  3.2747e-03],
        [ 2.0081e-02,  3.0050e-03, -2.2943e-03],
        [ 5.8822e-03,  9.4309e-03, -6.8368e-03],
        [-1.6893e-04,  7.4881e-04, -4.4154e-03],
        [ 7.0440e-03,  4.4245e-04,  2.0658e-03],
        [-5.6452e-03, -1.3355e-02,  4.4833e-04],
        [-4.1757e-03,  1.3112e-02, -1.1304e-02],
        [ 1.6212e-04, -6.9280e-04, -1.1726e-02],
        [ 2.0097e-02,  6.6631e-03, -4.6387e-03],
        [ 1.8571e-02,  1.6317e-03, -3.7413e-03],
        [ 8.6029e-03,  7.0376e-03, -1.6751e-02],
        [ 1.1942e-02, -7.2048e-04,  1.2682e-03],
        [-1.0340e-02, -6.9927e-03,  5.9644e-03],
        [ 1.4000e-02, -6.3608e-03, -1.3452e-02],
        [ 3.2739e-03, -1.2858e-02, -1.2719e-02],
        [ 1.3783e-02,  9.0160e-03, -1.3344e-02],
        [-3.1026e-03, -6.3361e-04, -5.0063e-04],
        [ 7.4821e-03, -1.0590e-02,  1.0299e-02],
        [ 8.1514e-03,  3.2611e-03, -9.6396e-03],
        [ 4.7885e-03, -1.2009e-03,  7.6446e-03],
        [ 2.4091e-03,  4.9698e-03, -1.3944e-03],
        [ 7.8479e-03, -3.4898e-03, -4.8108e-03],
        [ 4.9311e-03, -1.1409e-03,  3.2747e-03],
        [ 1.5641e-02, -1.2214e-02, -6.5984e-03],
        [ 3.8217e-03,  4.0990e-03, -5.6909e-03],
        [ 4.6752e-03, -5.8295e-03, -9.4954e-03],
        [ 1.0534e-02,  7.3280e-03, -3.3626e-03],
        [ 3.9688e-03, -1.4167e-03,  1.5344e-03],
        [ 4.9445e-03,  3.3589e-03,  7.8082e-03],
        [ 8.5031e-03,  3.8490e-03, -7.6420e-04],
        [ 3.1360e-03,  4.6555e-03, -4.1781e-03],
        [ 1.2343e-02,  1.6089e-02, -5.2213e-03],
        [-3.9928e-03, -3.0516e-03,  1.9016e-03],
        [-1.4381e-02, -1.0178e-02,  5.7431e-03],
        [ 2.4831e-02,  6.8236e-03, -1.2357e-02],
        [ 3.6895e-03,  2.5365e-03, -5.2051e-03],
        [ 9.5064e-03, -8.4082e-03, -4.2857e-03],
        [ 3.9136e-03,  6.9000e-04, -6.8074e-03],
        [ 4.8023e-03, -6.8927e-03, -1.1050e-02],
        [ 2.9613e-04,  3.7443e-03,  2.9758e-03],
        [-3.8007e-03,  6.2401e-03, -1.9937e-03],
        [ 1.1092e-02, -1.1008e-02, -7.6192e-03],
        [-3.0457e-03,  7.4057e-03, -1.0015e-03],
        [-1.2003e-03,  4.9661e-03, -1.1304e-02],
        [ 4.2191e-03, -5.4183e-03, -1.9537e-05],
        [ 9.0035e-03,  7.2317e-04, -4.4812e-04],
        [ 1.7005e-02,  1.7029e-03,  3.0280e-03],
        [ 7.4330e-03,  5.6873e-03, -1.1243e-02],
        [ 1.1949e-02, -8.2941e-03, -8.2394e-03],
        [ 1.6171e-03, -2.9012e-03, -2.8199e-03],
        [-5.8195e-03, -7.1257e-03, -1.3667e-02],
        [ 9.4858e-03,  6.9748e-05, -1.5215e-03],
        [ 1.2649e-02, -9.0405e-03,  4.1552e-04],
        [ 7.1624e-03, -5.1210e-03, -2.3614e-03],
        [-5.6796e-03, -9.7159e-03, -8.0690e-03],
        [ 3.5406e-03, -2.2281e-04, -1.3969e-02],
        [ 1.7466e-02, -5.9544e-03, -1.8725e-03],
        [ 7.1858e-03, -4.4425e-04, -3.7937e-03],
        [ 2.3506e-03, -4.2700e-03,  3.2496e-03],
        [ 4.2329e-03, -6.6396e-03,  2.2507e-03],
        [-5.6093e-03, -4.5099e-04,  7.6039e-04],
        [-3.8914e-03,  8.3605e-03, -2.7912e-03],
        [ 1.6039e-02, -2.7600e-03, -1.1928e-02],
        [ 6.8015e-03,  2.1460e-03, -9.9291e-03],
        [ 1.4389e-03, -1.9522e-02, -8.7813e-03],
        [-4.0931e-04, -2.0618e-02,  5.0277e-03],
        [ 1.0491e-02,  1.6180e-03, -1.3187e-02],
        [-2.8803e-03,  2.6409e-03, -5.5718e-03],
        [-3.3568e-04,  4.3307e-03, -1.3302e-02],
        [ 7.5328e-03,  2.9692e-03, -4.8467e-03],
        [ 2.9080e-03, -9.1657e-03, -1.7012e-02],
        [ 5.5962e-03, -4.4553e-03, -3.2651e-03],
        [-1.1567e-02, -3.6185e-03, -7.7206e-03],
        [ 2.0477e-02, -2.0594e-03, -9.6353e-03],
        [ 3.9980e-04, -1.1475e-03,  3.9071e-03],
        [ 7.4629e-03, -1.5479e-02, -1.4938e-02],
        [ 4.1349e-03, -4.4882e-03, -8.1159e-03],
        [ 4.9311e-03, -1.1409e-03,  3.2747e-03],
        [ 1.0214e-02, -9.2029e-04, -3.9626e-03],
        [ 1.5474e-03,  1.2444e-02, -7.4061e-03],
        [-1.0363e-03, -2.1576e-03,  3.8896e-03],
        [ 1.5513e-03,  1.1799e-02,  1.1557e-02],
        [ 7.2741e-03, -2.8018e-03, -3.2079e-04],
        [ 2.1164e-02,  7.0859e-03, -1.1449e-02],
        [ 6.5599e-03,  4.3446e-04,  2.6110e-03],
        [ 7.9881e-04,  1.1407e-02,  1.8533e-03],
        [ 3.3111e-03,  5.1272e-03, -6.8164e-03],
        [ 1.0639e-02, -7.7419e-03, -7.6106e-03],
        [ 4.4682e-03, -4.1351e-03, -3.1310e-03],
        [ 1.0255e-02, -4.0359e-03,  4.8228e-03],
        [ 1.7162e-02, -1.2821e-02, -2.2194e-03],
        [ 1.6584e-02, -4.3483e-03, -6.8422e-03],
        [ 1.1798e-02, -5.3472e-03,  5.4854e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000028
penultimate_layer.0.bias: grad mean = 0.000406
output_layer.0.weight: grad mean = 0.000814
output_layer.0.bias: grad mean = 0.011737
[correct] no_actions is False
task_labels:  tensor([0, 2, 1, 0, 0, 2, 0, 2, 0, 0, 2, 1, 0, 1, 0, 1, 2, 0, 1, 0, 1, 0, 1, 2,
        1, 2, 0, 1, 1, 2, 0, 0, 2, 1, 0, 0, 2, 1, 1, 0, 1, 1, 1, 2, 2, 1, 0, 1,
        2, 2, 0, 1, 0, 0, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 0, 1, 2, 1, 2, 2, 2, 0,
        1, 2, 0, 1, 2, 1, 1, 0, 1, 2, 1, 0, 0, 0, 1, 1, 2, 2, 1, 2, 1, 0, 2, 1,
        1, 0, 2, 1, 2, 1, 1, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 1, 1, 0, 1, 2, 0,
        1, 2, 2, 1, 0, 1, 0, 0]) , task_preds: tensor([[ 1.2379e-02, -2.9914e-03, -6.1568e-03],
        [ 6.5162e-03, -1.8680e-03, -6.4171e-03],
        [ 5.6897e-03, -1.7768e-03,  3.3251e-03],
        [ 1.0111e-02,  5.2185e-03, -1.9663e-02],
        [ 7.3636e-03, -1.0098e-02,  1.3666e-03],
        [ 1.0082e-02, -5.1245e-03,  4.1977e-03],
        [ 4.3544e-04,  1.8920e-03, -7.0274e-03],
        [-1.1151e-02, -1.3222e-02, -7.0000e-03],
        [ 9.4916e-03,  4.3210e-04, -9.7479e-04],
        [ 6.2101e-03, -1.9308e-03,  2.9976e-04],
        [ 1.3975e-02, -1.6123e-03,  4.1423e-03],
        [ 4.9553e-03, -1.9383e-03, -1.1310e-03],
        [ 5.6897e-03, -1.7768e-03,  3.3251e-03],
        [ 5.9417e-04, -9.5749e-03, -8.7072e-03],
        [ 1.4977e-02, -4.0101e-03, -2.7296e-03],
        [ 2.2504e-02,  6.0548e-03, -1.0617e-02],
        [ 1.8368e-02, -9.3486e-03, -1.0183e-02],
        [ 1.1934e-02,  2.4400e-03, -3.6027e-03],
        [ 9.9699e-03,  4.3650e-03, -8.6661e-03],
        [ 4.8071e-03,  3.5307e-03, -1.2448e-02],
        [ 8.8635e-03, -1.6174e-03, -1.0158e-02],
        [ 1.3226e-02,  1.1802e-03, -1.4351e-02],
        [-1.1328e-02,  6.2176e-03,  1.0514e-04],
        [ 4.0361e-03,  9.4827e-04,  5.4310e-03],
        [ 5.6897e-03, -1.7768e-03,  3.3251e-03],
        [ 7.3922e-03,  4.7838e-03,  3.1099e-03],
        [ 1.9719e-02,  6.4575e-03, -1.0081e-02],
        [ 1.5312e-02,  6.5406e-03, -5.0388e-03],
        [ 6.9239e-03,  3.2926e-03, -1.1421e-02],
        [ 6.0971e-03, -5.1290e-03, -7.8503e-03],
        [ 1.1559e-02,  5.0645e-03, -9.7363e-03],
        [ 8.0447e-03, -6.0149e-03,  5.5992e-04],
        [ 1.2414e-02, -2.5422e-03, -1.8060e-02],
        [ 2.9264e-03,  3.8403e-03, -1.1707e-02],
        [ 2.1995e-02,  9.3497e-03,  1.9974e-05],
        [ 7.5408e-03,  1.9302e-03,  2.1559e-03],
        [ 1.5259e-02,  5.6632e-03, -4.4648e-03],
        [ 3.4424e-03, -3.0893e-04,  6.1440e-03],
        [ 1.2579e-02, -2.8175e-03, -1.2612e-03],
        [ 3.7619e-03,  1.4886e-03, -9.2653e-04],
        [ 5.0651e-03, -6.7430e-03,  3.4401e-03],
        [ 1.3318e-02,  1.0227e-02, -4.1308e-03],
        [ 7.3438e-03, -1.2379e-02,  1.2387e-03],
        [ 4.8971e-03, -4.7500e-04, -1.3325e-02],
        [ 8.3437e-03, -2.9649e-03, -2.9494e-03],
        [ 8.2012e-03,  3.6074e-03,  3.9334e-03],
        [-5.8022e-04, -1.0752e-04,  1.5229e-03],
        [-1.4768e-02, -1.2058e-02,  2.8375e-03],
        [ 1.0665e-02, -2.8838e-03, -5.7627e-03],
        [ 9.8491e-03, -2.8650e-03, -3.7910e-03],
        [-1.9609e-03, -1.0119e-03, -3.8093e-03],
        [ 4.7134e-03, -7.1955e-03, -7.7834e-03],
        [ 8.9804e-03,  1.4357e-03,  2.0528e-03],
        [ 7.8728e-03,  1.2456e-02, -3.0730e-03],
        [ 1.3523e-02, -3.5375e-03, -9.5861e-03],
        [ 1.9494e-04, -2.0406e-03, -1.8334e-03],
        [-1.1249e-02,  1.1256e-02, -2.2887e-03],
        [ 1.0614e-02, -1.2829e-03, -4.8689e-03],
        [ 7.7027e-03, -4.9396e-03,  3.6663e-03],
        [ 1.0542e-02, -7.9549e-03, -9.9563e-03],
        [ 3.8031e-03,  2.1953e-03, -2.4958e-03],
        [ 9.9736e-03, -1.2191e-03,  2.7304e-03],
        [ 1.1226e-02, -1.8129e-03,  4.8571e-03],
        [-1.8705e-03, -1.4597e-03, -4.5585e-04],
        [ 1.0779e-02,  6.9193e-03, -4.8595e-03],
        [ 1.8824e-02, -7.3569e-03, -1.1959e-02],
        [ 1.3756e-02,  3.7353e-03, -1.4298e-02],
        [ 1.6889e-02, -7.3340e-03, -3.1552e-03],
        [ 1.6097e-02,  8.8355e-03, -6.6670e-03],
        [ 6.7574e-03,  3.4945e-03, -1.7251e-03],
        [ 2.9195e-03, -7.7801e-03,  6.9488e-04],
        [ 1.3034e-02,  8.2777e-03, -1.0172e-02],
        [ 3.2610e-03,  7.9133e-04, -8.6790e-03],
        [ 2.4448e-03, -7.9781e-03, -4.3303e-03],
        [ 1.1618e-02, -1.3651e-03, -1.6765e-02],
        [-2.9580e-03, -8.3357e-04, -1.0039e-02],
        [ 1.3017e-02, -1.1489e-02,  1.3718e-03],
        [ 8.6673e-03, -5.9222e-05, -1.2559e-02],
        [ 9.3500e-03,  4.6672e-03, -1.0756e-02],
        [ 6.1265e-03,  6.7075e-05,  1.3458e-03],
        [ 1.6924e-02, -1.0828e-02, -1.5921e-03],
        [-2.3009e-04,  2.3366e-03, -1.3185e-03],
        [ 1.9265e-03, -2.1014e-03, -1.3604e-02],
        [ 1.9993e-02, -4.4204e-03, -2.1880e-02],
        [ 3.6012e-03, -3.1393e-03, -1.4030e-03],
        [ 1.0120e-02, -1.4865e-03,  1.5382e-03],
        [ 4.8888e-03,  8.3064e-03,  4.6007e-03],
        [ 6.7917e-04, -4.4408e-03, -1.6821e-03],
        [ 7.7995e-03, -4.7729e-03, -3.9624e-03],
        [ 8.7532e-03, -7.1734e-05,  2.6561e-03],
        [ 1.7332e-02,  5.8802e-03, -4.8653e-03],
        [ 8.0638e-03, -1.8781e-03, -2.8999e-03],
        [ 8.1110e-04,  1.3495e-03, -1.6763e-02],
        [ 4.9308e-03, -1.2565e-03, -5.5182e-03],
        [-1.4377e-02, -2.5853e-03, -1.0351e-02],
        [ 6.6974e-03, -9.8580e-03, -5.4016e-03],
        [ 8.8311e-03,  8.5601e-03, -4.6377e-03],
        [ 7.1533e-03,  3.9606e-03, -5.6280e-03],
        [-9.4158e-04, -2.2471e-03, -1.2237e-02],
        [ 3.7218e-03, -8.6319e-03, -1.3897e-02],
        [ 1.8540e-02,  1.3238e-03, -8.4402e-03],
        [ 8.8412e-04, -6.8904e-04, -1.6843e-03],
        [ 4.0926e-03, -2.7789e-03,  1.4017e-02],
        [ 5.6897e-03, -1.7768e-03,  3.3251e-03],
        [ 4.7090e-03,  1.0135e-02,  3.0313e-03],
        [ 5.6897e-03, -1.7768e-03,  3.3251e-03],
        [-8.0185e-04, -9.1169e-03,  2.2943e-05],
        [ 1.3740e-03, -1.3848e-03, -6.4330e-03],
        [ 1.6949e-03, -1.0069e-02, -7.3572e-03],
        [ 1.3509e-03, -7.3897e-03,  1.0633e-03],
        [ 4.3086e-03,  7.2539e-03, -6.8183e-03],
        [-7.3274e-03, -8.1356e-03, -1.6785e-02],
        [ 7.0284e-03, -1.6881e-03,  1.3960e-03],
        [-1.3319e-03, -1.0371e-02,  3.9411e-03],
        [ 1.0801e-03, -5.7164e-03, -4.0875e-03],
        [ 6.1523e-03,  8.3475e-03, -1.0310e-02],
        [-1.1066e-02, -5.3690e-03,  2.6395e-03],
        [-1.1603e-03, -2.4126e-03,  5.0091e-03],
        [-1.4269e-02, -4.6312e-03,  2.4360e-03],
        [ 2.1347e-02, -8.5006e-04,  4.6590e-03],
        [ 1.1375e-02,  4.9665e-03,  1.5613e-03],
        [ 6.3655e-03, -1.1257e-02,  1.6125e-03],
        [ 5.6897e-03, -1.7768e-03,  3.3251e-03],
        [-3.4867e-03,  5.3179e-03, -4.3748e-03],
        [-3.2025e-03, -2.3640e-03, -9.2937e-03],
        [ 1.0668e-02, -2.7981e-03, -7.1638e-03],
        [ 4.2072e-03,  1.9031e-04,  1.6385e-05],
        [ 6.0413e-03, -8.4893e-03, -1.0320e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000035
penultimate_layer.0.bias: grad mean = 0.001149
output_layer.0.weight: grad mean = 0.001024
output_layer.0.bias: grad mean = 0.032431
[correct] no_actions is False
task_labels:  tensor([2, 1, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 1, 2, 2, 0,
        0, 0, 0, 2, 0, 0, 1, 2, 1, 0, 2, 1, 2, 1, 2, 2, 2, 2, 2, 0, 1, 1, 2, 2,
        1, 1, 2, 1, 2, 2, 0, 1, 0, 1, 0, 2, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 2, 2, 2, 2, 0, 2, 1, 2, 0, 2, 2, 2, 1, 1, 0, 1, 1, 1, 2,
        1, 0, 0, 2, 1, 0, 1, 1, 1, 0, 2, 2, 0, 2, 1, 0, 0, 2, 2, 0, 0, 1, 2, 2,
        1, 1, 1, 0, 1, 1, 2, 2]) , task_preds: tensor([[ 7.9500e-03, -5.7387e-03, -3.0907e-03],
        [-4.4590e-03, -1.1466e-02,  1.4267e-02],
        [ 1.2791e-02,  8.3680e-04, -6.3790e-04],
        [ 2.8716e-03, -7.8163e-04, -3.7240e-03],
        [ 7.2580e-03,  8.2043e-03,  4.5137e-04],
        [ 1.0871e-02, -2.0022e-04, -1.2346e-03],
        [ 5.0912e-03, -3.0698e-03,  8.0160e-07],
        [ 2.2018e-02, -8.7347e-03, -3.4360e-03],
        [ 2.8415e-03, -3.5147e-03,  6.6101e-03],
        [ 1.4004e-02,  1.8933e-03, -5.3181e-03],
        [ 8.9127e-03, -9.0280e-04, -4.6793e-03],
        [ 1.1791e-02, -9.8416e-03, -9.6085e-03],
        [ 2.3236e-03,  8.9276e-03, -8.1511e-03],
        [ 1.8874e-02,  2.3362e-03, -4.9054e-03],
        [ 7.9491e-04, -3.7735e-03, -5.1093e-04],
        [ 1.0990e-02,  5.7073e-03, -3.2340e-03],
        [ 5.2334e-03,  2.3021e-03, -3.6619e-03],
        [-4.7242e-03,  3.5533e-03,  7.1180e-03],
        [ 8.6016e-03, -6.2670e-03, -4.0070e-03],
        [ 4.3315e-03, -8.0880e-04,  9.4804e-04],
        [ 9.5948e-04, -6.6115e-03, -8.4852e-03],
        [ 6.1485e-03, -2.3500e-03,  3.5457e-03],
        [ 9.3118e-03, -6.9051e-04,  2.8445e-03],
        [ 1.0692e-02,  7.7569e-04, -2.2046e-03],
        [ 1.3541e-02, -4.8113e-04,  2.1456e-03],
        [-1.9082e-03, -1.0083e-02, -2.2042e-02],
        [ 6.0103e-03, -1.3242e-02, -1.7790e-03],
        [ 4.4697e-04, -2.1881e-02,  5.6474e-03],
        [ 8.6250e-03, -1.1878e-02,  1.0710e-02],
        [ 4.0057e-03, -8.1421e-03,  5.1497e-03],
        [ 7.2074e-03,  2.4237e-03, -1.0754e-02],
        [-2.9430e-03,  2.9647e-03,  5.4828e-03],
        [ 1.5517e-02, -3.8148e-03, -2.0974e-03],
        [ 1.7980e-02, -4.6722e-03, -6.9611e-03],
        [-3.2495e-05, -8.3696e-03,  3.6429e-03],
        [ 5.3695e-04,  3.6104e-03, -1.0657e-02],
        [-4.0771e-03,  5.5009e-03,  3.0305e-03],
        [ 1.5643e-02, -5.9392e-03, -4.0547e-03],
        [ 1.5685e-02,  5.3969e-03, -4.1039e-03],
        [ 6.4838e-03, -2.8064e-03, -1.5028e-03],
        [ 5.7562e-03,  2.0692e-03,  7.0307e-04],
        [-4.4089e-03, -1.3451e-04,  2.4676e-03],
        [-2.2658e-03, -1.8211e-03, -5.7616e-03],
        [ 1.3493e-02,  7.5251e-03, -9.7630e-03],
        [-9.2602e-05, -6.8549e-03, -6.4115e-03],
        [ 7.0430e-03, -9.6547e-03,  1.6063e-03],
        [ 2.2597e-02,  7.1712e-04, -4.6725e-03],
        [ 1.6176e-02,  9.0654e-04, -2.5241e-03],
        [ 6.3330e-03, -3.2199e-03, -1.5600e-03],
        [ 1.6582e-02, -3.8665e-03, -2.4354e-03],
        [ 1.0768e-02, -2.5163e-03, -2.7633e-03],
        [-2.1785e-03,  1.3807e-03, -4.8383e-03],
        [ 1.7683e-03, -1.0560e-02, -6.9138e-03],
        [ 1.8615e-02,  3.8073e-04, -7.5298e-03],
        [ 1.0169e-02, -1.3173e-03, -6.3644e-03],
        [ 1.2998e-02, -3.2482e-03, -1.1579e-03],
        [ 4.8866e-03,  5.9478e-03,  1.2124e-02],
        [-1.1322e-02,  5.7045e-03,  6.1872e-04],
        [ 1.2969e-02, -2.1136e-03, -1.3187e-04],
        [ 3.7882e-04, -7.9426e-03,  8.9296e-04],
        [ 1.0286e-02,  3.9554e-03, -4.2402e-03],
        [ 2.6543e-03, -7.4193e-03,  7.5895e-03],
        [ 1.5208e-03, -9.7772e-04, -3.2269e-03],
        [-3.9151e-03, -5.6179e-03, -7.8781e-03],
        [ 3.1164e-03, -4.9049e-04, -2.4490e-03],
        [ 4.6261e-03,  9.3797e-04, -1.7110e-03],
        [ 9.5920e-03, -3.2036e-03,  2.9378e-03],
        [ 1.2547e-02,  2.0180e-03, -6.8417e-04],
        [ 2.1453e-03, -1.5834e-03,  4.7654e-03],
        [ 9.5238e-03,  7.4633e-04,  8.4094e-03],
        [ 6.0414e-03,  2.9152e-04, -5.6751e-03],
        [ 2.1247e-03,  3.9435e-03, -9.2280e-03],
        [ 1.0083e-03, -6.5146e-03, -3.2102e-03],
        [ 1.1235e-02, -4.0223e-03,  1.0017e-02],
        [ 5.7054e-03,  2.9164e-03,  3.6846e-03],
        [ 3.6931e-03, -3.6956e-04,  1.5802e-03],
        [-1.1070e-02, -3.6644e-03, -7.8406e-03],
        [ 7.1132e-03,  5.9521e-03, -1.6657e-03],
        [ 5.0830e-03,  8.1277e-03, -1.6799e-02],
        [ 9.9805e-03, -3.7536e-03, -2.8911e-04],
        [ 2.1427e-03,  2.3396e-03, -1.9416e-03],
        [-6.3347e-03,  1.0424e-02, -3.3328e-03],
        [ 8.4733e-03, -2.9296e-03,  5.9792e-04],
        [ 8.0566e-03, -2.2038e-03,  1.5833e-03],
        [ 7.0233e-03, -5.1774e-03, -4.1261e-03],
        [ 1.3397e-02, -1.1759e-02,  3.8343e-03],
        [ 7.2186e-03,  5.7146e-03,  5.9245e-03],
        [ 8.1023e-03, -2.5485e-03, -2.2474e-03],
        [-1.3561e-03,  1.5891e-02, -5.6648e-03],
        [ 1.2137e-02,  3.8889e-03, -9.3342e-03],
        [ 4.3878e-03, -6.3050e-04, -5.0585e-04],
        [ 1.6382e-02, -8.6493e-04, -7.0299e-03],
        [ 1.8293e-02,  2.1414e-03, -9.6709e-03],
        [ 3.5588e-03, -5.3837e-03, -9.3146e-03],
        [ 1.2466e-02, -5.0261e-04, -7.5266e-03],
        [-7.0406e-03,  1.3635e-03, -3.6457e-03],
        [ 5.1459e-03,  8.4986e-04, -1.1430e-02],
        [ 6.8422e-03,  3.1973e-03,  1.2595e-03],
        [ 1.8661e-02, -7.3382e-03, -1.4005e-03],
        [ 1.0538e-02, -2.7622e-03, -1.9904e-03],
        [ 2.0788e-02,  1.1488e-02, -9.0543e-03],
        [ 9.5616e-03,  8.4245e-04,  2.1968e-03],
        [-1.2831e-03,  2.9504e-03, -1.1442e-02],
        [-7.3686e-03, -4.6194e-03, -4.9720e-03],
        [ 9.0894e-03, -3.3927e-03, -1.0678e-02],
        [ 1.7729e-02,  6.1647e-03, -4.5423e-03],
        [ 3.3603e-03,  1.2077e-03, -4.4929e-03],
        [ 1.0999e-02, -3.7020e-03, -5.1831e-03],
        [-1.5840e-03, -1.4008e-02, -4.9731e-03],
        [ 1.2687e-02,  1.3417e-02,  4.1348e-04],
        [ 1.9661e-02, -1.4537e-03, -4.0537e-03],
        [ 2.7892e-03, -3.4311e-03, -4.9022e-03],
        [ 1.7269e-02, -9.4100e-03, -7.4821e-03],
        [ 1.2301e-02,  8.1161e-04, -2.4415e-03],
        [-4.6230e-03,  6.1469e-03,  2.4472e-03],
        [ 6.4520e-03, -2.8931e-03,  1.0980e-03],
        [ 8.5125e-03, -4.1271e-03,  1.4520e-03],
        [ 1.1383e-02, -9.6128e-03,  6.9802e-03],
        [ 7.0306e-04,  1.3314e-03, -4.3135e-03],
        [ 6.9220e-03, -3.0064e-03,  3.2216e-03],
        [ 6.0060e-03,  6.6940e-03, -1.3573e-02],
        [ 3.2772e-03, -4.9960e-03, -2.2741e-03],
        [ 1.1100e-02,  6.3453e-03, -2.8103e-03],
        [ 1.1869e-02, -4.5371e-04,  6.5572e-03],
        [-7.8719e-04, -3.0626e-03,  5.3784e-03],
        [-2.7782e-03, -5.1263e-03,  3.3778e-03],
        [-2.2349e-03,  1.0206e-02, -6.5235e-03],
        [-7.7610e-03, -9.1718e-03, -1.4961e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000033
penultimate_layer.0.bias: grad mean = 0.000860
output_layer.0.weight: grad mean = 0.001038
output_layer.0.bias: grad mean = 0.024807
[correct] no_actions is False
task_labels:  tensor([1, 2, 2, 1, 1, 0, 0, 1, 1, 2, 1, 0, 2, 2, 2, 1, 0, 2, 1, 0, 2, 1, 1, 1,
        0, 2, 1, 0, 1, 0, 2, 1, 2, 0, 2, 0, 0, 1, 2, 1, 0, 2, 2, 0, 0, 1, 2, 2,
        0, 0, 1, 0, 0, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 0, 0, 0, 2, 0, 2, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 0, 2, 1, 1,
        1, 1, 2, 0, 0, 0, 0, 1, 2, 2, 1, 2, 2, 2, 0, 2, 1, 0, 2, 0, 1, 2, 0, 2,
        2, 1, 0, 2, 1, 0, 1, 1]) , task_preds: tensor([[ 6.3197e-03, -5.8028e-03,  1.4108e-03],
        [ 1.9289e-02, -1.4903e-02, -1.1230e-02],
        [ 4.8789e-03, -2.1341e-03, -1.7023e-02],
        [ 7.3392e-03, -1.0859e-02, -4.8984e-03],
        [ 1.0644e-02, -4.7634e-03,  6.2054e-03],
        [ 1.0824e-02, -5.4565e-03,  8.4096e-03],
        [ 1.4274e-02, -4.2011e-03, -6.9290e-03],
        [ 4.2956e-03,  3.4757e-03, -7.3279e-03],
        [-3.1682e-03,  9.7150e-04, -4.3340e-03],
        [ 1.8327e-02, -1.0420e-02,  1.8605e-03],
        [-8.8562e-03, -3.6569e-03,  5.3865e-04],
        [ 1.5314e-02,  8.1229e-04,  1.6973e-03],
        [ 1.1615e-02, -1.4493e-03,  3.5878e-03],
        [ 9.3898e-03,  7.4092e-04, -8.0138e-03],
        [ 2.6427e-02,  4.1371e-03, -1.0854e-02],
        [ 6.8428e-03, -1.2989e-02,  3.7107e-03],
        [ 4.6117e-03,  5.8546e-03,  6.0175e-03],
        [ 1.9027e-02, -1.1134e-02, -8.8685e-03],
        [ 1.6280e-02,  3.7901e-03, -1.4472e-02],
        [ 8.8560e-03,  2.8666e-03,  9.6087e-03],
        [ 6.2853e-04, -7.9533e-03, -6.1374e-03],
        [ 1.8927e-03, -7.0092e-03, -2.1705e-03],
        [ 3.5287e-03, -4.2084e-03, -9.7501e-03],
        [ 1.1416e-02,  3.2822e-03, -8.1320e-04],
        [ 8.5933e-03,  1.0701e-02, -1.8301e-03],
        [ 1.4063e-02,  5.3392e-04, -7.8422e-03],
        [ 7.7016e-03, -6.6459e-03, -1.2367e-03],
        [ 4.1409e-05, -8.5853e-04, -5.4687e-03],
        [-8.4715e-03, -1.0709e-02,  5.9259e-03],
        [ 9.7433e-03,  4.7645e-03, -7.0851e-03],
        [ 1.2449e-02, -4.7676e-06, -1.0276e-02],
        [ 5.7482e-03,  9.2230e-04, -3.6765e-03],
        [-8.4977e-04, -3.9223e-03, -1.0586e-02],
        [-3.3880e-03,  3.2844e-04, -1.2385e-02],
        [-3.9820e-03, -7.5737e-03, -1.0002e-02],
        [ 6.7260e-03, -6.2269e-03, -1.0339e-02],
        [-8.8237e-04, -4.0902e-03, -8.6971e-03],
        [ 7.7631e-03,  5.7808e-03,  8.6591e-03],
        [ 9.9147e-03, -1.1310e-03, -7.9789e-04],
        [ 2.8440e-03,  1.3151e-03,  5.0179e-03],
        [ 7.7916e-03, -1.3460e-02, -5.3796e-03],
        [ 1.8126e-02, -4.0468e-03,  2.6042e-03],
        [ 8.6886e-03, -6.2805e-03, -5.3873e-03],
        [ 6.5926e-03, -1.2210e-02,  6.5179e-04],
        [ 1.8686e-02, -6.4870e-04, -1.9047e-03],
        [ 4.3657e-03,  6.5989e-04, -4.7303e-03],
        [-1.6278e-03, -3.1810e-03, -2.8437e-03],
        [ 1.2969e-02,  9.5970e-04, -1.4389e-02],
        [ 1.5654e-03, -7.6188e-03, -8.4377e-03],
        [ 2.1761e-02,  4.5044e-03, -3.7319e-03],
        [ 6.8779e-04, -3.1828e-03, -9.5720e-03],
        [ 9.7522e-03,  2.1577e-04, -2.3595e-03],
        [ 7.7804e-03, -1.3268e-03,  4.3111e-04],
        [-3.0159e-05, -8.8280e-03,  1.4834e-03],
        [ 1.2073e-02, -4.1548e-03, -9.9862e-04],
        [ 3.3895e-03, -3.0737e-03, -9.2026e-03],
        [-9.4785e-03, -5.1745e-03,  5.6048e-03],
        [ 1.3158e-02, -4.1750e-03, -1.6972e-02],
        [ 7.1088e-03,  2.5807e-04, -6.5523e-03],
        [ 6.5114e-03, -3.2074e-03,  4.1397e-03],
        [ 1.7431e-02, -8.6723e-03, -2.2143e-03],
        [ 2.4021e-02,  3.7750e-03,  3.6124e-03],
        [ 1.6157e-02, -2.2881e-03,  1.9548e-04],
        [ 2.0386e-02, -4.5358e-03,  8.2764e-03],
        [ 4.5187e-03, -4.5348e-03, -7.0221e-04],
        [ 9.0849e-03, -4.5516e-03,  4.5908e-03],
        [ 2.1606e-02, -7.2758e-04, -4.0016e-03],
        [ 2.3225e-02,  4.8041e-03, -1.0724e-02],
        [ 1.0909e-02, -9.9691e-03, -5.5413e-03],
        [ 7.3167e-03,  3.3058e-04, -5.4026e-03],
        [ 2.4372e-03, -3.8415e-03, -1.1217e-02],
        [-5.7135e-03, -7.1613e-03, -5.2285e-03],
        [-9.9629e-03, -5.9103e-03, -6.6127e-03],
        [ 1.3747e-02,  3.8742e-03,  6.8978e-03],
        [ 1.6145e-02, -1.1589e-03, -7.9320e-03],
        [ 1.8719e-02, -4.2072e-03, -1.0692e-02],
        [ 9.0418e-03, -1.0089e-03,  9.4974e-04],
        [-6.9745e-04,  6.6497e-03, -9.1166e-03],
        [ 9.4961e-03,  5.3057e-05, -4.6791e-03],
        [ 9.1773e-03, -1.6077e-03, -7.9388e-03],
        [ 1.0379e-02,  3.3988e-03, -1.3662e-02],
        [ 1.5229e-02,  7.2920e-03, -2.3505e-03],
        [ 2.9279e-03,  3.4448e-03, -1.1544e-02],
        [ 5.4980e-04,  3.6785e-03, -2.8780e-03],
        [ 1.0834e-02,  2.5090e-03, -6.5802e-03],
        [ 1.1762e-02,  3.3439e-03, -8.6911e-03],
        [ 6.0433e-03, -2.7750e-04, -9.1231e-03],
        [ 1.4792e-02,  7.7277e-03, -1.2835e-02],
        [ 1.0396e-02, -9.2786e-03, -1.6631e-02],
        [ 4.7615e-03, -7.7728e-04, -1.5857e-02],
        [ 1.1862e-02, -2.8636e-03, -1.6901e-02],
        [-8.0002e-04,  3.4716e-03,  4.4840e-03],
        [ 1.4368e-03, -5.4418e-03, -9.8476e-03],
        [ 8.2526e-04, -4.9734e-03, -2.1110e-03],
        [-1.3705e-03, -1.2433e-02, -7.2477e-03],
        [ 1.3580e-02, -6.4895e-03, -4.8392e-03],
        [ 4.0165e-03, -1.6030e-03,  1.3347e-03],
        [ 1.1165e-02, -3.0920e-03,  2.9833e-03],
        [ 1.7003e-02, -1.0019e-03,  8.2029e-03],
        [ 6.3574e-03, -5.1118e-03,  8.1491e-04],
        [ 2.5480e-03, -1.1589e-02, -3.3157e-03],
        [ 1.0132e-02,  1.4596e-03, -6.4590e-03],
        [-3.1418e-03, -1.2030e-02, -1.8784e-04],
        [-6.6432e-03, -6.7773e-03, -8.2923e-03],
        [ 1.7684e-02,  2.9200e-03, -2.5207e-04],
        [ 1.0644e-02, -6.1436e-03, -6.4193e-03],
        [ 1.1270e-02, -4.5589e-03, -5.8259e-03],
        [ 1.1554e-02,  2.5832e-03, -7.5987e-03],
        [ 1.0947e-02, -2.4475e-03,  2.4903e-03],
        [ 5.9254e-03, -3.6870e-03,  9.3109e-03],
        [ 2.5378e-02, -5.8342e-04, -7.1801e-03],
        [ 1.1213e-02, -3.0311e-03, -1.0106e-03],
        [ 1.5649e-03, -1.9924e-03, -1.6124e-02],
        [ 2.0801e-02, -6.0075e-03, -2.0888e-02],
        [ 1.6397e-02,  3.2335e-03, -9.1539e-03],
        [ 1.4777e-02, -5.4715e-03, -3.2860e-03],
        [ 1.7615e-02, -1.2020e-02, -9.2395e-04],
        [ 1.6630e-02,  6.9186e-03, -5.1292e-03],
        [ 1.3811e-02, -1.3360e-02,  1.3066e-04],
        [-2.3235e-03, -5.2382e-03, -3.8102e-03],
        [ 1.2188e-02, -1.0979e-02,  4.5084e-03],
        [ 2.0985e-02,  1.0277e-02,  1.5501e-03],
        [ 9.4194e-03, -7.1889e-03, -5.4356e-03],
        [ 1.1669e-03, -1.1308e-02, -1.6651e-03],
        [ 2.8362e-03, -7.0127e-04, -1.6108e-03],
        [ 9.7537e-03,  1.5878e-03,  5.7243e-04],
        [-1.0436e-03, -2.6454e-03,  9.8664e-05],
        [-9.1338e-04, -8.0068e-04, -7.7497e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000032
penultimate_layer.0.bias: grad mean = 0.000639
output_layer.0.weight: grad mean = 0.000964
output_layer.0.bias: grad mean = 0.021887
[correct] no_actions is False
task_labels:  tensor([2, 0, 1, 1, 1, 2, 1, 1, 1, 0, 0, 2, 1, 2, 1, 0, 1, 1, 1, 1, 0, 1, 2, 0,
        0, 1, 1, 1, 2, 2, 0, 0, 1, 0, 0, 2, 0, 2, 0, 2, 1, 1, 0, 2, 0, 1, 2, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 2, 1, 0, 0, 2, 2, 0, 2, 2, 1, 1, 0, 2,
        0, 1, 0, 1, 1, 2, 1, 1, 2, 0, 2, 2, 0, 1, 1, 1, 0, 2, 1, 1, 2, 0, 1, 1,
        0, 2, 1, 0, 2, 0, 0, 1, 1, 2, 0, 1, 2, 1, 0, 1, 2, 1, 2, 2, 0, 0, 2, 1,
        0, 1, 1, 1, 1, 2, 1, 2]) , task_preds: tensor([[-1.9712e-03, -8.1279e-03, -4.0217e-03],
        [ 1.5024e-02, -1.2282e-02,  1.8790e-03],
        [ 1.9049e-05, -4.4136e-03,  6.1270e-03],
        [ 1.5031e-02,  7.3017e-03, -1.2591e-02],
        [ 3.1023e-03, -1.6223e-03, -8.7236e-04],
        [-3.1520e-03, -8.7378e-03,  5.7726e-03],
        [ 5.3346e-03, -5.8503e-03, -9.3288e-03],
        [ 5.3662e-03, -4.6723e-04, -8.5246e-04],
        [-4.1956e-05,  5.5932e-05, -5.2207e-03],
        [-3.7649e-03,  1.9789e-03,  7.9821e-03],
        [ 9.2028e-03, -1.3725e-03,  5.7274e-03],
        [ 1.2187e-02, -4.9422e-04, -3.6232e-03],
        [-1.3915e-02, -1.4115e-02,  4.2790e-03],
        [ 2.9295e-03,  5.4750e-04, -7.1434e-04],
        [-1.1590e-03, -1.3075e-02, -6.7521e-03],
        [ 1.2655e-02, -3.9830e-04, -9.8295e-03],
        [ 4.8890e-03,  6.7260e-03,  6.2065e-03],
        [ 8.9147e-03,  2.2814e-04, -5.0945e-04],
        [ 1.4173e-02, -1.6536e-02,  1.1075e-03],
        [ 3.5812e-03,  1.1194e-02, -9.4957e-03],
        [ 2.1217e-02, -6.7451e-03, -2.0458e-02],
        [ 6.6891e-03, -8.8720e-04, -4.9749e-03],
        [ 1.2786e-02,  1.2130e-02,  1.6500e-03],
        [ 1.9505e-02,  2.5557e-04, -7.6339e-03],
        [ 1.8268e-02,  3.4998e-03, -8.4486e-03],
        [ 6.8013e-03, -3.8747e-03,  4.5942e-03],
        [ 6.1779e-03,  2.3230e-03, -8.0008e-03],
        [ 2.2591e-02, -1.7911e-02,  2.4694e-03],
        [ 2.8238e-03,  7.3617e-03, -3.5886e-03],
        [ 1.8180e-02, -2.8726e-03, -7.3152e-03],
        [ 2.8542e-03, -3.6646e-03, -1.9405e-03],
        [ 5.2544e-03, -5.7400e-03, -8.8167e-03],
        [ 1.3306e-02, -6.0213e-03,  1.5620e-03],
        [-5.1349e-04, -1.5436e-02, -4.3473e-03],
        [ 1.3340e-03, -6.3463e-03, -2.9829e-03],
        [ 2.6239e-03, -2.3207e-02, -5.9213e-03],
        [-1.0021e-03, -3.0614e-03, -2.4753e-03],
        [ 1.2598e-03,  5.8021e-03,  5.2484e-03],
        [ 1.2157e-02, -5.0965e-03,  5.8586e-03],
        [ 7.0779e-03, -1.7388e-03, -2.4729e-04],
        [ 7.3361e-03, -3.8957e-03, -3.9212e-03],
        [ 1.6299e-02, -7.9463e-03, -7.6051e-03],
        [ 1.0233e-02,  3.6677e-03,  3.4752e-03],
        [ 1.2112e-02, -7.7902e-03, -4.5256e-03],
        [ 1.0478e-02, -5.1130e-04, -2.1826e-03],
        [ 2.1526e-02,  3.7285e-03,  5.2548e-04],
        [ 1.1889e-03, -2.3307e-02,  6.5314e-03],
        [ 8.0870e-03,  5.2038e-03,  8.9957e-03],
        [ 1.7713e-03, -5.2870e-03, -1.1196e-02],
        [ 1.8499e-02, -1.0816e-02, -7.0167e-03],
        [ 1.3203e-02, -1.2047e-02,  2.0346e-03],
        [ 1.2686e-02,  2.4643e-03, -7.1287e-03],
        [ 9.1358e-03, -5.3309e-03,  7.8888e-04],
        [ 6.8013e-03, -3.8747e-03,  4.5942e-03],
        [ 9.6196e-03, -2.9338e-03, -2.1834e-03],
        [ 1.1250e-02, -3.9530e-03,  3.8037e-03],
        [-8.7329e-04, -4.7190e-03, -9.7458e-03],
        [ 7.6963e-03,  3.9184e-03, -1.4216e-02],
        [ 1.2524e-02,  7.9267e-03,  5.5863e-03],
        [ 3.6366e-03,  7.1468e-03,  4.1213e-03],
        [ 1.6136e-02,  2.2987e-04, -1.8256e-03],
        [ 1.4857e-02,  2.6861e-03, -1.1308e-02],
        [ 1.3821e-02, -1.3849e-03, -9.9640e-03],
        [ 1.1894e-02,  7.6320e-03, -1.0762e-02],
        [ 2.6862e-02,  3.2596e-03, -1.0293e-02],
        [ 1.0212e-02, -1.1914e-02, -5.3350e-03],
        [ 1.2662e-02,  7.1188e-03, -1.2320e-02],
        [-1.8683e-03, -7.1682e-03,  3.3729e-03],
        [ 1.3767e-03, -7.6403e-03, -7.7628e-03],
        [ 1.5575e-02, -8.2083e-03, -6.6997e-03],
        [ 5.8035e-03, -4.8256e-03,  1.5666e-03],
        [ 9.3129e-03, -3.3704e-03,  8.3380e-03],
        [ 5.6730e-03, -6.0467e-03, -7.2071e-03],
        [ 9.8677e-03, -3.1226e-03,  3.1897e-03],
        [ 2.6603e-03, -3.8762e-03,  4.1079e-03],
        [-1.2017e-03, -2.0099e-03,  2.0873e-03],
        [ 1.1479e-02, -5.3798e-03, -5.1457e-03],
        [ 1.0974e-02, -4.0704e-03, -1.3305e-03],
        [ 1.5062e-02, -7.7969e-03,  1.1759e-03],
        [ 4.9562e-04, -2.6429e-03, -7.8383e-03],
        [ 1.5524e-02, -5.3269e-03, -1.1302e-02],
        [ 5.1403e-03, -8.3070e-03, -5.5783e-03],
        [ 8.5118e-03, -1.9434e-03,  3.4890e-03],
        [ 1.3021e-02, -4.0940e-03,  3.0150e-03],
        [ 1.9153e-02, -1.4762e-03, -1.4229e-03],
        [-4.1590e-03, -7.0944e-03, -8.6500e-03],
        [ 1.5385e-02,  6.6730e-03, -1.8388e-03],
        [ 3.3326e-03, -7.2030e-03,  3.7225e-03],
        [ 1.5383e-02, -2.1758e-03,  4.3354e-03],
        [ 2.1359e-02,  4.4833e-05, -5.2718e-04],
        [ 3.2568e-03, -1.1434e-03, -5.0016e-03],
        [ 3.8961e-03, -6.3127e-03, -1.4195e-03],
        [ 1.6885e-02,  6.0160e-03, -4.4054e-03],
        [ 1.4495e-02, -2.0927e-02, -1.5582e-03],
        [ 2.1124e-02, -2.7404e-03, -1.2378e-02],
        [ 6.9062e-03, -4.4986e-03, -7.0571e-04],
        [ 9.6647e-03,  1.1204e-04, -2.6802e-04],
        [ 3.4119e-03, -1.0851e-02,  3.4680e-03],
        [ 1.7475e-02, -1.5285e-03, -1.8414e-02],
        [ 1.1224e-02, -6.2024e-03,  8.8629e-03],
        [-1.3394e-02, -5.1853e-03, -8.4364e-03],
        [ 9.0946e-03, -1.2365e-02,  2.3408e-03],
        [ 6.8581e-03, -1.4502e-02, -6.4234e-03],
        [ 9.3774e-03,  5.6594e-03, -6.5126e-03],
        [ 1.3403e-02, -1.1427e-03,  5.8966e-03],
        [-5.1027e-03,  1.6387e-03,  5.6290e-03],
        [ 7.1349e-03, -5.8376e-03, -1.5147e-03],
        [-8.7311e-03, -4.1845e-03,  9.8266e-04],
        [ 1.2524e-02, -1.1518e-02, -8.4684e-03],
        [ 1.1910e-02,  4.2740e-03, -7.1719e-04],
        [ 1.2571e-02,  2.5626e-03, -1.2218e-03],
        [ 1.3071e-02, -7.1752e-03, -5.2052e-03],
        [ 7.0280e-03, -7.9682e-03, -4.7157e-03],
        [-7.3784e-04, -3.2036e-03,  4.2687e-04],
        [ 5.3285e-03,  8.5973e-04,  4.4771e-03],
        [-3.0703e-03, -9.9066e-03, -1.3232e-02],
        [ 6.8013e-03, -3.8747e-03,  4.5942e-03],
        [ 1.2569e-02,  7.5836e-03, -1.0281e-03],
        [ 6.8013e-03, -3.8747e-03,  4.5942e-03],
        [ 1.0058e-02, -4.8758e-03, -9.9188e-03],
        [ 1.8106e-02,  2.0642e-03,  2.3258e-03],
        [ 9.9350e-03,  9.4145e-04, -7.4222e-03],
        [ 1.5374e-02, -1.9454e-03, -4.4413e-03],
        [ 9.0920e-03, -8.3445e-03,  6.2085e-03],
        [ 4.7988e-03, -6.3597e-03,  5.5747e-03],
        [ 1.1871e-02, -3.5836e-03, -3.4946e-03],
        [ 5.8164e-03, -4.9854e-04, -1.0567e-02],
        [ 9.9790e-03, -5.1953e-03, -1.5077e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000028
penultimate_layer.0.bias: grad mean = 0.000146
output_layer.0.weight: grad mean = 0.000800
output_layer.0.bias: grad mean = 0.005048
[correct] no_actions is False
task_labels:  tensor([1, 0, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 1, 1, 2, 0, 1, 0, 1,
        0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 2, 0, 1, 0,
        1, 1, 0, 1, 1, 2, 2, 1, 2, 1, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 0, 2, 1, 2,
        2, 0, 0, 0, 1, 1, 2, 0, 2, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 2, 2, 2, 1, 2, 1, 2, 0, 1, 2, 1, 2, 2, 1, 1, 2, 2, 0, 1, 2,
        2, 1, 1, 0, 1, 1, 0, 1]) , task_preds: tensor([[ 9.2172e-04, -4.6838e-03,  3.1103e-04],
        [ 1.0126e-02, -1.3386e-02, -1.0110e-02],
        [ 1.3269e-03, -5.8471e-03,  5.8376e-03],
        [ 3.0124e-03, -7.5313e-03, -1.2202e-03],
        [-7.6929e-03, -1.0492e-02,  7.4178e-03],
        [ 1.1117e-02, -3.8888e-03,  3.8448e-03],
        [-1.3378e-03, -1.1246e-02, -5.6238e-03],
        [-1.9778e-03,  9.4323e-04,  6.8072e-03],
        [ 1.3809e-02, -8.1907e-03, -2.1839e-03],
        [ 1.6818e-03, -1.0142e-03, -6.3160e-03],
        [ 1.5264e-02, -7.0990e-03,  5.2910e-03],
        [ 7.9426e-03,  1.0502e-02,  7.4130e-03],
        [-6.5063e-03, -1.0776e-02, -1.4717e-02],
        [ 2.1325e-03, -9.8261e-03,  2.9529e-03],
        [ 1.4259e-02, -4.1330e-03,  3.5450e-03],
        [ 1.0701e-02, -1.1119e-02, -4.1991e-03],
        [ 1.1877e-02,  3.4485e-03,  5.9128e-03],
        [ 7.1273e-03, -4.1936e-03,  4.6621e-03],
        [ 7.5981e-03,  8.8774e-03, -5.1328e-04],
        [ 1.6467e-02, -6.2350e-03,  5.6909e-03],
        [ 1.5575e-02, -5.9119e-03, -4.5240e-04],
        [ 3.8267e-03,  1.7159e-03,  8.1372e-04],
        [ 4.7076e-03, -4.8826e-03, -6.1224e-03],
        [-2.8474e-03, -7.2261e-03,  5.5971e-03],
        [-1.6733e-03,  4.8205e-03, -8.8247e-04],
        [ 9.2045e-03, -5.2525e-03,  2.2200e-03],
        [ 5.8946e-03,  1.4256e-03, -5.2747e-03],
        [ 3.7108e-03,  1.0486e-03, -5.2517e-03],
        [ 5.0746e-03, -2.8237e-03, -4.1081e-03],
        [ 6.6867e-03,  5.9689e-03, -8.2924e-03],
        [ 2.1779e-03, -8.6060e-03, -9.2527e-03],
        [ 9.6877e-03, -8.4129e-03, -9.8022e-04],
        [ 6.2712e-03,  2.4064e-03, -1.1473e-02],
        [ 1.1112e-02, -6.6634e-03,  6.8408e-03],
        [ 7.2997e-03, -5.3114e-03,  2.9192e-03],
        [ 6.3066e-03, -1.6967e-02, -4.1449e-03],
        [ 1.7591e-03,  1.2029e-03, -1.1732e-02],
        [ 1.4162e-02, -6.1017e-04,  4.4948e-03],
        [ 1.9531e-02, -6.0784e-03, -1.1290e-02],
        [ 4.6309e-03, -8.1106e-03, -1.1254e-02],
        [ 2.0022e-02, -2.1076e-03,  6.7217e-03],
MLP Fine-Tuning: task_loss = 1.1032: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.96it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 13.80it/s]
        [ 1.0428e-02, -8.5104e-03, -1.6803e-04],
        [ 1.1873e-02, -9.0823e-03, -9.8216e-06],
        [ 6.3522e-03,  8.6669e-03, -2.3399e-03],
        [ 9.2220e-03, -4.6277e-03, -8.9611e-06],
        [ 5.6723e-03,  1.9666e-03, -7.9357e-04],
        [ 9.7097e-03, -5.1875e-03, -8.9279e-03],
        [ 1.2999e-02, -5.5261e-03,  3.9191e-03],
        [ 7.4066e-03, -5.2357e-03,  2.9672e-03],
        [-1.3792e-03, -9.2395e-03,  2.5853e-03],
        [ 8.6062e-03, -2.0305e-03, -6.9958e-03],
        [ 5.7714e-03,  4.6395e-03,  7.0273e-03],
        [ 7.4093e-03, -5.9651e-03,  1.6558e-03],
        [ 3.1518e-03, -1.4273e-02,  3.4013e-03],
        [ 4.7195e-03, -3.3212e-03,  3.2361e-03],
        [-1.7413e-03,  3.9513e-03,  1.5291e-03],
        [ 1.5148e-02,  1.9247e-03, -1.2707e-03],
        [ 1.2118e-02, -5.9331e-03, -3.7577e-03],
        [ 2.1000e-02, -8.7926e-03,  4.3105e-03],
        [ 1.3368e-02, -1.1274e-03,  4.5526e-03],
        [ 1.0416e-03, -1.4906e-02, -1.6951e-02],
        [ 1.0899e-02, -4.8101e-03,  3.6507e-03],
        [ 8.6235e-03, -5.3827e-03,  1.6473e-03],
        [-5.2349e-03, -1.3100e-02, -7.9452e-03],
        [ 2.6379e-02, -2.0222e-03, -6.4835e-03],
        [ 6.8698e-04, -1.8332e-03, -4.9753e-03],
        [ 2.4218e-03, -7.8596e-03, -2.4780e-03],
        [ 1.1735e-02, -3.6849e-03,  2.9563e-03],
        [ 1.5992e-03, -6.7307e-03, -3.6775e-03],
        [ 1.2585e-02,  2.1746e-03, -8.1392e-03],
        [ 1.3345e-02, -4.5202e-03,  2.0520e-03],
        [ 3.3300e-03, -6.5540e-03,  3.7918e-03],
        [ 7.3922e-03, -5.6423e-03, -3.0049e-03],
        [ 1.4351e-02,  3.5952e-03, -1.4847e-03],
        [ 5.3440e-03,  4.8731e-03, -5.4554e-03],
        [ 1.3554e-02, -9.7664e-03, -2.2365e-03],
        [-4.5093e-03,  3.6886e-03, -7.6750e-03],
        [ 1.1025e-02, -5.9760e-03,  3.5261e-03],
        [ 2.1764e-02, -2.1760e-03, -2.6347e-03],
        [ 2.7758e-02, -1.3043e-02, -7.3256e-03],
        [-2.1276e-03,  2.3677e-03,  8.1694e-03],
        [-6.7797e-03, -1.0575e-02,  4.9764e-03],
        [ 3.6327e-03, -3.8563e-03, -8.5959e-03],
        [ 7.1918e-03, -1.1676e-02,  3.5693e-03],
        [ 1.6536e-02, -5.6679e-03, -1.4381e-02],
        [ 3.8415e-03,  4.5984e-03, -2.3703e-03],
        [ 6.7591e-03, -1.5410e-02, -1.4030e-04],
        [ 2.2872e-03, -7.7525e-04, -6.4412e-03],
        [-2.6405e-03,  3.0390e-03,  3.7744e-04],
        [ 1.0823e-02, -1.0484e-03,  3.1392e-03],
        [ 2.5524e-02, -1.7228e-03,  2.8394e-03],
        [-3.6559e-03, -1.0055e-02, -1.2382e-02],
        [ 7.4689e-03,  1.4074e-03,  1.7484e-03],
        [ 6.9622e-03, -3.6195e-03, -5.5669e-03],
        [-1.3310e-04,  2.3165e-03, -1.8705e-02],
        [ 3.1539e-03, -9.5054e-03,  1.5383e-03],
        [ 9.6707e-03, -6.5964e-04,  6.1490e-04],
        [ 7.4025e-03, -4.5048e-03, -4.9000e-04],
        [ 1.2019e-02, -1.8485e-03,  1.3002e-03],
        [ 4.5506e-03, -6.2112e-03,  1.5792e-03],
        [ 1.6381e-02, -9.3117e-04, -7.9844e-04],
        [ 3.5139e-03, -5.0658e-03, -1.0055e-03],
        [ 8.3777e-03, -8.2481e-03, -8.4853e-04],
        [ 1.2752e-02, -4.8946e-03, -1.5149e-03],
        [ 1.1102e-02,  4.4592e-03, -1.6092e-02],
        [ 5.3555e-03, -1.0377e-02, -6.0126e-03],
        [ 1.6022e-02, -1.8425e-03,  7.0208e-03],
        [ 3.7869e-03, -8.9542e-04, -2.6650e-03],
        [ 2.1853e-02,  3.5551e-03, -8.7664e-03],
        [ 6.5572e-04, -4.5579e-03, -8.1256e-03],
        [ 4.6094e-03, -3.1480e-03, -3.0964e-04],
        [ 7.1854e-03, -1.4067e-02,  3.8541e-03],
        [ 7.9131e-03, -1.1579e-02, -4.6225e-03],
        [ 7.4129e-03, -7.7579e-03, -2.9075e-03],
        [ 1.0972e-02,  3.2751e-03, -6.5149e-03],
        [ 1.2221e-02,  2.3440e-03, -4.9050e-04],
        [ 9.0914e-03,  1.1362e-03,  1.2401e-03],
        [ 4.7843e-03, -9.1984e-03, -2.2998e-03],
        [-5.1568e-03, -8.0385e-03, -4.7688e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000030
penultimate_layer.0.bias: grad mean = 0.000595
output_layer.0.weight: grad mean = 0.000904
output_layer.0.bias: grad mean = 0.016390
[correct] no_actions is False
task_labels:  tensor([2, 0, 1, 1, 0, 1, 2, 2, 1, 0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 1, 1, 1, 2, 2,
        1, 2, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 1, 1, 0, 2, 1, 1, 1,
        1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 2, 2, 0, 2, 2, 2, 0, 2, 0, 1, 2, 1, 2,
        2, 0, 1, 2, 0, 1, 0, 2, 2, 1, 1, 0, 1, 2, 2, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 2, 1, 2, 1, 0, 2, 1, 0, 2, 0, 2, 0, 1, 0, 1, 0, 1, 1, 0, 1, 2, 2, 2,
        1, 2, 0, 2, 2, 0, 1, 1]) , task_preds: tensor([[ 1.3741e-02, -2.0717e-03, -2.8257e-03],
        [ 6.2119e-03, -2.4100e-03,  5.5681e-03],
        [ 7.3874e-03, -4.4600e-03,  4.7294e-03],
        [ 7.9477e-03, -4.0217e-03,  3.5329e-03],
        [ 8.5262e-03,  4.3622e-03, -2.5539e-03],
        [ 3.8867e-03,  1.4227e-03,  1.0651e-03],
        [ 1.2120e-02, -1.2456e-02,  6.0747e-03],
        [ 1.0819e-02, -6.6307e-03, -1.0352e-03],
        [ 2.5226e-03, -1.0775e-03, -6.3182e-03],
        [ 7.6038e-03, -1.0168e-02,  3.2302e-03],
        [ 1.7170e-02,  4.8755e-03, -3.4610e-03],
        [ 1.8573e-02, -7.3981e-04, -6.9255e-03],
        [ 7.3874e-03, -4.4600e-03,  4.7294e-03],
        [-7.6925e-03, -2.3503e-03, -8.8908e-03],
        [ 9.0216e-03, -4.2278e-03,  2.4069e-03],
        [ 1.3872e-02,  1.6439e-03,  3.9645e-03],
        [ 1.2665e-02, -1.1197e-02, -2.4077e-03],
        [ 3.7710e-03, -1.3302e-03, -2.2038e-03],
        [ 8.9711e-03,  5.8491e-04, -1.8095e-03],
        [ 1.3860e-02,  1.3540e-03, -8.9587e-03],
        [ 2.6202e-03, -3.3889e-03, -8.3826e-04],
        [-1.1145e-02,  3.9195e-03,  2.3025e-03],
        [ 4.5631e-03, -7.2690e-03, -1.0640e-02],
        [ 1.2866e-02,  1.7968e-03, -7.9720e-03],
        [ 1.0149e-02, -4.1782e-03, -8.5513e-03],
        [ 2.4812e-02, -1.0112e-02, -6.4847e-03],
        [ 9.1242e-03, -1.1925e-03,  4.1226e-03],
        [ 1.2202e-02,  3.6315e-03, -1.8629e-03],
        [ 2.3338e-03, -6.5020e-03,  3.0226e-04],
        [ 2.2686e-02, -8.0826e-04, -6.2367e-04],
        [ 4.2480e-03, -6.3285e-03,  4.7352e-03],
        [ 1.0982e-02,  3.4086e-03,  2.1408e-03],
        [ 9.6859e-03,  3.7180e-03,  5.1386e-03],
        [ 6.7491e-03,  6.2055e-03,  6.7372e-03],
        [-3.5335e-03, -1.0359e-02, -1.2170e-02],
        [ 1.2533e-02, -6.1586e-03,  2.4888e-03],
        [ 1.9689e-02, -1.0948e-02, -8.0111e-03],
        [ 1.6634e-02, -1.2834e-02, -1.9328e-03],
        [ 1.0591e-02,  6.9826e-05,  1.4776e-03],
        [ 1.8740e-02,  7.1255e-04, -3.4043e-03],
        [ 2.5001e-05, -2.8926e-03, -5.0032e-03],
        [ 1.5223e-02, -1.1401e-02,  2.0919e-03],
        [ 1.1461e-02, -7.4880e-03,  6.1583e-03],
        [ 3.7186e-03, -4.7252e-03,  4.1508e-03],
        [ 5.0107e-03,  6.4709e-03, -1.5040e-02],
        [ 1.1476e-02, -4.5130e-03, -2.1643e-03],
        [ 8.0468e-03, -2.3388e-04, -8.6848e-03],
        [ 3.1895e-03, -1.3183e-02, -1.4157e-03],
        [ 1.1296e-02,  4.2607e-03, -1.6045e-02],
        [ 4.1459e-03, -4.9729e-03,  2.0899e-03],
        [ 7.0259e-03,  5.7621e-04, -4.2399e-03],
        [-6.8566e-03,  2.7241e-03, -2.5809e-04],
        [-1.4618e-03, -6.5593e-03,  3.5237e-03],
        [ 6.7806e-03,  5.6596e-03, -8.0488e-03],
        [ 6.2300e-03, -1.1833e-03, -1.0184e-02],
        [ 1.1460e-02,  2.7929e-03, -6.3999e-03],
        [ 7.4138e-03, -4.8741e-03, -7.2805e-04],
        [ 1.5498e-02,  6.1273e-03, -1.3717e-03],
        [-4.6309e-04, -1.8959e-03,  3.9488e-03],
        [ 1.0786e-02, -2.9610e-03,  4.0073e-03],
        [ 2.5625e-03, -1.7201e-03, -9.8456e-04],
        [ 1.7955e-02, -1.0931e-02, -4.0306e-03],
        [ 5.9206e-03, -3.2917e-03, -1.1240e-02],
        [ 1.0423e-02, -1.3360e-03,  8.8760e-03],
        [ 1.0788e-02, -8.0804e-03, -4.5404e-03],
        [ 1.1996e-02, -2.2477e-03, -8.8721e-04],
        [-3.8941e-04, -5.6618e-03, -3.5179e-03],
        [ 1.7297e-02, -9.7971e-04, -7.6337e-03],
        [ 4.1818e-03, -1.4412e-02, -8.0842e-03],
        [ 1.1575e-02, -7.5305e-03, -7.8239e-04],
        [-2.7229e-03, -1.6443e-03, -2.9743e-03],
        [ 1.5012e-02,  3.9143e-03, -1.5303e-03],
        [ 9.5107e-03, -8.4977e-03,  1.0471e-02],
        [ 2.3567e-04, -1.6199e-02, -4.1509e-03],
        [ 1.1125e-02, -6.8536e-03,  7.0242e-03],
        [ 6.9909e-03, -5.3216e-03, -3.2309e-03],
        [ 2.3026e-02,  2.9021e-03, -3.0765e-03],
        [ 1.3914e-02, -6.5012e-03,  1.7119e-03],
        [ 1.1716e-02, -4.8703e-03, -3.7052e-03],
        [ 8.3869e-03, -8.7855e-03, -3.0266e-04],
        [-9.5600e-04, -5.7536e-03, -8.6101e-03],
        [ 8.5942e-03,  4.5911e-03,  9.2249e-03],
        [ 2.2504e-03,  3.1835e-03, -8.5764e-03],
        [ 1.1621e-02, -1.3612e-03, -2.2056e-03],
        [ 1.7825e-03, -5.6032e-03,  8.8997e-03],
        [ 9.3013e-03, -1.5980e-03,  1.5944e-02],
        [ 2.0125e-03, -1.2049e-02, -5.5826e-03],
        [ 8.3901e-03,  2.3491e-04, -2.6238e-03],
        [ 5.1347e-03, -5.2085e-03, -4.9979e-03],
        [ 2.2431e-02,  9.3911e-03, -8.1896e-03],
        [ 7.3874e-03, -4.4600e-03,  4.7294e-03],
        [ 5.1526e-03, -3.2343e-03, -3.7455e-03],
        [ 8.6268e-03, -2.6148e-03, -6.4067e-03],
        [ 2.0336e-02, -9.7174e-03, -2.7842e-04],
        [ 9.7911e-03, -1.3344e-03,  2.4812e-03],
        [ 1.3920e-02, -6.5483e-03,  1.6132e-03],
        [ 5.5322e-03, -1.4807e-03,  7.2363e-04],
        [ 1.3544e-02, -8.4244e-03, -1.8820e-02],
        [ 4.4638e-03, -6.7838e-03, -1.3902e-03],
        [-4.8393e-03, -4.7831e-03,  4.6197e-03],
        [ 1.6713e-02, -6.4829e-03, -9.2343e-03],
        [ 5.0050e-03, -5.2144e-03, -6.0149e-03],
        [ 1.6232e-02, -9.4971e-03,  1.8743e-03],
        [ 1.1660e-02,  4.6408e-03, -1.5094e-03],
        [ 1.2023e-02, -3.5159e-03, -2.5029e-03],
        [ 7.3874e-03, -4.4600e-03,  4.7294e-03],
        [ 1.5496e-02, -2.2247e-02, -9.8318e-04],
        [ 2.1831e-02, -2.7130e-03, -2.1330e-03],
        [ 2.0254e-02,  2.1739e-03,  1.0338e-03],
        [ 4.5434e-03,  2.2914e-03, -6.3157e-03],
        [ 1.5049e-02, -1.5907e-03, -1.2942e-02],
        [ 1.5666e-02, -2.6054e-03, -3.9988e-03],
        [ 2.1565e-03, -9.0737e-03, -1.2817e-02],
        [-8.8186e-03, -4.5921e-03,  1.4673e-03],
        [ 1.2943e-02,  1.8429e-03, -7.8749e-03],
        [ 2.0406e-02, -3.0358e-04, -3.4006e-03],
        [ 5.6735e-03,  8.3530e-04, -2.5142e-03],
        [-2.6068e-03,  4.3587e-03, -1.0189e-02],
        [ 7.0763e-03, -8.0389e-03, -5.6226e-03],
        [ 4.2728e-03, -1.6674e-02, -7.6540e-03],
        [ 1.6192e-02,  1.1528e-02, -1.0968e-02],
        [ 3.5537e-03, -4.5417e-04,  9.2238e-03],
        [ 9.4245e-03, -6.1073e-03,  1.6258e-03],
        [ 2.8390e-03, -4.7858e-03,  7.6002e-04],
        [ 1.6676e-02, -9.7650e-03, -2.7363e-03],
        [ 1.3771e-02, -1.0296e-02, -1.8607e-03],
        [ 4.5382e-03, -6.0724e-03,  6.7955e-03],
        [ 8.2140e-03,  5.6541e-03,  8.1286e-04]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000026
penultimate_layer.0.bias: grad mean = 0.000244
output_layer.0.weight: grad mean = 0.000734
output_layer.0.bias: grad mean = 0.006154
[correct] no_actions is False
task_labels:  tensor([1, 1, 0, 2, 0, 0, 1, 2, 1, 1, 0, 0, 0, 2, 1, 2, 2, 0, 2, 2, 2, 2, 0, 1,
        1, 2, 0, 2, 2, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 1, 0, 1, 0, 0, 2, 2, 0, 0,
        2, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 1, 2, 1, 2, 1, 1, 0, 0, 1, 0,
        1, 0, 2, 1, 1, 2, 0, 1, 0, 0, 1, 2, 2, 0, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1,
        0, 1, 2, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 1, 0, 2, 1, 1, 2, 1, 2, 1, 0, 1,
        0, 2, 2, 0, 2, 1, 2, 1]) , task_preds: tensor([[ 2.0922e-02,  9.6311e-04, -3.9594e-04],
        [ 1.6574e-02, -9.4130e-03, -6.2489e-03],
        [-1.2913e-03,  3.9850e-03, -3.1735e-04],
        [ 3.6384e-03,  6.1440e-03,  5.1423e-03],
        [ 7.0083e-03, -3.3126e-03,  8.7768e-03],
        [ 1.2758e-02, -7.3600e-03, -1.0361e-04],
        [-3.2233e-03, -8.0162e-03,  6.7077e-03],
        [ 8.9897e-03, -3.6797e-03, -2.0391e-03],
        [-9.8870e-04, -1.9583e-03, -6.5045e-03],
        [-2.6046e-06, -3.7031e-03,  3.4950e-04],
        [ 8.6407e-04, -3.1576e-03,  3.5281e-03],
        [ 1.3320e-02, -5.5756e-03,  3.1625e-03],
        [ 4.1078e-03, -1.4085e-02,  1.1667e-03],
        [ 1.0197e-02, -1.4512e-03, -6.3834e-03],
        [-1.3660e-02, -1.4970e-02,  4.9538e-03],
        [ 1.1974e-02, -4.0703e-03,  1.5559e-03],
        [ 1.3947e-02, -6.0994e-03, -1.5618e-02],
        [ 7.8683e-03, -1.5663e-02, -6.0257e-03],
        [ 9.9513e-03, -4.8414e-03,  9.3643e-03],
        [-6.5107e-03, -1.1478e-02, -1.3991e-02],
        [ 1.4334e-02, -1.9620e-03, -5.4871e-03],
        [ 5.6384e-03, -6.7898e-04,  1.5061e-03],
        [-8.8255e-03, -7.9037e-03, -5.4386e-03],
        [ 6.1417e-03, -1.0848e-02, -1.7921e-03],
        [ 1.0344e-02, -1.4425e-03,  2.9635e-03],
        [-5.1605e-03, -1.3816e-02, -7.2636e-03],
        [-3.8245e-03,  2.6926e-03,  1.4747e-02],
        [ 1.5332e-03, -6.6112e-03,  6.4660e-03],
        [ 6.5671e-03, -5.8392e-03,  1.1029e-02],
        [ 9.7284e-03,  6.2723e-03, -3.0326e-03],
        [-1.3681e-04,  9.9697e-03, -5.0978e-03],
        [ 6.4412e-03, -1.1373e-03,  5.0883e-03],
        [ 1.0081e-02, -3.3561e-03,  1.6359e-03],
        [ 1.1177e-02, -5.4454e-03,  5.4028e-03],
        [-1.0783e-03, -3.7773e-04, -7.3250e-03],
        [ 9.5994e-03,  3.5247e-03, -1.4391e-03],
        [-2.7033e-03, -1.4241e-02, -5.7499e-03],
        [ 1.1956e-02, -1.2582e-02, -3.8487e-03],
        [ 8.7409e-03,  3.2880e-03, -6.8037e-03],
        [ 1.5194e-02,  6.9617e-03, -1.2386e-02],
        [ 1.0415e-02, -1.3603e-02,  2.5855e-03],
        [ 1.2859e-02,  1.8882e-03, -5.2945e-04],
        [ 8.3462e-03, -8.5429e-03,  4.1555e-03],
        [-2.9766e-03,  7.3835e-04,  8.6403e-03],
        [ 1.3515e-02, -8.5340e-03, -7.3145e-03],
        [ 1.0374e-02,  2.5174e-03,  5.3597e-03],
        [ 9.7228e-03,  8.3376e-03, -2.7358e-04],
        [ 1.1456e-02, -1.5663e-03,  3.1679e-03],
        [ 3.1657e-03, -2.4817e-02, -4.6863e-03],
        [ 1.3303e-02, -3.4965e-03, -9.9328e-03],
        [ 1.3721e-02,  6.3965e-03, -1.2397e-02],
        [-3.3546e-03, -4.2188e-03, -1.7515e-02],
        [ 1.5854e-02, -7.4730e-03, -1.2798e-03],
        [ 2.7501e-03,  4.7969e-03,  1.3115e-02],
        [ 8.6937e-03, -1.0603e-02, -8.5057e-03],
        [ 1.2524e-02, -5.9409e-04, -7.3160e-03],
        [ 1.6741e-02, -1.9819e-03,  4.0633e-03],
        [ 4.5661e-03,  1.3881e-03,  1.8464e-03],
        [ 3.2798e-03, -8.1395e-03,  4.7223e-03],
        [ 2.0116e-02, -2.9430e-03, -6.7259e-04],
        [-1.5527e-03, -6.2464e-03, -7.8435e-03],
        [ 1.7841e-02, -1.0456e-02, -1.2275e-02],
        [ 1.4462e-02, -8.9784e-03,  5.4015e-03],
        [-7.8796e-04, -2.1891e-02, -3.2570e-03],
        [ 1.7572e-02, -7.0212e-03,  3.2274e-05],
        [ 6.8581e-03,  1.9446e-03, -3.7032e-03],
        [ 8.5331e-03, -3.1345e-03,  2.5855e-04],
        [ 3.4969e-03, -1.6575e-04, -1.6485e-03],
        [ 4.5691e-03, -4.3976e-03,  4.4686e-03],
        [ 1.6248e-02, -5.3652e-03,  3.8083e-04],
        [-2.6782e-03, -5.0552e-03, -2.1732e-03],
        [ 3.7411e-03, -4.9621e-03, -1.3050e-03],
        [ 9.2661e-04, -5.3646e-03,  1.0090e-03],
        [ 1.2297e-02, -1.2873e-02, -6.8808e-03],
        [ 1.6145e-02, -9.8208e-03, -4.4333e-03],
        [ 5.5392e-04, -9.3003e-03,  1.4216e-05],
        [ 1.6620e-02,  7.5736e-03,  6.5960e-04],
        [ 9.4406e-03, -5.4129e-03,  6.1898e-04],
        [-6.2087e-04, -4.0952e-03, -1.7246e-03],
        [ 7.9271e-03, -4.8291e-03, -2.3879e-03],
        [ 8.9280e-03, -3.0055e-03, -6.3099e-03],
        [ 1.1219e-02,  1.3841e-03, -2.3310e-03],
        [ 3.0028e-03, -6.4218e-03, -3.2601e-03],
        [ 8.3947e-03, -5.3476e-03,  4.4515e-03],
        [ 9.3582e-03, -5.6661e-03, -4.0734e-03],
        [ 1.4096e-02, -8.7588e-04,  1.0667e-03],
        [ 1.4585e-02, -2.8350e-03,  4.1352e-03],
        [-8.0375e-04, -6.4019e-04,  1.0932e-03],
        [ 1.5294e-02, -2.9835e-03,  3.3499e-03],
        [ 5.8979e-03, -5.1735e-03,  4.7232e-03],
        [-7.6242e-04, -3.8791e-03,  1.2108e-02],
        [ 1.8415e-02, -1.3419e-02, -1.3562e-04],
        [ 1.4567e-02, -4.5401e-03,  2.8179e-03],
        [ 4.6286e-03,  6.1297e-03,  7.0118e-03],
        [ 4.7724e-03, -2.8040e-04,  2.2214e-03],
        [-4.4392e-03, -9.3800e-03, -2.5476e-03],
        [ 5.9017e-03, -2.2675e-03, -1.5850e-02],
        [ 8.4607e-03, -1.9787e-03, -3.6866e-04],
        [ 6.6174e-03,  1.1811e-03, -1.2075e-03],
        [ 1.1895e-02, -3.4930e-03, -4.8682e-03],
        [ 1.6316e-03, -7.9785e-03, -1.5237e-03],
        [ 1.6625e-02, -4.2097e-03, -1.0457e-02],
        [ 1.2124e-02,  2.1158e-03, -5.5137e-03],
        [ 3.4755e-03, -4.1671e-03, -8.1590e-03],
        [ 1.6128e-02, -6.5471e-03, -1.0520e-02],
        [ 3.9942e-03,  6.4044e-03,  1.2213e-03],
        [ 7.2413e-03, -1.4884e-02,  4.6522e-03],
        [ 5.1404e-03,  1.4571e-03, -4.7373e-03],
        [ 1.5765e-02, -1.0218e-02,  1.8042e-03],
        [ 5.7085e-03, -6.7568e-03, -8.6915e-03],
        [ 1.3963e-02,  1.2448e-03, -7.6903e-03],
        [ 1.5407e-02, -4.7865e-03,  5.5446e-03],
        [ 5.0706e-04, -1.1930e-02, -6.2306e-03],
        [ 6.7867e-04, -1.1983e-02,  3.0093e-03],
        [-3.4667e-03, -1.0141e-02,  7.4690e-03],
        [ 1.3556e-02, -4.3656e-03,  4.4451e-03],
        [ 7.0512e-03, -4.8189e-03,  8.0190e-03],
        [-1.4115e-03, -3.1370e-03,  3.4045e-03],
        [ 5.5250e-03, -7.5049e-03,  1.8341e-03],
        [ 1.8064e-02, -2.2786e-03,  8.9537e-03],
        [ 7.7828e-03, -2.6813e-03, -8.2917e-03],
        [ 2.7507e-02,  1.6566e-03, -9.1479e-03],
        [ 3.7083e-03, -1.0261e-02,  7.6545e-04],
        [ 1.4128e-02, -3.4995e-03,  7.9260e-03],
        [ 7.9832e-03, -4.8673e-03,  2.1415e-04],
        [ 3.3743e-03, -3.2756e-03,  6.1542e-04],
        [ 7.5822e-03, -4.5961e-03,  4.8177e-03],
        [ 4.1331e-04, -5.4900e-03, -6.9694e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000029
penultimate_layer.0.bias: grad mean = 0.000527
output_layer.0.weight: grad mean = 0.000783
output_layer.0.bias: grad mean = 0.015241
[correct] no_actions is False
task_labels:  tensor([0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 1, 1, 1, 0, 2, 0, 2, 1, 2,
        0, 1, 2, 1, 2, 2, 0, 2, 2, 1, 2, 0, 1, 0, 1, 1, 0, 2, 0, 0, 0, 2, 2, 1,
        2, 0, 0, 0, 1, 1, 2, 1, 2, 2, 0, 2, 1, 0, 1, 2, 0, 1, 0, 2, 1, 0, 2, 1,
        2, 0, 2, 0, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 1, 2, 1, 1, 0, 0, 1, 2, 1, 2,
        2, 2, 2, 1, 1, 1, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 1, 1, 2, 1, 2, 2, 1, 0,
        1, 0, 0, 0, 2, 1, 2, 1]) , task_preds: tensor([[ 1.4336e-02, -5.3141e-03, -1.4862e-02],
        [ 6.4547e-03, -3.9139e-03, -1.8487e-03],
        [ 1.5705e-02, -1.2065e-02,  2.3972e-03],
        [ 1.9686e-02, -1.3993e-02, -6.4430e-03],
        [ 1.1962e-02, -7.7505e-03,  9.8756e-03],
        [ 1.2397e-02, -2.1511e-03, -2.0003e-03],
        [ 2.4812e-02, -2.6560e-03, -8.0993e-04],
        [ 1.1769e-02, -1.7208e-03,  3.0767e-03],
        [ 1.2182e-02, -3.8764e-03, -4.6986e-03],
        [ 1.3514e-02, -1.2707e-02, -5.6643e-03],
        [-1.1345e-02,  3.3616e-03,  3.0332e-03],
        [-1.2246e-03, -7.2288e-03,  4.0282e-03],
        [ 9.5946e-04, -5.6564e-03,  1.2847e-03],
        [ 7.2355e-03, -1.3696e-02,  5.0402e-04],
        [ 2.2713e-02, -1.8119e-03,  3.9072e-04],
        [-1.0459e-03, -2.1359e-03, -6.2774e-03],
        [ 6.7316e-03, -1.3305e-03,  1.7938e-03],
        [-1.7800e-03, -1.1353e-02, -6.5994e-03],
        [ 4.5040e-03, -5.4771e-03,  4.3104e-03],
        [ 1.3114e-03, -1.1619e-02, -7.6699e-03],
        [ 9.0278e-03, -1.1931e-02, -5.7805e-03],
        [ 1.1137e-02, -8.2970e-03,  2.4603e-03],
        [ 1.8061e-03, -4.9383e-04, -4.5043e-03],
        [ 6.4122e-03, -3.9530e-03, -4.4599e-03],
        [ 1.0341e-02, -1.4025e-02, -3.2600e-03],
        [ 1.6197e-03, -4.5919e-03,  4.1954e-05],
        [ 1.2793e-02,  1.1126e-03,  4.5696e-03],
        [ 9.3969e-03, -6.2060e-03,  6.3045e-04],
        [-1.0801e-04,  6.1326e-04,  6.4178e-03],
        [ 1.2206e-02,  2.5535e-03, -9.5704e-04],
        [ 2.0797e-02, -7.0573e-03, -1.1284e-02],
        [ 7.1107e-03, -3.8914e-03, -2.5593e-03],
        [ 2.2074e-02, -6.8593e-03, -5.9322e-03],
        [ 2.2781e-02,  8.7303e-03, -7.7822e-03],
        [ 6.7362e-03,  8.2113e-04, -9.2668e-04],
        [ 1.0271e-02,  5.2339e-03,  1.9669e-03],
        [ 3.2776e-03,  5.8030e-03,  1.1150e-02],
        [ 1.7930e-03, -8.3951e-03, -1.2178e-03],
        [ 1.5131e-02, -7.9646e-03, -9.5623e-03],
        [ 1.8175e-02, -1.0681e-02, -7.4316e-04],
        [ 7.1249e-03, -3.6477e-04, -1.0279e-02],
        [ 7.8596e-03, -4.8302e-03,  4.7321e-03],
        [ 1.6533e-02, -1.4039e-02,  2.5055e-03],
        [ 1.1258e-02, -1.7759e-02, -1.6331e-02],
        [ 1.1920e-02, -1.3996e-03,  5.4257e-03],
        [ 1.0289e-02, -2.0428e-03,  9.7080e-03],
        [ 1.6047e-02, -1.8895e-03,  4.4673e-04],
        [ 1.9975e-02, -1.1919e-02, -8.1762e-03],
        [ 1.6770e-02,  8.2984e-04, -8.9379e-03],
        [ 2.1626e-02,  3.2872e-03,  9.4669e-03],
        [ 1.3190e-02,  4.8699e-03,  5.6333e-03],
        [ 2.2240e-02, -8.1725e-03, -2.4302e-03],
        [ 3.2925e-03, -4.6654e-03, -1.1300e-02],
        [ 6.0965e-03, -2.9947e-03, -6.5866e-03],
        [ 5.1793e-03, -9.9226e-03, -1.2520e-03],
        [ 1.1026e-02, -7.2429e-03,  7.4991e-03],
        [ 9.1594e-03, -3.9801e-03, -1.8633e-03],
        [ 1.4547e-02, -1.3659e-02,  1.9536e-03],
        [ 1.0804e-02, -1.3965e-02,  2.6561e-03],
        [ 1.4659e-02,  9.6897e-05, -1.1281e-02],
        [-1.4690e-03, -3.4358e-03,  3.7561e-03],
        [ 7.8596e-03, -4.8302e-03,  4.7321e-03],
        [ 6.2443e-03, -1.8940e-02, -6.1919e-03],
        [ 7.5153e-03, -3.9368e-03,  4.4457e-03],
        [ 8.7217e-03, -9.0400e-03, -2.7365e-03],
        [ 7.0434e-03, -1.2815e-02,  4.8596e-03],
        [ 7.8654e-03, -5.5835e-03,  3.4629e-03],
        [-6.4333e-03, -1.1814e-02, -1.3705e-02],
        [ 2.3417e-02, -1.9535e-02,  3.4871e-03],
        [ 7.0848e-03, -1.6752e-02,  9.8727e-04],
        [ 7.7919e-04, -1.1593e-02,  1.2885e-03],
        [ 1.9333e-02, -4.2181e-05, -1.1813e-03],
        [ 1.4630e-02, -4.1636e-03, -1.5740e-03],
        [ 2.4287e-02,  2.9432e-03, -5.3422e-03],
        [ 8.3031e-03, -9.7377e-03,  7.4353e-04],
        [-3.8192e-04, -9.3124e-03, -2.3354e-03],
        [ 1.1751e-02, -1.1986e-02, -6.7676e-03],
        [ 1.2726e-02,  1.0822e-02,  3.0372e-03],
        [-1.2917e-03, -4.3482e-03,  1.0008e-02],
        [-4.8754e-03, -5.6533e-03,  5.5506e-03],
        [ 2.2210e-02,  6.5922e-03, -4.8043e-03],
        [ 6.9208e-03,  1.1763e-04, -3.6872e-03],
        [ 7.5261e-03, -7.2507e-03, -3.8022e-03],
        [ 1.4011e-02,  4.2299e-03, -3.9912e-03],
        [ 6.8803e-03,  1.7829e-03, -3.5584e-03],
        [ 1.2589e-02, -7.1880e-03, -2.8274e-03],
        [ 1.0350e-02, -5.3887e-03,  4.9182e-03],
        [ 7.1850e-03, -6.4210e-03, -1.7998e-03],
        [ 1.6933e-02, -9.7767e-03, -6.1562e-03],
        [ 5.1766e-03, -3.9888e-03, -2.9855e-03],
        [ 1.7225e-02, -5.5384e-03, -7.9203e-03],
        [ 1.6934e-02, -3.8812e-03,  2.1686e-03],
        [ 8.3594e-03, -1.2061e-02, -4.4937e-03],
        [-5.6996e-03,  1.9972e-03,  7.1137e-03],
        [ 1.4530e-02, -9.2683e-03,  5.6459e-03],
        [ 1.4053e-02, -6.4486e-03, -1.5342e-02],
        [ 3.5520e-03, -1.3268e-02,  5.8524e-03],
        [ 7.0532e-03, -8.6974e-03, -4.9246e-03],
        [ 4.9814e-03, -6.7180e-03, -1.1446e-03],
        [-8.3946e-04, -2.8253e-03,  3.3558e-03],
        [ 1.5498e-02,  4.7130e-03, -1.9355e-03],
        [ 1.6026e-02, -7.5624e-03, -3.0171e-03],
        [ 1.3066e-03, -5.9006e-04, -1.1221e-03],
        [ 2.2654e-03, -4.1524e-03,  2.2902e-04],
        [ 1.4206e-02,  8.6723e-04, -7.4909e-03],
        [ 1.0208e-02, -1.4687e-02,  1.2344e-02],
        [ 7.4981e-03,  7.9497e-05,  3.0964e-03],
        [-2.6949e-03,  3.7797e-04,  8.7914e-03],
        [ 4.0841e-04, -5.8571e-03, -6.5849e-03],
        [ 2.8459e-02, -1.4395e-02, -6.4828e-03],
        [ 1.8698e-02,  1.0955e-03, -6.3115e-03],
        [ 1.9579e-03, -6.7893e-04,  4.2971e-03],
        [-3.6488e-03, -1.2895e-03,  4.1107e-03],
        [ 4.9053e-03, -1.3493e-02,  5.7170e-03],
        [ 1.7093e-03, -6.9618e-03,  6.6915e-03],
        [ 1.5239e-02,  6.8293e-03, -1.2289e-02],
        [ 5.0674e-05, -3.5150e-03, -4.3821e-03],
        [ 2.1805e-02, -3.6777e-03, -1.1149e-03],
        [ 9.3156e-03,  2.8245e-03, -2.4381e-03],
        [ 2.2187e-02, -8.2368e-03, -1.9690e-02],
        [ 6.7322e-03, -1.6032e-03, -1.8497e-03],
        [-1.3125e-03, -7.2508e-03, -6.9236e-03],
        [ 1.6114e-02, -2.3366e-02, -3.1502e-04],
        [-4.7172e-04, -4.3895e-03, -1.5417e-03],
        [ 7.7906e-03,  1.0032e-03,  8.7993e-03],
        [ 2.3895e-03, -5.6271e-03, -7.9384e-03],
        [ 4.0113e-03, -4.4333e-03,  1.0462e-02],
        [-9.6727e-04, -1.1689e-03, -3.5198e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000028
penultimate_layer.0.bias: grad mean = 0.000464
output_layer.0.weight: grad mean = 0.000867
output_layer.0.bias: grad mean = 0.013383
[correct] no_actions is False
task_labels:  tensor([1, 1, 1, 1, 1, 2, 2, 0, 0, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 0, 1,
        2, 1, 2, 1, 2, 2, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 2, 1, 2, 0, 0,
        1, 0, 0, 0, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0,
        0, 1, 2, 2, 1, 0, 2, 1, 2, 1, 0, 1, 0, 2, 2, 1, 1, 0, 0, 0, 2, 2, 0, 0,
        1, 0, 2, 0, 0, 0, 1, 0, 0, 2, 1, 1, 1, 0, 2, 0, 1, 2, 1, 2, 1, 1, 0, 1,
        2, 1, 0, 1, 2, 2, 1, 0]) , task_preds: tensor([[ 5.8953e-03, -2.2888e-03,  5.9769e-04],
        [ 1.0187e-02, -1.6410e-02,  3.1294e-03],
        [ 3.9333e-04, -4.0476e-03,  3.8752e-04],
        [ 9.5483e-03, -1.4965e-03,  4.1693e-03],
        [-9.6101e-04, -1.1068e-03, -6.6686e-03],
        [ 1.0101e-02, -1.2688e-02, -2.1130e-03],
        [ 8.0943e-03, -5.1426e-03,  4.8680e-03],
        [ 1.5841e-02, -8.0068e-03, -8.8916e-04],
        [ 7.4398e-03, -4.1525e-03,  9.3097e-03],
        [ 3.1968e-03, -8.8655e-03,  5.5356e-03],
        [ 1.2111e-02, -1.0522e-02, -1.7016e-03],
        [ 1.1662e-02,  3.7896e-03, -6.3713e-04],
        [ 5.8141e-03, -7.4609e-03,  2.4308e-03],
        [ 1.1190e-02, -6.3855e-03, -3.2415e-03],
        [ 8.0943e-03, -5.1426e-03,  4.8680e-03],
        [ 1.1201e-02, -5.9796e-03, -9.6794e-03],
        [ 8.9259e-03,  3.6335e-03,  9.9514e-03],
        [ 1.0392e-02, -5.8331e-03,  2.3523e-03],
        [ 9.4768e-03, -2.0995e-03, -2.5169e-03],
        [ 1.9737e-03, -7.6291e-03, -6.0504e-03],
        [ 1.1412e-02,  2.0927e-05, -8.1997e-03],
        [ 2.7823e-02,  6.4939e-04, -8.3548e-03],
        [ 4.2651e-03, -5.3068e-03, -3.4998e-03],
        [ 1.0476e-02, -2.1879e-03,  1.0662e-02],
        [ 2.4059e-03,  3.4059e-03,  1.4078e-03],
        [ 1.3473e-02, -8.6660e-03,  4.1884e-03],
        [ 1.1812e-02,  3.7869e-03,  8.8980e-04],
        [ 1.1291e-02, -6.3684e-03,  6.2689e-03],
        [ 8.1026e-04, -1.0945e-02,  5.6099e-03],
        [ 1.3333e-02, -7.5079e-03, -4.4696e-03],
        [ 2.3638e-02,  1.5996e-03, -2.2095e-03],
        [ 2.0121e-02,  3.8509e-03, -1.3096e-03],
        [ 8.3572e-03, -1.6454e-02, -5.5937e-03],
        [ 1.0962e-02, -6.7009e-03, -8.7600e-03],
        [ 1.1354e-02,  5.7238e-04, -1.5970e-03],
        [ 2.0425e-02, -1.9066e-03, -6.1307e-03],
        [ 4.0924e-03, -1.5097e-02,  2.2269e-03],
        [ 1.0618e-02, -6.4868e-03,  2.9243e-04],
        [ 1.4970e-02, -3.5949e-04, -5.3599e-03],
        [ 1.1774e-02, -1.6891e-03, -4.2720e-04],
        [ 8.0943e-03, -5.1426e-03,  4.8680e-03],
        [ 8.0943e-03, -5.1426e-03,  4.8680e-03],
        [ 5.7122e-03, -6.4249e-03, -5.3198e-03],
        [ 9.9679e-03, -6.7706e-03,  4.5546e-03],
        [-5.5124e-04, -4.4892e-03,  1.4253e-03],
        [ 1.0770e-02, -3.4517e-03, -7.6787e-04],
        [ 1.8801e-02, -1.0296e-02, -4.0712e-03],
        [ 5.2363e-03, -5.9780e-04, -8.9491e-03],
        [ 4.4110e-03,  5.6111e-03,  7.7134e-03],
        [-3.4718e-03, -8.9503e-03,  6.4102e-05],
        [ 2.0682e-02, -1.0224e-02,  1.0958e-03],
        [ 1.8044e-02, -1.3745e-03, -6.1483e-03],
        [-1.6745e-03,  3.1324e-03, -9.1068e-03],
        [ 2.6335e-03,  6.6175e-03, -2.8031e-03],
        [ 1.2951e-02, -5.3615e-03, -1.5196e-02],
        [ 7.7520e-03,  1.1026e-02,  2.7159e-03],
        [ 1.1836e-02, -1.3380e-02, -2.9299e-03],
        [ 8.3882e-03, -1.1874e-02, -8.5715e-03],
        [ 7.6518e-03, -7.2892e-03,  4.6841e-03],
        [ 1.7875e-02, -7.9421e-03,  7.4683e-04],
        [ 1.2959e-02,  7.2856e-04, -6.7431e-03],
        [-3.0937e-03, -1.9120e-02, -9.0276e-04],
        [ 1.0590e-02, -9.0028e-03,  8.4562e-03],
        [ 1.1841e-02, -7.7711e-03, -2.9633e-03],
        [-7.3280e-03, -1.4414e-02,  1.0490e-02],
        [ 9.6634e-04, -8.9256e-04, -4.6268e-03],
        [-2.0877e-03,  3.8197e-03, -1.0746e-02],
        [ 1.4166e-02,  5.4481e-03, -1.1773e-02],
        [ 1.1866e-02, -1.5301e-02,  9.1998e-04],
        [ 2.2449e-03, -1.7613e-03, -1.4695e-02],
        [ 1.4132e-02, -1.1271e-02,  1.9088e-03],
        [ 9.1506e-03, -1.0695e-02, -7.4587e-03],
        [ 1.4095e-02, -2.4586e-03, -8.3791e-04],
        [ 4.1112e-03, -1.5571e-02, -6.8319e-03],
        [ 9.6300e-03, -2.3559e-03, -7.0938e-03],
        [ 2.5370e-02, -1.1411e-02, -5.5901e-03],
        [-1.6682e-03, -3.9642e-03, -4.7387e-03],
        [ 1.3328e-02, -6.0845e-03,  1.3930e-03],
        [ 5.2460e-03,  6.0421e-04, -3.6662e-03],
        [ 3.1891e-03,  1.3926e-03, -2.5311e-03],
        [ 5.9684e-03, -4.2573e-03, -1.0285e-02],
        [ 1.3404e-02,  1.4469e-03, -5.0841e-04],
        [-2.1096e-03, -1.8437e-02,  2.8925e-03],
        [-7.7983e-03, -1.1402e-02,  8.4274e-03],
        [ 1.4636e-02, -4.3596e-03,  8.4172e-03],
        [ 9.6486e-03, -6.2739e-03,  1.3394e-03],
        [ 1.6574e-02, -4.5466e-03,  2.1766e-03],
        [ 1.0617e-02,  4.3371e-03, -1.2861e-02],
        [-4.2130e-03,  6.4327e-03, -1.5432e-02],
        [ 1.0772e-02, -6.8541e-03, -1.3876e-02],
        [ 6.5473e-03, -5.9323e-03,  4.9641e-03],
        [ 1.0660e-02,  7.7947e-04,  3.4742e-03],
        [ 9.6315e-03, -7.1459e-03,  7.6707e-04],
        [ 4.5019e-03,  5.5495e-03, -1.3691e-02],
        [ 1.1070e-03, -1.1931e-02,  1.5671e-03],
        [ 1.5801e-02,  4.0271e-03, -7.9807e-03],
        [ 1.5639e-02, -8.2721e-03,  6.1969e-03],
        [ 1.5243e-02, -1.2273e-02, -5.8784e-03],
        [ 8.6129e-03, -3.4377e-03, -2.7724e-04],
        [ 1.1926e-02,  1.0583e-02, -3.1941e-03],
        [ 9.2962e-03, -3.6409e-03, -1.5013e-02],
        [ 1.2828e-02, -6.6281e-03,  4.6862e-03],
        [ 1.3898e-02, -3.1242e-03,  1.1984e-03],
        [ 1.3464e-02, -6.6177e-03,  4.1295e-03],
        [ 1.5321e-02, -2.5779e-03, -1.2141e-02],
        [ 8.7896e-03, -1.4777e-02, -4.3283e-03],
        [ 9.8625e-03, -3.2139e-03, -6.8495e-03],
        [ 6.9935e-03, -1.8113e-03, -5.4917e-03],
        [ 1.2010e-02, -7.5898e-03, -2.5277e-03],
        [ 1.8408e-03, -9.0520e-03,  1.0888e-03],
        [ 1.1623e-02, -3.7105e-03,  4.1124e-03],
        [-8.4155e-03, -8.7907e-03, -4.8394e-03],
        [ 6.0939e-03, -1.1763e-02, -8.0922e-04],
        [ 1.1797e-02, -1.3655e-02,  7.5624e-03],
        [ 6.4619e-03, -2.6743e-03, -1.3569e-02],
        [-3.9022e-03,  2.0024e-03,  6.1751e-03],
        [ 4.2140e-03, -1.6825e-02, -2.7364e-03],
        [ 6.0547e-03,  5.3567e-03, -1.2271e-02],
        [ 2.1145e-02, -7.9072e-03, -6.1556e-03],
        [ 1.2367e-02,  6.9097e-03, -2.0269e-03],
        [ 1.0283e-02, -6.2198e-03,  2.3532e-03],
        [ 7.9833e-03, -5.3639e-03, -6.8374e-04],
        [ 1.5727e-02, -1.0311e-02, -6.8852e-03],
        [ 5.4153e-04,  5.0096e-03, -1.3556e-02],
        [ 6.0515e-04, -1.4921e-04, -8.4851e-03],
        [-3.3290e-03, -1.1545e-02, -1.1114e-02],
        [ 1.3275e-02,  5.5334e-03,  6.8754e-03],
        [ 2.0603e-02, -3.9015e-03, -6.5705e-05]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000041
penultimate_layer.0.bias: grad mean = 0.001515
output_layer.0.weight: grad mean = 0.001150
output_layer.0.bias: grad mean = 0.041785
[correct] no_actions is False
task_labels:  tensor([0, 1, 2, 1, 2, 0, 1, 1, 1, 1, 0, 2, 1, 0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 2,
        0, 2, 2, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0, 1, 2, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 2, 2, 0, 2, 2, 2, 1, 1, 0, 2, 1, 1, 0, 2, 2, 1, 2, 2, 0, 2, 2, 0, 1,
        1, 2, 2, 1, 1, 0, 2, 0, 1, 2, 0, 1, 2, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        2, 0, 0, 2, 1, 2, 2, 0, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 0, 1, 2, 1,
        0, 1, 1, 0, 1, 2, 1, 1]) , task_preds: tensor([[-2.3266e-03, -3.3117e-04,  9.2381e-03],
        [ 7.1883e-03, -2.1094e-03, -4.1177e-03],
        [-1.1012e-02, -1.8005e-02, -2.1596e-03],
        [ 9.9419e-03, -1.0532e-02, -1.0203e-02],
        [ 5.6525e-03,  4.2402e-04, -2.6561e-04],
        [ 1.2976e-02,  1.1411e-03, -5.8790e-03],
        [-8.9555e-04, -1.2231e-03, -6.6006e-03],
        [ 3.1562e-03, -1.1642e-02, -6.6004e-03],
        [ 7.9667e-03, -2.5877e-04, -5.6199e-03],
        [ 8.2508e-03, -5.2005e-03,  4.8042e-03],
        [ 1.4613e-02, -6.0315e-03, -1.4340e-02],
        [ 4.5522e-03, -2.9205e-03, -4.4343e-03],
        [ 1.3942e-02, -3.3584e-03, -9.7540e-03],
        [ 8.6452e-03, -9.9092e-03, -1.9762e-03],
        [ 1.3128e-02, -1.2535e-02, -1.3880e-03],
        [ 1.8954e-02, -5.0990e-03,  6.7722e-03],
        [ 1.5194e-02, -8.4741e-03, -9.0844e-03],
        [ 2.1316e-02,  2.2717e-03,  1.4965e-03],
        [ 7.2390e-04, -1.0204e-02,  4.5372e-03],
        [-2.5694e-04, -8.3907e-03,  1.1766e-02],
        [ 1.3672e-02, -9.8863e-03, -1.1749e-03],
        [ 1.5434e-02, -2.6826e-03, -1.2121e-02],
        [-1.2103e-03, -2.5104e-03, -5.7616e-03],
        [ 1.0036e-02, -1.2794e-02, -1.9520e-03],
        [ 2.1983e-02, -6.8431e-03, -1.0561e-02],
        [ 7.5028e-03, -3.2538e-03, -1.6545e-02],
        [ 1.8324e-02,  5.3719e-04, -5.3127e-03],
        [ 1.0250e-03, -3.3589e-03, -3.6666e-03],
        [-1.8937e-03, -1.8599e-02,  2.8907e-03],
        [ 1.1918e-03, -5.3656e-03, -1.1968e-02],
        [ 2.2776e-02,  1.6571e-03, -7.5265e-03],
        [ 9.8271e-03, -1.6386e-02, -5.7593e-03],
        [ 1.6153e-02,  2.7955e-03,  7.1754e-03],
        [ 1.3489e-02,  4.1219e-03, -7.5049e-03],
        [ 6.6201e-03,  7.3893e-03, -1.2413e-03],
        [ 8.1083e-03, -5.3734e-03, -7.7394e-04],
        [ 6.9048e-03, -1.0880e-02, -4.3149e-03],
        [ 4.0636e-03,  1.1760e-04,  2.2674e-03],
        [ 7.3979e-04,  4.9134e-03, -8.3732e-03],
        [ 3.1503e-04, -6.5521e-03, -5.7897e-03],
        [ 2.5403e-03, -2.1578e-03,  5.9285e-04],
        [ 1.2970e-02, -2.9426e-03, -9.4786e-04],
        [ 6.9098e-03, -8.4660e-03,  4.5355e-03],
        [ 8.5760e-03, -4.4778e-03, -4.4338e-03],
        [ 1.2558e-02, -7.2855e-03,  4.3153e-04],
        [ 1.3676e-02, -4.3025e-03,  5.6155e-03],
        [ 1.6771e-02, -6.8524e-03, -1.3351e-02],
        [ 2.8930e-04, -1.2573e-02, -5.3959e-03],
        [ 9.6282e-03, -1.5667e-03,  4.1772e-03],
        [ 8.8731e-04, -1.0606e-02, -1.2560e-02],
        [-1.2536e-03, -1.0311e-03,  8.2823e-03],
        [ 1.4557e-03, -1.0777e-03,  2.3903e-03],
        [ 1.2665e-02,  1.0215e-02,  3.7121e-03],
        [ 1.1818e-02, -3.7834e-03,  4.0336e-03],
        [ 7.0321e-03, -9.3245e-03, -4.2587e-03],
        [ 7.0688e-03, -7.6242e-03, -3.2767e-03],
        [ 1.9384e-03,  1.0080e-03, -9.1017e-03],
        [ 6.5599e-03, -6.7941e-03,  2.6836e-06],
        [ 1.6453e-02, -7.1870e-03, -1.6755e-03],
        [ 1.3211e-02,  5.7963e-03,  8.2713e-03],
        [ 1.1998e-03, -1.0373e-02,  4.5946e-03],
        [ 1.3552e-02, -3.9261e-03, -1.8158e-03],
        [ 6.2052e-03,  2.8139e-03, -1.2077e-02],
        [ 1.1841e-02, -1.2699e-02, -6.1021e-03],
        [-5.6060e-03, -1.7960e-02, -6.7432e-03],
        [-3.6124e-03, -1.1104e-02,  8.5794e-03],
        [-3.0279e-03, -1.1977e-02, -1.1133e-02],
        [ 1.2611e-02, -4.6272e-03, -4.2574e-03],
        [ 2.2149e-02, -7.5221e-03, -5.3033e-03],
        [ 9.6484e-04, -1.1983e-02,  1.5432e-03],
        [ 8.2411e-03, -1.1363e-02,  3.9617e-03],
        [ 7.8641e-03, -5.5684e-03, -1.5764e-03],
        [-3.5569e-03, -1.9109e-03,  4.6803e-03],
        [ 3.5355e-03, -2.1360e-03,  1.5730e-03],
        [ 1.5329e-02, -5.7289e-03,  4.1573e-03],
        [ 1.6590e-02, -7.6087e-03, -7.9802e-03],
        [ 1.0563e-02,  2.1963e-03, -6.5474e-03],
        [ 7.3998e-03, -1.0470e-03, -9.7901e-03],
        [-2.7627e-04, -7.6874e-03, -1.0722e-02],
        [ 1.9899e-02, -2.0860e-02, -4.4412e-03],
        [ 1.5497e-02, -3.6647e-03, -2.7816e-03],
        [ 1.5171e-02,  1.2022e-03,  3.2439e-03],
        [ 9.9026e-03, -1.1093e-02, -1.7915e-03],
        [ 5.2927e-03, -2.9817e-03, -1.9033e-03],
        [ 1.9849e-02, -1.4755e-02, -5.7805e-03],
        [ 1.0744e-02, -9.7739e-03, -2.7580e-03],
        [ 2.3843e-02,  1.4037e-03, -2.1667e-03],
        [-2.0496e-03, -1.5408e-02, -5.0555e-03],
        [ 6.7173e-04, -7.2929e-03, -6.6182e-03],
        [ 1.8501e-02, -1.1468e-02, -1.1749e-02],
        [ 1.5079e-03, -1.2398e-03, -6.0655e-04],
        [ 2.9911e-03,  8.0780e-03,  2.4033e-03],
        [ 1.4223e-02,  1.2605e-03,  2.5249e-03],
        [ 9.6961e-03, -5.8891e-03,  2.7207e-03],
        [ 9.5042e-03, -2.6119e-03,  9.2064e-03],
        [-2.4943e-04, -4.9560e-03, -1.1348e-03],
        [ 8.5321e-03, -8.1462e-03,  6.6298e-03],
        [ 1.4575e-02, -2.0082e-03,  1.8591e-03],
        [ 1.5012e-02, -1.5664e-02,  6.5920e-03],
        [ 3.3983e-03,  4.2022e-03,  7.0623e-03],
        [ 1.2894e-02,  6.3301e-03,  1.2498e-03],
        [ 2.1532e-02,  1.0295e-04,  1.8978e-03],
        [ 1.4700e-02, -4.9027e-04, -1.0706e-02],
        [ 1.2533e-02, -5.2327e-03,  2.5180e-03],
        [-1.6142e-03,  1.7791e-03, -1.0029e-02],
        [ 2.1663e-02, -2.3854e-03,  3.6543e-03],
        [ 1.7713e-02, -1.4991e-02, -7.3323e-03],
        [ 7.8301e-03,  6.7974e-03, -9.5308e-03],
        [ 1.4220e-02, -2.3147e-05,  3.3873e-03],
        [ 3.0802e-02, -6.9478e-03, -1.0349e-02],
        [ 8.2105e-03, -3.0880e-03,  2.9707e-03],
        [ 6.7083e-03,  7.9963e-05,  4.0841e-03],
        [-1.6431e-03, -3.9665e-03,  4.4387e-03],
        [ 1.9232e-02,  5.6358e-03, -1.0849e-02],
        [ 1.3149e-02, -4.6903e-03,  6.9878e-03],
        [ 9.4090e-03, -4.0701e-03, -5.8392e-03],
        [ 1.9736e-02, -5.2618e-03, -3.5247e-03],
        [ 1.0990e-02, -9.7884e-03, -8.2435e-03],
        [ 1.4253e-02,  1.6186e-04,  4.1852e-03],
        [-1.1479e-02, -3.2659e-03, -1.8151e-03],
        [ 2.0069e-03, -9.1533e-03, -5.9690e-04],
        [ 8.2508e-03, -5.2005e-03,  4.8042e-03],
        [ 1.3179e-02, -1.7716e-03,  3.1561e-04],
        [ 3.4952e-03,  6.3261e-03, -4.1504e-03],
        [ 1.8026e-02, -8.1014e-03,  7.9380e-04],
        [ 1.0964e-03, -1.0551e-02, -2.5262e-03],
        [ 4.3534e-03, -1.0031e-02, -3.7003e-03],
        [ 2.3546e-03, -2.6248e-03,  2.2124e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000041
penultimate_layer.0.bias: grad mean = 0.001565
output_layer.0.weight: grad mean = 0.001309
output_layer.0.bias: grad mean = 0.043364
[correct] no_actions is False
task_labels:  tensor([1, 1, 2, 1, 0, 2, 2, 1, 2, 0, 0, 1, 2, 2, 1, 1, 0, 1, 0, 2, 1, 1, 1, 0,
        0, 2, 0, 1, 0, 1, 1, 1, 0, 2, 1, 0, 2, 0, 2, 2, 1, 0, 2, 0, 2, 1, 2, 0,
        0, 2, 2, 1, 2, 2, 1, 0, 1, 1, 1, 2, 1, 2, 2, 2, 1, 0, 1, 0, 1, 1, 1, 1,
        2, 0, 1, 0, 2, 1, 2, 2, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 0, 1, 2, 1, 0,
        2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 0, 0, 1, 1, 1, 1, 2, 2, 1,
        1, 0, 1, 1, 2, 2, 0, 0]) , task_preds: tensor([[ 1.9460e-02, -1.0495e-02, -3.0882e-04],
        [ 1.6660e-03, -2.2174e-03,  5.5543e-03],
        [ 1.1856e-02, -5.1607e-03, -7.6668e-03],
        [ 6.0460e-03, -2.5307e-03,  7.2711e-04],
        [ 4.7518e-03, -5.7877e-03,  5.7683e-03],
        [ 1.8197e-02,  2.9490e-03,  2.5582e-03],
        [ 9.4428e-03, -4.5766e-03, -1.4702e-03],
        [ 1.4539e-02,  5.7415e-03, -4.7420e-04],
        [ 1.0433e-02, -6.4709e-03,  1.5624e-03],
        [ 1.0882e-02, -1.4254e-03,  2.7958e-03],
        [ 1.1112e-02, -5.5942e-03, -4.1016e-03],
        [ 2.7795e-03, -6.8965e-03, -8.4515e-04],
        [ 4.7656e-03,  3.5579e-03, -1.2686e-02],
        [-2.2096e-03, -2.5533e-03,  1.2015e-02],
        [ 8.6610e-03, -3.9704e-03, -5.2470e-03],
        [ 5.2922e-03, -7.3996e-03, -1.4196e-03],
        [ 8.7819e-03, -5.9496e-03, -8.2663e-03],
        [-1.1473e-02,  2.8988e-03,  3.6113e-03],
        [ 1.2929e-02, -1.0853e-02, -1.5125e-03],
        [ 9.6167e-03, -7.4207e-03,  1.0627e-03],
        [ 7.7616e-03, -5.0080e-03, -6.7479e-03],
        [ 1.8566e-02, -4.1132e-03,  1.5954e-03],
        [ 8.1965e-03, -1.9698e-03, -7.0116e-03],
        [ 1.0011e-02, -2.7536e-03,  5.0444e-03],
        [ 2.2744e-02, -9.0889e-03, -1.9248e-02],
        [-1.6319e-03,  2.3838e-03,  3.0656e-03],
        [ 2.3206e-02, -8.2251e-03,  5.6286e-03],
        [ 7.7357e-04, -6.3488e-03,  7.5143e-03],
        [ 9.8645e-03, -9.5738e-03,  2.5916e-03],
        [ 1.0882e-03, -3.2094e-03,  2.5793e-04],
        [ 8.4530e-03, -5.2052e-03,  4.6502e-03],
        [ 3.2053e-03, -8.9919e-03,  5.6580e-03],
        [ 1.6016e-02, -6.7936e-03, -4.6819e-03],
        [ 1.3524e-02, -5.9826e-03, -2.3185e-03],
        [ 8.0028e-03, -3.4448e-03, -7.5162e-03],
        [ 1.9274e-02, -6.0323e-04, -6.1185e-03],
        [ 5.9492e-03, -3.4739e-03, -1.1466e-02],
        [ 1.0463e-02, -6.7999e-03,  1.5117e-03],
        [ 4.5111e-03, -2.0529e-04,  3.6927e-03],
        [ 7.6523e-03, -4.4684e-03, -8.2896e-03],
        [ 6.0170e-03,  5.4296e-03, -1.2322e-02],
        [ 1.0245e-02, -6.8308e-03,  8.9613e-03],
        [-4.3110e-03, -1.1041e-02,  6.3284e-03],
        [ 2.1053e-02, -5.3250e-03,  4.2933e-03],
        [ 1.3269e-02, -4.7014e-04, -4.6168e-03],
        [ 1.0583e-02, -1.6579e-02,  2.9920e-03],
        [-2.5443e-03, -1.7539e-02, -6.5161e-03],
        [ 1.4326e-02,  2.5767e-03, -3.1770e-03],
        [ 1.8559e-02, -1.2277e-02, -3.1146e-03],
        [ 7.1178e-03, -7.0603e-03,  1.1864e-02],
        [-4.7952e-03, -6.4989e-03,  6.3684e-03],
        [ 1.4691e-02, -9.8781e-03,  6.1494e-03],
        [ 6.8517e-03,  1.3863e-03, -3.1328e-03],
        [ 6.7837e-03,  5.6134e-06, -1.1326e-04],
        [-4.6820e-03, -1.0000e-02, -1.7221e-03],
        [ 1.4750e-02,  3.5135e-06, -7.0221e-03],
        [ 1.1928e-02,  3.3457e-03, -1.5611e-02],
        [ 7.8590e-03, -7.8403e-03, -1.6057e-03],
        [ 5.1060e-03,  3.4886e-03, -4.6272e-03],
        [ 1.1955e-02,  6.5567e-04,  1.2120e-02],
        [ 1.4849e-02, -6.1258e-03,  4.4445e-03],
        [ 8.4530e-03, -5.2052e-03,  4.6502e-03],
        [ 2.1885e-03, -7.7579e-03,  7.1452e-03],
        [ 1.2726e-02, -2.5389e-03, -1.9508e-03],
        [ 2.3240e-02,  8.0637e-03, -7.4512e-03],
        [ 1.7942e-03, -9.8725e-03, -6.3074e-03],
        [ 1.6541e-02,  1.0538e-03,  1.0063e-02],
        [ 2.1127e-02, -1.2681e-02, -7.1792e-03],
        [ 4.2322e-03, -1.5766e-02, -6.7237e-03],
        [ 7.2889e-03,  5.3494e-03, -6.1135e-03],
        [ 9.1231e-03,  3.5234e-03,  9.9104e-03],
        [ 8.1387e-03,  3.2169e-03, -7.4326e-03],
        [ 1.7469e-02,  2.9097e-03, -1.6768e-03],
        [ 4.6104e-03, -6.1396e-03, -7.7028e-04],
        [ 1.5307e-02,  6.5167e-03, -1.2024e-02],
        [ 1.6825e-02, -1.1492e-02,  2.2971e-03],
        [ 1.4622e-02, -3.3649e-03, -4.2632e-03],
        [ 7.9064e-03,  3.5719e-03,  4.0007e-03],
        [ 2.6404e-03, -3.6933e-03, -2.9205e-03],
        [ 8.4530e-03, -5.2052e-03,  4.6502e-03],
        [ 1.4219e-02, -1.6031e-02,  5.1045e-03],
        [ 3.5061e-03,  1.2929e-03, -5.7922e-04],
        [ 1.1648e-02, -1.4580e-02, -9.0926e-04],
        [ 1.3410e-02, -2.9726e-03, -1.9425e-03],
        [ 1.7633e-04, -4.2534e-03, -3.7146e-03],
        [ 2.4477e-02, -1.2143e-02, -1.8480e-03],
        [ 4.5859e-03, -9.1208e-04, -1.0753e-02],
        [ 1.2818e-02, -1.2633e-02, -3.2592e-03],
        [ 1.0288e-02, -2.7982e-03,  1.0490e-02],
        [ 1.0578e-02, -2.7738e-03, -5.3063e-03],
        [ 6.6152e-03, -7.7020e-03, -2.5578e-03],
        [ 6.2103e-03, -6.7203e-03, -5.4040e-03],
        [ 7.0351e-03, -2.4454e-03, -1.2166e-03],
        [ 6.9336e-03, -9.0385e-03,  2.0793e-03],
        [ 1.7378e-02, -1.1773e-02, -3.3770e-03],
        [ 1.0748e-02, -3.3727e-03,  6.5590e-03],
        [ 1.0840e-02, -8.3635e-03, -4.7734e-03],
        [ 9.5878e-03, -6.8233e-03, -3.0550e-03],
        [ 9.9322e-03,  5.5978e-03, -2.5038e-03],
        [ 2.9900e-03, -8.0181e-03,  3.5019e-03],
        [ 1.9138e-03,  1.9273e-03,  1.1387e-02],
        [ 1.2704e-02, -7.8730e-03, -5.1596e-03],
        [ 6.8693e-03, -6.9092e-03, -1.4911e-03],
        [-1.3390e-02, -1.5784e-02,  5.5825e-03],
        [ 1.2383e-02, -1.2174e-02, -9.0088e-03],
        [ 1.1412e-02, -6.1005e-03,  5.9055e-03],
        [ 1.2445e-02,  3.8547e-04, -2.3842e-03],
        [ 1.2731e-02, -1.4082e-02, -5.9823e-03],
        [ 1.0436e-02, -2.9293e-03, -1.0145e-02],
        [ 7.8441e-03, -5.8499e-03,  1.3325e-03],
        [ 5.3888e-03,  4.2171e-04, -8.5845e-04],
        [ 1.7247e-02, -1.4895e-02,  2.8328e-03],
        [ 1.4951e-02, -9.2950e-03,  9.1039e-03],
        [ 3.6243e-03, -4.8138e-03, -7.6143e-03],
        [ 1.3628e-02, -3.5338e-03, -6.5702e-04],
        [ 1.1805e-02, -1.7760e-03, -1.8787e-04],
        [-3.5763e-03, -4.8610e-03,  7.1157e-03],
        [-4.6189e-03, -1.6487e-03,  8.2784e-03],
        [ 8.3961e-03,  5.6496e-04, -1.4020e-03],
        [ 7.8794e-03, -1.2636e-02, -1.1502e-02],
        [-3.1697e-03, -3.3917e-03, -4.0039e-03],
        [ 1.0293e-02,  6.8563e-03,  8.1943e-04],
        [ 8.8315e-03,  3.2835e-03,  1.0901e-02],
        [ 6.3755e-03,  8.8037e-04,  5.2149e-03],
        [ 1.2820e-02,  8.7161e-04,  1.1738e-03],
        [ 1.9506e-03, -2.5240e-02,  7.9289e-03],
        [ 2.1614e-02,  2.2864e-03,  5.8993e-03],
        [ 2.0616e-02,  5.8148e-03, -4.2800e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000028
penultimate_layer.0.bias: grad mean = 0.000601
output_layer.0.weight: grad mean = 0.000886
output_layer.0.bias: grad mean = 0.017143
[correct] no_actions is False
task_labels:  tensor([2, 2, 2, 2, 0, 2, 2, 0, 1, 0, 1, 2, 2, 2, 0, 1, 1, 1, 1, 2, 2, 1, 2, 2,
        0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0,
        1, 2, 0, 0, 1, 2, 0, 2, 1, 1, 2, 2, 0, 0, 1, 1, 0, 2, 0, 1, 1, 2, 2, 2,
        1, 0, 0, 2, 2, 2, 1, 1, 2, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1,
        2, 0, 2, 2, 1, 2, 0, 1, 0, 1, 2, 0, 1, 2, 2, 0, 1, 2, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 2, 2, 2, 0, 1]) , task_preds: tensor([[ 1.4118e-02, -3.1744e-03,  1.0780e-03],
        [ 1.3338e-02, -1.2375e-02, -3.4089e-03],
        [ 1.1770e-02,  1.6971e-03,  1.1922e-03],
        [ 8.4509e-03, -5.2392e-03,  4.6858e-03],
        [ 1.2858e-02, -1.2967e-02, -1.4757e-02],
        [ 2.2021e-02,  1.3375e-03, -4.3970e-03],
        [ 1.7753e-03, -1.3126e-02,  3.9977e-04],
        [ 9.2995e-03,  2.7253e-03, -1.4745e-03],
        [ 4.4487e-04, -2.3230e-03, -3.5948e-03],
        [ 1.6166e-02, -5.1731e-04,  3.3960e-03],
        [ 1.5955e-02, -5.5146e-03, -1.1439e-02],
        [ 1.2621e-02,  1.5402e-03, -2.3677e-04],
        [-1.3125e-03, -9.2346e-03,  5.0630e-03],
        [ 2.1758e-02, -4.7824e-03,  6.8067e-05],
        [ 2.1165e-03, -5.7093e-03,  3.5958e-03],
        [ 1.5379e-02, -3.7672e-03, -2.5843e-03],
        [ 1.1079e-02, -9.9532e-03, -8.7949e-03],
        [-3.2937e-03, -1.1508e-02,  1.0553e-03],
        [ 3.7100e-03, -3.1439e-03,  7.0023e-03],
        [ 1.2343e-02,  4.1721e-03, -3.6778e-03],
        [ 1.9186e-03, -2.5342e-02,  8.0594e-03],
        [-2.5672e-03, -1.3749e-02, -2.8855e-03],
        [ 1.2505e-02, -5.2337e-03, -1.0017e-02],
        [ 1.4548e-02, -3.5721e-03, -3.9908e-03],
        [ 1.9431e-02,  8.1592e-04,  5.2298e-03],
        [ 2.2058e-02,  2.2072e-03,  1.0250e-02],
        [ 1.0231e-02, -6.2848e-03, -1.0392e-02],
        [ 3.5434e-03,  5.1384e-03,  6.2511e-03],
        [ 2.5673e-02, -5.1802e-03,  5.7009e-03],
        [ 4.9736e-03, -3.2865e-03, -3.1053e-03],
        [ 3.7071e-03, -6.7839e-03, -3.4485e-03],
        [ 1.9035e-02, -1.8920e-04, -5.2497e-03],
        [ 1.5996e-02, -4.0448e-03,  3.9029e-03],
        [ 3.0343e-03, -7.1227e-04,  3.5879e-04],
        [ 1.2329e-02, -1.4164e-02, -5.5707e-03],
        [ 1.4512e-02, -6.7505e-03, -4.0865e-03],
        [ 1.7823e-03, -4.4763e-03,  9.6406e-03],
        [ 2.5840e-02,  1.0422e-03,  5.8618e-05],
        [ 9.5253e-04,  2.4088e-03, -1.2422e-03],
        [ 1.5649e-02, -3.9620e-03, -1.1561e-02],
        [ 8.4979e-03, -1.1540e-02,  3.9420e-03],
        [ 1.4875e-02, -6.5497e-03, -2.8223e-04],
        [ 1.0583e-03, -1.2054e-02,  1.5425e-03],
        [ 1.4717e-02, -9.4907e-03, -2.7092e-03],
        [ 6.8496e-03, -3.3483e-03, -7.1566e-03],
        [ 1.8342e-03, -4.9920e-03,  1.4375e-03],
        [ 8.4509e-03, -5.2392e-03,  4.6858e-03],
        [ 2.1400e-03, -1.0009e-02, -3.1327e-05],
        [ 1.2935e-02,  3.9968e-04, -6.3873e-03],
        [ 1.0220e-03, -1.4993e-02,  4.7282e-04],
        [ 2.1124e-02, -4.4557e-03,  9.9533e-05],
        [ 1.4810e-02, -6.2849e-03, -1.4231e-02],
        [ 1.6609e-04, -1.0379e-02,  1.4326e-03],
        [ 8.4509e-03, -5.2392e-03,  4.6858e-03],
        [ 2.4114e-02,  1.1448e-03, -2.1102e-03],
        [ 8.7995e-05, -4.3736e-03, -3.5210e-03],
        [ 4.3856e-03,  1.1958e-03, -5.0710e-03],
        [ 8.4509e-03, -5.2392e-03,  4.6858e-03],
        [ 2.8914e-03, -4.9580e-03,  5.1424e-03],
        [ 9.4063e-03, -3.1742e-03, -6.7480e-03],
        [ 1.1416e-03, -3.4888e-03, -3.6236e-03],
        [ 1.8774e-02, -1.1746e-02, -1.1675e-02],
        [ 5.9800e-03, -1.2177e-02, -2.9111e-04],
        [ 4.5403e-03, -1.0209e-02, -3.6628e-03],
        [ 8.6297e-03, -1.0599e-02, -1.1139e-02],
        [ 1.0562e-02, -1.5420e-02,  3.6307e-03],
        [ 8.8400e-03, -1.6798e-02, -5.6170e-03],
        [ 1.5197e-03,  8.6396e-04,  4.0792e-04],
        [ 1.9729e-03, -6.3698e-03,  1.1603e-02],
        [ 1.5380e-02, -7.9832e-03, -1.1039e-02],
        [ 1.2707e-02,  7.1122e-04,  1.4289e-03],
        [ 7.4705e-03, -9.8974e-04,  4.2237e-03],
        [ 7.3694e-03, -5.1425e-03,  5.1410e-03],
        [ 1.1302e-02, -3.3650e-03, -5.1016e-04],
        [ 1.1400e-03, -1.7879e-02, -3.1237e-03],
        [ 1.0175e-02, -3.1058e-03,  5.7415e-03],
        [ 1.9274e-02, -4.4865e-03, -3.0270e-03],
        [ 1.9276e-03, -7.8203e-03,  1.9782e-03],
        [ 1.5192e-02,  6.5038e-03, -1.1922e-02],
        [-3.4264e-03,  9.9237e-03,  1.4356e-02],
        [ 1.3738e-02, -1.0146e-02, -4.6627e-03],
        [ 1.5863e-02, -8.4947e-03,  6.2473e-03],
        [ 5.6587e-03, -7.7397e-05, -1.5719e-03],
        [ 1.1186e-02, -5.0523e-03, -6.8499e-03],
        [ 1.4474e-02,  5.6484e-03, -3.2717e-04],
        [ 1.5177e-02, -1.5948e-02,  6.7569e-03],
        [ 2.9918e-03, -4.2224e-03,  5.3492e-03],
        [-4.8262e-03, -1.5057e-02, -6.2374e-03],
        [ 1.5855e-02, -9.9834e-03,  4.2147e-03],
        [ 1.4720e-02, -9.4137e-03,  3.0935e-03],
        [ 5.3626e-03, -4.7951e-03,  3.0317e-03],
        [ 1.1000e-03, -8.7952e-03, -6.3739e-03],
        [ 5.9704e-03, -2.6434e-03,  9.0172e-04],
        [-1.2571e-02, -7.8078e-03, -6.3597e-03],
        [ 1.5087e-02,  5.4757e-03, -1.9399e-03],
        [-1.1355e-02, -1.2274e-02, -9.1630e-03],
        [ 1.7626e-02,  3.5734e-03, -2.2221e-03],
        [ 1.0046e-02, -1.6531e-02, -5.7792e-03],
        [ 1.7033e-02, -3.1125e-03, -3.7443e-03],
        [ 1.3001e-02, -1.4466e-02, -3.7514e-03],
        [ 6.3138e-03, -1.8158e-03,  4.1023e-03],
        [-1.4694e-03, -5.9361e-03, -8.1779e-03],
        [ 1.5932e-02, -6.9458e-03, -4.4590e-03],
        [-4.8008e-03, -9.5005e-03, -3.5441e-03],
        [ 8.4509e-03, -5.2392e-03,  4.6858e-03],
        [ 7.1167e-03, -1.0664e-02,  6.5183e-03],
        [ 1.1336e-02, -6.2618e-03,  6.1319e-03],
        [ 2.4498e-02, -1.2253e-02, -1.7498e-03],
        [ 1.1381e-02, -6.8011e-03,  6.6469e-03],
        [ 6.1207e-03, -4.1365e-03, -5.4762e-03],
        [ 1.0538e-02, -6.4385e-03,  1.0570e-02],
        [ 1.0757e-02, -1.5504e-02,  1.2757e-02],
        [ 1.3227e-02,  2.7459e-03, -8.0898e-03],
        [ 5.8098e-03, -8.2441e-03, -1.7711e-03],
        [ 2.3274e-02,  7.9479e-03, -7.3583e-03],
        [ 4.1241e-03,  5.6426e-03,  7.9017e-03],
        [-4.4424e-04,  2.4025e-03,  6.6609e-04],
        [ 1.0287e-02,  6.6543e-03,  1.0342e-03],
        [ 4.6577e-03, -6.2905e-03, -6.5120e-04],
        [ 1.7807e-02,  2.6954e-03,  1.7381e-03],
        [ 1.4333e-02, -3.8138e-03, -7.9491e-03],
        [ 1.4560e-02, -1.4984e-02,  5.2653e-03],
        [ 3.4304e-03, -3.9200e-03,  5.7706e-03],
        [ 3.6471e-03, -6.4132e-03,  4.1836e-03],
        [ 1.8348e-02, -6.9694e-03, -2.2949e-03],
        [ 7.4576e-03, -8.6316e-03, -6.8163e-03],
        [ 7.8900e-03, -4.5811e-03,  9.4040e-03],
        [ 1.1042e-02, -6.5568e-03,  1.5314e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000038
penultimate_layer.0.bias: grad mean = 0.001405
output_layer.0.weight: grad mean = 0.001056
output_layer.0.bias: grad mean = 0.038640
[correct] no_actions is False
task_labels:  tensor([0, 0, 1, 1, 0, 2, 2, 0, 1, 1, 1, 2, 1, 1, 1, 0, 1, 2, 0, 2, 1, 0, 1, 1,
        1, 2, 1, 2, 0, 1, 2, 0, 1, 1, 0, 1, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 2, 1,
        0, 1, 0, 1, 2, 0, 1, 0, 2, 1, 1, 1, 0, 2, 1, 1, 1, 2, 0, 0, 0, 1, 0, 2,
        1, 0, 2, 2, 0, 1, 1, 2, 2, 1, 0, 0, 0, 0, 2, 0, 0, 2, 1, 2, 0, 2, 2, 1,
        1, 1, 2, 2, 2, 1, 2, 0, 2, 1, 1, 2, 1, 0, 2, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 2, 1, 1, 0, 1, 1]) , task_preds: tensor([[ 1.6765e-03, -1.6613e-03, -3.0337e-04],
        [ 6.3430e-03, -7.0535e-03, -5.1627e-03],
        [ 6.3996e-03,  6.3690e-04,  4.0007e-03],
        [ 4.4730e-03, -1.2921e-03, -4.3287e-03],
        [ 1.4793e-02, -3.6292e-04, -6.6755e-03],
        [ 2.2843e-02, -3.2479e-03,  1.7758e-03],
        [ 1.3519e-02, -6.2457e-03, -7.7564e-04],
        [ 2.7525e-02, -2.8195e-03,  2.6069e-03],
        [ 8.2411e-03, -7.4735e-03,  1.5932e-03],
        [-9.4733e-04, -1.4830e-03, -6.2931e-03],
        [ 1.1512e-02, -1.3906e-02, -2.1326e-03],
        [ 5.9674e-03, -4.6758e-03, -9.8533e-03],
        [-9.5827e-04, -1.5293e-02, -4.6160e-03],
        [ 4.1382e-03, -2.2136e-03,  7.9138e-04],
        [ 3.8380e-03, -1.0951e-02, -3.9844e-03],
        [ 1.0309e-02, -4.8675e-03,  2.2206e-03],
        [ 3.7245e-03, -1.0346e-02, -1.5132e-04],
        [ 1.4082e-02, -1.6381e-02,  5.5741e-03],
        [ 1.2697e-02, -1.7323e-03,  2.2592e-03],
        [ 6.7562e-03, -5.4778e-03, -3.3408e-03],
        [ 1.3099e-02, -6.7453e-03,  6.3342e-03],
        [ 9.4033e-03, -9.2548e-03,  1.1199e-02],
        [ 5.2039e-03, -7.4881e-03, -1.2613e-03],
        [ 1.9627e-03, -3.5632e-03, -6.3870e-03],
        [ 1.1252e-02, -3.8901e-03,  2.4529e-03],
        [ 2.5714e-02, -1.2120e-02, -5.1302e-03],
        [ 9.9958e-03,  3.2998e-03,  4.0266e-03],
        [-1.5409e-03, -7.7699e-03, -6.0616e-03],
        [ 1.1276e-02, -1.0089e-02, -9.7689e-04],
        [ 1.1588e-02,  3.4165e-03, -1.9323e-04],
        [-4.9645e-03, -6.8450e-03,  6.8608e-03],
        [ 1.2879e-02, -5.1437e-03, -3.9318e-03],
        [ 5.8896e-03, -7.9393e-03, -7.6085e-03],
        [ 5.1889e-03, -9.6950e-04,  3.6946e-03],
        [ 9.8826e-04, -1.5711e-02, -1.6925e-03],
        [-1.1721e-02,  2.7299e-03,  3.9780e-03],
        [ 3.5013e-03, -1.0329e-03,  5.2285e-03],
        [ 3.5475e-03, -2.7070e-03,  2.1546e-03],
        [ 5.3293e-03, -1.0994e-02, -4.3674e-03],
        [ 1.0104e-02, -3.1271e-03,  1.0975e-02],
        [ 7.2207e-03, -5.9334e-03,  1.3338e-03],
        [ 1.2336e-02, -8.8489e-03,  1.0717e-02],
        [-9.0377e-04, -1.3662e-02, -3.4570e-03],
        [-5.6259e-05, -5.8480e-03,  1.3601e-02],
        [ 2.1587e-03, -9.3160e-03,  1.1156e-02],
        [ 8.4956e-03, -5.1330e-03, -3.6889e-03],
        [ 1.1748e-02, -9.4974e-03,  3.2258e-03],
        [ 7.1090e-03, -2.2719e-03, -3.8897e-03],
        [ 8.3915e-03, -9.2487e-03, -9.7336e-04],
        [ 7.4626e-03, -5.3425e-03, -6.1684e-03],
        [ 1.0045e-02, -3.0778e-03,  5.3548e-03],
        [ 1.4596e-02,  4.7094e-03, -1.1352e-02],
        [ 2.2117e-02, -7.9179e-03, -4.8669e-03],
        [ 9.0100e-04, -7.7365e-03, -6.3388e-03],
        [ 1.0298e-04, -7.0581e-03, -5.0974e-03],
        [ 7.9866e-03, -5.8548e-03,  7.9198e-03],
        [ 7.4215e-03, -1.6644e-02,  6.3372e-03],
        [ 5.9778e-03,  4.3341e-04, -4.4407e-03],
        [ 1.3863e-02,  1.2711e-03, -6.8977e-04],
        [ 4.0744e-03, -2.8891e-04,  2.6772e-03],
        [ 8.3916e-03, -5.3177e-03,  4.8124e-03],
        [ 9.3568e-03, -7.7488e-03,  1.6053e-03],
        [ 1.0910e-02, -8.6322e-03,  6.5037e-03],
        [ 9.0021e-03,  3.3385e-03,  1.0196e-02],
        [ 1.4700e-02,  1.1506e-02, -2.3681e-03],
        [-3.5679e-03, -2.1636e-02, -1.8970e-03],
        [ 1.2812e-02,  1.6644e-03,  5.2355e-03],
        [ 5.1509e-03, -1.2786e-02, -9.1475e-03],
        [ 2.1955e-02, -7.9849e-03, -1.1237e-02],
        [ 8.3916e-03, -5.3177e-03,  4.8124e-03],
        [ 1.3252e-02, -4.1544e-03, -2.6655e-03],
        [ 1.2951e-02, -8.4087e-03, -1.8395e-03],
        [ 1.0124e-02, -1.2701e-02, -6.8070e-03],
        [ 1.4493e-02, -2.5491e-03, -1.0597e-03],
        [ 1.0158e-02, -6.8046e-03,  2.1212e-03],
        [ 3.0197e-03, -5.7950e-03, -1.6210e-03],
        [ 2.1975e-02, -1.0168e-02, -5.3471e-04],
        [ 7.9995e-04, -6.3370e-03,  2.1152e-03],
        [ 5.6687e-03, -2.2649e-03,  2.2801e-03],
        [ 1.0059e-02, -1.6501e-06, -1.6198e-03],
        [ 8.5610e-03,  4.3961e-04,  2.8399e-04],
        [ 1.2825e-02, -8.5218e-04,  6.2907e-04],
        [ 2.3043e-02,  1.1835e-03, -7.2437e-03],
        [ 5.1477e-03, -3.6890e-03,  3.9748e-03],
        [-2.1023e-03, -7.9419e-04,  9.5428e-03],
        [ 3.7588e-03, -5.8495e-03, -1.6558e-02],
        [ 1.0038e-02, -5.3441e-03,  2.7483e-03],
        [ 9.8438e-03, -1.6386e-03,  2.2868e-03],
        [ 2.1002e-02, -5.6823e-03,  4.7037e-03],
        [ 8.6412e-03,  4.4579e-04,  1.0514e-02],
        [-6.5826e-04, -3.2878e-03, -3.0003e-03],
        [ 3.1389e-03,  5.1436e-03, -4.2773e-03],
        [ 1.4757e-02, -2.4904e-03,  2.2139e-03],
        [ 3.6034e-03, -1.4884e-02,  7.4869e-03],
        [-1.7414e-02, -5.4716e-03,  6.3919e-03],
        [ 6.6792e-03, -1.7637e-03,  8.0497e-03],
        [-1.4647e-03, -2.6103e-03, -5.4599e-03],
        [ 7.1372e-03, -5.6543e-03,  5.3616e-03],
        [ 1.4636e-02, -1.6101e-03, -1.2958e-02],
        [ 3.9298e-03,  5.4845e-03,  2.2228e-03],
        [-2.1755e-03,  6.1661e-03, -5.7728e-03],
        [ 9.6196e-03, -1.7623e-03,  4.3819e-03],
        [ 2.4709e-03, -1.1537e-02,  4.4564e-03],
        [-1.7725e-03, -1.5934e-02, -4.7269e-03],
        [ 1.4084e-02, -8.4992e-03,  2.2624e-03],
        [ 1.0600e-02, -1.6838e-02,  3.2458e-03],
        [ 2.3152e-02, -1.5099e-02, -3.3441e-03],
        [ 9.7640e-03, -6.8914e-03,  1.8820e-03],
        [ 5.2218e-04,  1.1958e-03, -7.8104e-03],
        [ 5.9732e-03, -3.3436e-03,  2.2931e-03],
        [ 1.0291e-02, -2.2623e-04, -4.1575e-03],
        [ 5.6975e-03,  5.3522e-03, -1.1994e-02],
        [ 1.8156e-02, -8.5865e-03,  1.1937e-03],
        [ 1.0436e-02, -6.1900e-03, -3.4259e-03],
        [ 1.0798e-02, -7.8717e-03,  8.3237e-03],
        [ 1.3273e-02, -6.9492e-03, -2.7934e-03],
        [ 1.0777e-02, -3.6401e-03,  6.8109e-03],
        [ 1.0172e-02, -5.7342e-03, -6.9657e-03],
        [ 1.0241e-02, -1.5325e-02, -1.8355e-03],
        [-1.0674e-02, -9.0828e-03, -2.7678e-03],
        [-1.0610e-02, -1.0735e-02,  9.9966e-03],
        [ 1.8497e-02, -1.2585e-02, -2.7477e-03],
        [ 7.6172e-03, -6.1263e-03,  1.7949e-03],
        [ 2.3845e-03, -2.0652e-03, -1.4490e-02],
        [ 7.4607e-03, -5.9189e-03,  3.1470e-03],
        [ 1.6697e-02, -1.8006e-02, -2.4615e-03],
        [ 2.3894e-02, -2.0615e-02,  4.2256e-03],
        [ 1.8407e-02, -1.4611e-02, -5.9059e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000037
penultimate_layer.0.bias: grad mean = 0.001160
output_layer.0.weight: grad mean = 0.001111
output_layer.0.bias: grad mean = 0.032532
[correct] no_actions is False
task_labels:  tensor([1, 2, 1, 2, 1, 0, 1, 1, 1, 0, 2, 0, 2, 0, 0, 2, 2, 0, 1, 1, 2, 0, 2, 1,
        1, 1, 1, 2, 2, 2, 0, 0, 1, 1, 2, 1, 0, 2, 0, 1, 2, 2, 1, 1, 0, 1, 2, 1,
        1, 2, 1, 2, 1, 1, 1, 1, 1, 0, 2, 1, 0, 1, 0, 2, 2, 1, 1, 1, 1, 1, 2, 2,
        1, 0, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 0, 1, 0, 1, 2, 1, 0, 0, 0, 1, 0,
        2, 2, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 2, 2, 1, 0, 1, 0,
        0, 0, 2, 0, 2, 2, 1, 1]) , task_preds: tensor([[ 8.3638e-03, -5.3005e-03,  4.8156e-03],
        [ 1.4768e-02, -5.6332e-03,  3.2019e-03],
        [ 5.7267e-03, -7.6682e-03,  2.1497e-03],
        [-7.5331e-05, -5.9648e-03,  1.3737e-02],
        [ 8.3638e-03, -5.3005e-03,  4.8156e-03],
        [ 1.0574e-02, -1.5741e-02, -4.5490e-03],
        [ 9.7570e-03, -3.3293e-03, -6.6542e-03],
        [ 8.5543e-03, -4.0431e-03, -7.1279e-03],
        [ 5.7433e-03, -1.2090e-02, -4.5532e-03],
        [ 3.4683e-03, -1.3858e-02,  7.4216e-03],
        [-3.1958e-03, -1.2449e-02, -1.0514e-02],
        [ 5.3409e-03, -6.5146e-03, -9.8630e-04],
        [ 1.3394e-02, -8.1650e-03, -3.8354e-03],
        [ 1.2862e-02, -5.2542e-03, -3.8045e-03],
        [ 1.2317e-02, -8.9298e-03,  1.0816e-02],
        [-3.0834e-03, -2.3493e-03, -2.8944e-03],
        [ 6.9527e-03,  6.0067e-04, -3.6150e-03],
        [ 8.3638e-03, -5.3005e-03,  4.8156e-03],
        [ 1.8106e-02, -1.1727e-02,  3.9228e-04],
        [ 8.3638e-03, -5.3005e-03,  4.8156e-03],
        [ 3.3181e-03,  4.9403e-03,  6.6310e-03],
        [ 3.9422e-03, -1.6092e-02,  3.3775e-03],
        [ 2.3977e-02, -4.0212e-03, -1.4011e-02],
        [ 3.4028e-03, -4.1739e-03,  2.3601e-03],
        [ 6.0558e-03,  7.2959e-04,  5.6180e-03],
        [ 6.9946e-03, -7.9083e-03,  6.0530e-03],
        [ 8.9249e-03, -4.5297e-03,  3.8278e-03],
        [ 1.4462e-02, -1.8304e-02, -2.3315e-03],
        [ 1.9882e-02, -1.5418e-02, -5.1197e-03],
        [ 7.1357e-03, -1.5923e-03,  6.3718e-03],
        [ 1.6972e-02, -1.1988e-02,  2.6973e-03],
        [ 5.8337e-03, -4.8172e-03,  2.5952e-03],
        [ 9.9146e-03, -6.8648e-03, -7.3648e-03],
        [ 3.9273e-03, -1.6150e-02, -6.0867e-03],
        [ 8.3638e-03, -5.3005e-03,  4.8156e-03],
        [ 2.8689e-03, -9.2447e-03,  6.1809e-03],
        [ 1.8534e-02, -5.7289e-03, -6.2135e-03],
        [ 1.7604e-02, -5.0284e-03,  2.3488e-03],
        [ 1.2886e-02, -1.3269e-02, -1.4466e-02],
        [ 3.1877e-03, -4.5576e-03, -9.3852e-03],
        [ 5.7942e-03, -7.8736e-04,  9.8663e-04],
        [ 9.3041e-03, -5.4831e-03,  9.2546e-04],
        [ 2.7449e-02,  5.1878e-03, -8.5407e-03],
        [ 1.7336e-02, -1.1525e-03, -5.5199e-03],
        [ 1.5160e-02, -1.6257e-02,  7.0896e-03],
        [ 5.5640e-03,  5.3661e-03, -1.1905e-02],
        [-4.9428e-03, -1.5327e-02, -5.8657e-03],
        [ 1.4334e-02,  5.4337e-03,  4.1929e-06],
        [-1.1845e-02,  2.7217e-03,  4.0826e-03],
        [-4.6774e-04, -9.5234e-03, -1.3666e-02],
        [-1.8110e-03, -9.3249e-03, -7.9088e-03],
        [ 1.8548e-02, -3.4143e-03, -1.7770e-02],
        [ 1.4635e-02,  1.1468e-02, -2.2793e-03],
        [ 1.1497e-02,  3.3815e-03, -8.5668e-05],
        [-3.9331e-03, -9.3541e-03,  8.6485e-03],
        [ 3.5450e-03, -2.5840e-03, -5.5734e-03],
        [ 1.1516e-02, -8.3720e-03, -2.0918e-03],
        [ 1.0783e-02, -3.6964e-03,  6.8638e-03],
        [ 7.4333e-03, -4.3760e-03,  5.9390e-03],
        [ 5.2467e-03, -1.1038e-02, -4.2575e-03],
        [ 2.1447e-02, -2.2576e-03, -2.2074e-03],
        [ 8.6297e-03,  3.3530e-03, -4.7907e-04],
        [ 2.1449e-02, -1.1715e-02,  9.0747e-04],
        [ 6.2086e-04, -1.0833e-02, -1.2116e-02],
        [ 9.7108e-03, -6.9675e-03,  2.0015e-03],
        [ 2.7247e-03, -2.2325e-03,  5.2272e-04],
        [ 8.4417e-03, -5.4305e-03, -4.5228e-04],
        [-1.3599e-02, -1.6058e-02,  6.0300e-03],
        [ 1.5380e-02,  3.9449e-03, -1.0538e-03],
        [ 8.7057e-03,  9.8890e-03,  9.6280e-03],
        [ 8.3638e-03, -5.3005e-03,  4.8156e-03],
        [-5.0598e-03, -6.9360e-03,  7.0301e-03],
        [ 1.8127e-02, -7.4785e-03, -5.8437e-03],
        [ 7.9238e-03, -4.8469e-03,  9.6537e-03],
        [ 7.8706e-03, -6.2450e-04, -5.1699e-03],
        [ 3.5135e-03, -1.5019e-02,  7.6970e-03],
        [ 6.7070e-03, -3.2675e-04,  4.2138e-03],
        [-1.2708e-02, -8.0803e-03, -5.9698e-03],
        [ 1.0199e-02, -1.4513e-03, -6.5336e-03],
        [ 9.5923e-03, -8.1831e-03,  1.4349e-03],
        [ 1.5782e-02, -8.6960e-03,  6.5183e-03],
        [ 1.8753e-02, -2.4234e-03,  1.7452e-03],
        [-7.3078e-03, -1.1641e-02,  6.4803e-03],
        [ 8.1476e-03, -5.3697e-03, -8.1403e-04],
        [ 2.5684e-02, -1.2215e-02, -5.0095e-03],
        [ 2.7117e-02, -3.8534e-03, -9.8553e-03],
        [ 1.6672e-02, -7.1131e-03, -1.3005e-02],
        [ 2.7871e-02, -4.0936e-03, -6.2578e-03],
        [ 3.3575e-03, -1.4672e-03,  5.6232e-03],
        [ 7.3435e-03, -1.6746e-02,  6.5034e-03],
        [ 2.3316e-02,  7.7245e-03, -7.1602e-03],
        [ 8.4819e-03, -1.1772e-02,  4.1930e-03],
        [ 1.3635e-02, -3.4201e-03, -1.6562e-03],
        [ 1.7275e-02, -6.9313e-03,  1.1935e-03],
        [ 7.2605e-03, -1.1040e-02, -1.1124e-02],
        [ 1.3297e-02, -1.8692e-04,  3.3248e-03],
        [ 1.2410e-02, -1.4517e-02, -5.2813e-03],
        [ 2.6708e-02, -9.3613e-04, -3.1946e-03],
        [ 1.5423e-02, -2.9545e-03, -1.1832e-02],
        [-4.1367e-04,  2.0605e-03,  9.9783e-04],
        [-5.8631e-03, -9.2399e-04,  1.0935e-02],
        [ 6.3114e-03, -2.5925e-03, -8.7889e-03],
        [ 1.1031e-02, -5.3781e-03, -6.3920e-03],
        [ 1.2523e-02, -2.1852e-03,  2.9570e-03],
        [-1.0417e-03,  1.1970e-03,  7.6090e-03],
        [ 4.4610e-03, -1.0777e-02, -6.8411e-03],
        [ 1.1757e-02,  3.1399e-03, -1.5268e-02],
        [ 1.2769e-02,  1.7164e-04, -6.0228e-03],
        [-1.4682e-03, -7.8186e-03,  1.2442e-02],
        [ 2.4526e-02, -1.2480e-02, -1.5357e-03],
        [ 1.7062e-02, -2.5116e-02,  7.5236e-04],
        [-5.1646e-03, -1.0181e-02, -1.1597e-03],
        [ 4.5251e-03, -6.3302e-03,  6.5088e-03],
        [ 4.4342e-03, -3.8496e-04,  2.1965e-03],
        [-6.6766e-03, -9.4574e-03, -1.4516e-02],
        [ 8.8265e-03, -1.0296e-02,  4.8322e-03],
        [ 1.1090e-02, -1.0994e-02,  2.2339e-03],
        [ 3.6155e-03, -4.5130e-03,  2.2791e-03],
        [ 9.2328e-03, -5.1446e-04,  4.6717e-04],
        [ 1.3467e-02, -7.5936e-03,  5.1395e-03],
        [ 1.2769e-02, -4.5294e-03, -3.8556e-03],
        [ 8.3790e-03, -9.3057e-03, -9.0478e-04],
        [ 1.3992e-02, -1.6456e-02,  5.7223e-03],
        [ 7.2012e-03, -1.8023e-02,  2.2122e-03],
        [ 2.1518e-03, -8.1560e-03,  7.5881e-03],
        [ 1.2029e-02, -3.9342e-03,  4.0201e-03],
        [-1.0748e-02, -1.0756e-02,  1.0125e-02],
        [ 2.4085e-02, -3.4850e-03, -1.1851e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000031
penultimate_layer.0.bias: grad mean = 0.000873
output_layer.0.weight: grad mean = 0.000952
output_layer.0.bias: grad mean = 0.028915
[correct] no_actions is False
task_labels:  tensor([0, 1, 2, 1, 1, 0, 2, 2, 0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2,
        1, 1, 1, 1, 2, 0, 0, 2, 0, 1, 0, 1, 2, 0, 1, 2, 1, 1, 0, 2, 2, 2, 1, 1,
        1, 0, 0, 2, 2, 0, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 2, 0, 0, 0, 2, 2, 0, 1,
        1, 1, 1, 0, 2, 1, 2, 1, 2, 0, 0, 0, 2, 1, 2, 0, 1, 1, 1, 0, 2, 1, 0, 1,
        1, 0, 1, 0, 2, 1, 0, 1, 0, 0, 1, 0, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 2, 2,
        1, 0, 2, 1, 0, 0, 1, 0]) , task_preds: tensor([[ 1.2314e-02, -7.6545e-03,  1.8447e-03],
        [ 1.1214e-02, -1.3438e-02, -1.2154e-03],
        [ 1.9067e-02, -4.8464e-03, -2.4928e-03],
        [ 2.8400e-03, -1.3816e-02,  6.8522e-03],
        [ 1.4534e-04,  8.3496e-04,  6.4453e-03],
        [ 1.0051e-02, -1.6708e-02, -5.5997e-03],
        [-1.1322e-02, -1.8595e-02, -1.3037e-03],
        [ 9.1602e-03, -4.8307e-03, -9.8683e-04],
        [ 1.4422e-02, -2.4529e-03, -1.1043e-03],
        [ 1.4677e-02, -2.5903e-03,  2.3797e-03],
        [ 6.7061e-03, -3.4047e-04,  2.3130e-03],
        [ 2.1337e-02, -1.2969e-02, -7.0479e-03],
        [ 1.3942e-02, -7.7183e-03, -1.3945e-02],
        [ 2.3047e-03, -1.1571e-02,  4.6218e-03],
        [ 4.6837e-04, -9.8933e-03, -6.4849e-03],
        [ 5.0255e-03, -6.8896e-03,  3.3173e-03],
        [ 1.3240e-02, -1.5343e-02, -1.6721e-03],
        [ 1.7608e-03, -2.5551e-02,  8.3976e-03],
        [ 9.9200e-04, -1.3004e-02, -3.8049e-03],
        [ 9.9345e-03, -8.6534e-03,  4.9770e-03],
        [ 1.4319e-02, -3.9936e-03, -3.3759e-03],
        [ 4.6277e-03, -6.5886e-03, -3.1894e-04],
        [ 1.1347e-02, -1.4357e-02,  8.6399e-03],
        [ 7.9153e-03,  1.8453e-03, -4.9817e-03],
        [ 7.5074e-03, -5.7770e-03, -1.0843e-03],
        [ 7.1517e-03, -3.9186e-03, -6.1208e-04],
        [ 2.2763e-03, -2.0558e-03, -1.4415e-02],
        [ 8.3957e-03,  3.1242e-03,  1.1403e-02],
        [ 1.0426e-02, -6.8067e-03,  1.1043e-02],
        [ 1.1936e-02,  3.3349e-03, -1.2464e-02],
        [ 1.3542e-02, -6.8623e-04,  6.0276e-04],
        [ 1.1685e-02, -9.6338e-03,  3.4165e-03],
        [ 1.0521e-02,  4.1436e-03, -5.0117e-03],
        [ 8.2909e-03, -5.2429e-03,  4.8124e-03],
        [ 1.0710e-02, -1.5732e-02,  1.3028e-02],
        [ 8.5086e-03, -1.1880e-02, -2.7648e-03],
        [ 7.1352e-03, -1.3155e-03,  4.8228e-03],
        [ 8.7974e-03, -1.7055e-02, -5.3187e-03],
        [ 4.8932e-03, -5.7184e-03,  7.8180e-04],
        [ 7.0633e-03, -8.2581e-03, -5.1327e-03],
        [ 9.5395e-03, -2.5089e-03,  8.0068e-03],
        [ 1.5657e-02, -1.3711e-02,  1.3933e-03],
        [ 1.8810e-02, -1.2064e-02, -1.1374e-02],
        [ 9.3608e-03, -1.3249e-02, -9.5383e-04],
        [ 3.4485e-03, -3.6330e-03,  4.3509e-04],
        [ 5.9063e-03, -5.3775e-03,  9.5683e-05],
        [ 5.2419e-03, -4.2713e-03,  8.7375e-04],
        [-2.5539e-03,  3.3354e-03, -8.6379e-03],
        [ 1.1113e-02, -7.1869e-03, -8.3811e-03],
        [ 9.7765e-03, -1.1897e-02, -8.6019e-04],
        [ 8.2033e-03, -5.1748e-03,  7.9546e-03],
        [ 1.3326e-02, -9.6976e-03, -1.7328e-02],
        [ 1.8160e-02, -1.2892e-04, -4.4945e-03],
        [ 1.3506e-02, -1.0889e-02,  2.8848e-03],
        [ 2.7710e-03, -5.2443e-03,  2.3114e-03],
        [ 1.1636e-02, -1.1566e-02,  2.0462e-03],
        [ 2.3824e-02, -2.0696e-02,  4.3633e-03],
        [ 2.4959e-02,  2.7524e-03,  1.7509e-03],
        [ 9.4436e-03, -1.7303e-03,  4.4851e-03],
        [ 2.1809e-02, -8.3448e-03, -6.2330e-03],
        [ 1.0440e-02, -2.0289e-02,  9.3795e-04],
        [ 6.4066e-03,  7.1956e-03, -8.7802e-04],
        [-1.6524e-03, -1.9070e-02,  3.1881e-03],
        [-1.4650e-03, -1.8044e-02, -6.5592e-03],
        [ 1.1850e-02, -1.1453e-02, -5.3145e-04],
        [ 1.0535e-03, -1.8157e-02, -2.7678e-03],
        [ 2.0136e-03, -9.8947e-03,  1.6984e-04],
        [ 6.4233e-03, -4.0264e-05, -6.2909e-03],
        [ 3.5911e-03, -9.4374e-03, -4.9497e-03],
        [ 7.8684e-03, -4.4290e-03,  4.6715e-03],
        [ 2.2685e-02,  5.5098e-03, -4.0603e-03],
        [ 2.0360e-03,  3.4236e-03,  5.8699e-03],
        [ 6.9658e-03, -5.3020e-03,  5.6224e-03],
        [ 5.7180e-03, -7.9906e-03, -7.4213e-03],
        [ 1.8692e-02, -1.4385e-02,  6.3727e-04],
        [ 2.4131e-02,  8.5280e-04, -1.8211e-03],
        [ 4.2882e-03, -7.6721e-03, -2.9820e-03],
        [ 1.4814e-02, -4.8949e-03, -2.6072e-04],
        [-3.0759e-04, -6.4251e-03, -7.7015e-04],
        [ 3.5501e-04, -3.5144e-03,  6.6248e-03],
        [ 2.6181e-03, -5.8322e-03,  1.7717e-04],
        [ 1.1599e-02, -3.0286e-04,  8.1881e-04],
        [ 1.3825e-02, -1.6360e-02, -2.9436e-04],
        [ 2.1995e-02, -8.0341e-03, -1.1219e-02],
        [-5.2201e-04, -2.9543e-03, -7.7530e-03],
        [-8.1954e-03, -4.4488e-03, -6.3272e-03],
        [ 2.5141e-03, -1.5100e-03,  9.6638e-03],
        [ 8.2909e-03, -5.2429e-03,  4.8124e-03],
        [-3.0621e-03,  1.2082e-03,  2.5952e-03],
        [ 1.0620e-02, -1.0413e-02, -3.3143e-03],
        [ 1.6669e-02, -1.0954e-02, -4.7937e-03],
        [ 1.8833e-02, -1.0045e-02, -7.9479e-03],
        [ 4.3271e-03,  6.9309e-03, -2.7227e-03],
        [ 7.1483e-03, -1.1314e-02, -2.6420e-03],
        [ 1.3448e-02, -2.9179e-03, -5.5957e-04],
        [-8.0925e-03, -1.1504e-02,  8.7572e-03],
        [ 1.2845e-02,  6.2015e-03,  7.0993e-03],
        [ 2.8211e-02, -4.9357e-03, -4.9044e-03],
        [ 1.0420e-02, -4.1489e-03, -4.9835e-03],
        [ 1.5156e-02, -1.1757e-02, -1.0269e-02],
        [ 7.7126e-03,  6.0564e-03, -1.0940e-02],
        [ 1.4234e-02, -1.4735e-02, -2.1831e-04],
        [ 1.0828e-02, -1.0587e-02,  3.5509e-04],
        [ 1.8760e-03,  8.7452e-04, -8.9184e-03],
        [ 7.6754e-03,  2.8451e-03, -9.0926e-03],
        [ 2.7757e-03, -1.0970e-02, -1.1357e-02],
        [ 1.3567e-03, -5.2629e-03,  2.0916e-03],
        [ 1.1136e-02, -3.0093e-05, -8.0035e-04],
        [ 9.5859e-03, -7.5033e-03,  3.7862e-03],
        [ 7.3032e-03, -2.0760e-02, -1.9197e-02],
        [ 1.3959e-02, -6.8886e-03,  3.4083e-03],
        [ 6.9053e-03, -2.8979e-03, -6.4961e-04],
        [ 6.6166e-03, -1.3802e-02,  6.2171e-03],
        [ 5.0968e-04, -6.6657e-03,  8.0449e-03],
        [ 1.4798e-02, -9.2481e-03,  1.1606e-02],
        [ 6.4756e-03, -7.9545e-03, -2.9842e-03],
        [ 2.8726e-02,  1.2666e-03, -5.1508e-03],
        [ 6.3742e-03, -7.6507e-03, -6.0991e-03],
        [ 1.3157e-03, -3.5663e-03,  9.3273e-03],
        [ 9.8976e-03, -3.2050e-03,  1.1217e-02],
        [ 9.2838e-03,  8.7489e-03,  4.5520e-03],
        [ 1.7313e-02, -1.5346e-02,  3.2482e-03],
        [-3.7576e-03, -2.1699e-02, -1.6830e-03],
        [ 2.2772e-02,  1.3033e-03,  2.6939e-04],
        [ 2.4458e-03, -5.6642e-03,  8.1605e-03],
        [ 4.9450e-04,  2.9012e-03, -5.8620e-03],
        [-3.3531e-03, -3.6279e-03, -3.6203e-03],
        [ 4.8830e-03, -1.6381e-03, -4.7485e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000030
penultimate_layer.0.bias: grad mean = 0.000823
output_layer.0.weight: grad mean = 0.001004
output_layer.0.bias: grad mean = 0.029010
[correct] no_actions is False
task_labels:  tensor([2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 0, 1, 1, 1, 2, 1, 0, 2,
        2, 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 0, 1, 2, 0, 2, 2, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 2, 0, 1, 1, 2, 1, 0, 1, 0, 2, 0, 0, 0, 1, 2, 1, 0, 0, 0, 1,
        1, 1, 0, 2, 2, 0, 1, 0, 2, 2, 2, 0, 0, 1, 1, 1, 0, 2, 2, 1, 0, 2, 1, 1,
        2, 0, 1, 0, 2, 1, 1, 0, 2, 1, 0, 0, 2, 0, 2, 1, 1, 0, 1, 2, 0, 0, 1, 2,
        2, 1, 1, 1, 2, 2, 2, 0]) , task_preds: tensor([[-2.1205e-04, -4.6379e-03, -3.0118e-03],
        [ 1.7293e-02, -4.7304e-03, -1.6494e-03],
        [ 8.3006e-03, -5.2067e-03,  4.7670e-03],
        [-1.3534e-03, -1.6840e-03,  9.0384e-03],
        [-1.1378e-02, -1.8623e-02, -1.2306e-03],
        [ 1.3017e-02, -5.9877e-03, -1.4603e-02],
        [ 1.8777e-02, -1.3946e-04,  1.1428e-02],
        [ 1.8688e-02, -1.4374e-02,  6.2800e-04],
        [ 2.0821e-02, -2.3638e-03,  4.8134e-03],
        [ 5.3007e-03, -1.9031e-04, -1.1780e-03],
        [ 5.7542e-03, -2.8908e-03,  1.3249e-03],
        [ 6.0517e-03,  1.1373e-03, -5.1422e-03],
        [ 8.0510e-03, -6.9511e-03, -6.7644e-03],
        [ 1.4216e-02,  5.4436e-03,  8.6377e-05],
        [ 6.3777e-03,  7.2077e-03, -8.6836e-04],
        [ 1.1572e-02, -1.0537e-02, -1.9997e-03],
        [ 1.7242e-02, -2.9197e-03, -5.0890e-03],
        [ 1.7689e-02, -2.1915e-03,  8.6959e-04],
        [ 3.5765e-03, -7.3832e-03,  4.5293e-03],
        [ 7.8092e-03, -2.5388e-03, -6.1200e-03],
        [ 1.1494e-02, -1.3323e-02, -5.1887e-03],
        [ 7.4628e-03, -5.7782e-03, -1.0484e-03],
        [ 9.4575e-06, -9.9649e-03, -8.2008e-03],
        [ 2.2686e-02, -3.4565e-03,  2.1140e-03],
        [ 7.3331e-03, -6.1951e-03,  2.0879e-03],
        [ 8.3006e-03, -5.2067e-03,  4.7670e-03],
        [ 4.6362e-03,  1.0400e-02, -1.3358e-02],
        [-7.5677e-04, -8.2932e-03,  4.7652e-03],
        [ 6.4028e-03, -4.4431e-04,  6.5410e-04],
        [ 4.4153e-03, -6.4086e-03,  6.6767e-03],
        [ 3.9054e-03,  2.4937e-03, -2.7363e-03],
        [ 1.2825e-02, -8.5421e-03, -1.6012e-03],
        [ 1.0688e-02, -1.8517e-03,  3.3891e-03],
        [ 1.0071e-02, -1.6710e-02, -5.6132e-03],
        [-3.7968e-03, -2.1718e-02, -1.6332e-03],
        [ 1.0059e-02, -2.0827e-03,  3.9116e-04],
        [ 1.1301e-02, -3.8236e-03,  5.7760e-03],
        [-3.3325e-03, -1.0520e-02,  3.2129e-04],
        [ 1.2029e-02, -1.5873e-02, -7.3951e-03],
        [-1.7045e-03, -9.4311e-03,  5.5728e-03],
        [ 9.4782e-04, -1.2994e-02, -3.7819e-03],
        [-1.7796e-03, -2.5515e-03, -5.2755e-03],
        [ 2.1410e-02, -3.7006e-03, -6.0778e-04],
        [ 9.9740e-03, -5.7896e-03, -6.7539e-03],
        [ 6.0783e-03, -8.4340e-03, -1.2137e-02],
        [ 1.0383e-03, -3.6353e-03, -3.3923e-03],
        [-4.3313e-04,  1.9870e-03,  1.0902e-03],
        [ 3.5174e-03, -2.5480e-03, -5.5900e-03],
        [ 1.1048e-02, -7.1652e-03,  7.2850e-03],
        [ 8.4512e-03, -1.1787e-02,  4.2325e-03],
        [ 7.1952e-03, -1.1470e-02, -3.9311e-03],
        [ 7.8124e-03, -9.2655e-03,  2.7619e-04],
        [ 2.1946e-02, -8.0280e-03, -4.6191e-03],
        [ 9.9642e-03, -1.5515e-02, -1.4218e-03],
        [ 1.6719e-02, -1.9423e-03, -1.2791e-03],
        [ 1.4961e-02, -3.9241e-03, -2.0970e-03],
        [ 3.4177e-03, -2.6774e-02, -2.8612e-03],
        [ 6.9026e-03, -2.9198e-03, -6.2536e-04],
        [-1.7916e-03, -1.6080e-02, -4.5608e-03],
        [ 8.3006e-03, -5.2067e-03,  4.7670e-03],
        [ 1.1038e-02, -3.6675e-03,  7.3740e-06],
        [-5.0559e-03, -1.5363e-02, -5.7392e-03],
        [ 8.3006e-03, -5.2067e-03,  4.7670e-03],
        [ 6.7618e-03, -7.0600e-03,  1.1820e-04],
        [ 1.5956e-02, -1.2751e-02, -5.9467e-03],
        [ 1.3856e-02, -1.0082e-02, -1.5095e-03],
        [-4.7445e-03, -2.5421e-03,  6.1411e-03],
        [ 5.7881e-03, -1.8510e-03, -1.4945e-03],
        [ 1.8840e-02, -1.2099e-02, -1.1361e-02],
        [ 1.8149e-02, -9.1804e-03, -6.0137e-03],
        [ 1.6996e-02, -1.2068e-02,  2.7617e-03],
        [ 3.6958e-03, -5.3289e-03,  2.4836e-03],
        [ 4.7121e-03, -4.1235e-03,  4.7351e-03],
        [ 8.0696e-03, -5.2533e-03, -8.7489e-04],
        [ 4.1213e-03, -1.0097e-02,  7.4149e-04],
        [ 9.4880e-03, -8.2161e-03,  1.5502e-03],
        [ 1.1990e-02, -3.8546e-03,  3.9672e-03],
        [ 3.3390e-03,  8.8051e-04, -8.4700e-03],
        [ 1.4184e-02, -8.4240e-03, -2.5481e-03],
        [ 1.3358e-02, -7.6995e-03,  5.3350e-03],
        [ 8.0914e-04, -1.2094e-02,  1.7778e-03],
        [ 1.4882e-02, -5.8949e-03, -4.0673e-03],
        [ 1.4970e-03, -7.9788e-03,  2.4781e-03],
        [ 2.2057e-02,  1.8309e-03,  1.0641e-02],
        [ 1.4059e-02, -4.8026e-03,  1.6928e-03],
        [-1.9253e-04, -7.2100e-03, -4.7079e-03],
        [ 1.3822e-02,  1.3572e-03, -7.4833e-04],
        [ 1.1344e-02,  3.4227e-03, -8.0466e-06],
        [ 8.8170e-03, -1.7076e-02, -5.3124e-03],
        [ 4.8890e-03, -1.1053e-02, -7.1905e-03],
        [ 2.7867e-02, -3.7419e-04, -7.3301e-03],
        [ 5.8600e-03,  8.1257e-04,  5.6839e-03],
        [ 9.0648e-03, -5.0272e-03, -4.3282e-03],
        [ 9.6084e-03, -7.0054e-03,  2.1197e-03],
        [-1.0944e-02, -1.0707e-02,  1.0228e-02],
        [ 2.7057e-03, -9.2018e-03,  6.2632e-03],
        [ 9.2824e-03, -1.3273e-02, -8.6788e-04],
        [ 5.1935e-03,  1.1650e-04,  7.4961e-03],
        [ 9.5780e-03, -7.9256e-03,  3.6403e-03],
        [ 1.2297e-02, -8.9678e-03,  1.0870e-02],
        [ 1.4403e-02, -9.6146e-04, -9.9858e-03],
        [ 1.0501e-02, -7.8396e-03,  8.5213e-03],
        [ 1.2618e-02,  1.5762e-04, -5.8912e-03],
        [ 3.3863e-04, -5.0855e-03,  3.5341e-03],
        [ 6.3272e-03, -1.1552e-02, -7.7688e-03],
        [-1.1579e-03, -1.4700e-03, -6.1430e-03],
        [ 1.5170e-02, -5.0680e-03,  8.7299e-03],
        [ 1.5249e-02, -5.5215e-04, -3.1668e-03],
        [ 6.6684e-03, -2.9539e-03,  5.4619e-03],
        [ 1.8224e-03,  8.3732e-04, -2.6660e-03],
        [-2.7491e-03, -5.2674e-03,  3.0354e-03],
        [ 1.4903e-02, -7.1290e-03,  1.4215e-03],
        [-8.2409e-03, -4.4750e-03, -6.2645e-03],
        [ 1.8310e-02, -6.2541e-03, -3.5266e-03],
        [ 1.2472e-02, -5.7174e-03,  1.1696e-03],
        [ 1.1265e-02, -1.0741e-02,  8.9931e-03],
        [ 4.5671e-03, -1.0694e-02, -5.7008e-03],
        [ 6.4171e-03, -2.9272e-04, -4.4878e-03],
        [ 6.1815e-03, -6.4161e-03, -1.8668e-03],
        [ 1.1812e-02, -1.1496e-02, -4.5739e-04],
        [-1.8409e-03, -7.9253e-03, -5.6652e-03],
        [ 5.3578e-03,  5.4759e-03, -1.1859e-02],
        [ 6.0674e-03, -5.3085e-03,  2.7570e-03],
        [ 9.4001e-03, -1.7189e-03,  4.5069e-03],
        [ 1.2250e-02, -1.4567e-02, -5.1046e-03],
        [-4.2609e-03,  1.0752e-03,  7.4223e-03],
        [ 6.2350e-03,  1.2754e-03, -2.5411e-03],
        [ 2.1334e-02, -1.2940e-02, -2.6304e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000033
penultimate_layer.0.bias: grad mean = 0.000721
output_layer.0.weight: grad mean = 0.001007
output_layer.0.bias: grad mean = 0.021962
[correct] no_actions is False
task_labels:  tensor([1, 0, 1, 2, 0, 1, 1, 0, 2, 2, 0, 0, 1, 1, 2, 2, 0, 2, 2, 1, 1, 1, 0, 1,
        1, 0, 1, 2, 1, 1, 2, 1, 2, 1, 1, 0, 0, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 0,
        1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 0, 1, 2, 2, 2, 0, 2, 1, 1, 0, 2, 0, 2, 0,
        2, 1, 2, 0, 1, 0, 1, 2, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 2, 1, 0, 1, 0, 2,
        1, 2, 1, 0, 0, 2, 1, 2, 1, 0, 2, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0,
        1, 2, 1, 1, 0, 0, 1, 0]) , task_preds: tensor([[ 1.7699e-02, -4.3178e-03, -2.8630e-03],
        [ 1.2856e-02, -5.3930e-03, -3.6554e-03],
        [ 1.0328e-02, -5.2566e-03, -1.0303e-02],
        [ 1.6871e-02, -1.2805e-02, -5.4845e-03],
        [ 1.0601e-02, -7.1262e-03,  1.4729e-03],
        [ 1.7855e-02, -1.1312e-02, -5.2893e-03],
        [ 1.1114e-02, -7.2017e-03, -8.3667e-03],
        [ 9.8264e-03, -1.2979e-02,  9.5873e-03],
        [ 3.4015e-03,  2.6982e-03, -8.6960e-03],
        [ 1.1145e-02, -5.8287e-04,  4.4059e-03],
        [ 2.3036e-02,  1.0269e-03, -7.0762e-03],
        [ 1.3237e-02, -4.1148e-03,  1.9102e-03],
        [ 8.7781e-03, -4.5456e-03,  3.9581e-03],
        [ 2.7997e-03,  5.4387e-03, -1.6905e-03],
        [ 8.0518e-03, -1.1254e-02,  2.5062e-03],
        [ 1.1031e-02, -9.4177e-03, -1.0195e-03],
        [ 2.3039e-02,  1.7270e-03,  7.1922e-03],
        [ 3.6026e-03,  4.2057e-03, -4.2833e-03],
        [ 1.3810e-02,  9.4439e-05, -6.9578e-03],
        [ 1.4820e-02,  6.4923e-03, -1.1622e-02],
        [ 7.0194e-05, -3.9128e-03,  1.0938e-03],
        [ 5.1001e-03, -5.1595e-03,  3.6167e-03],
        [ 1.5950e-02, -1.5027e-02,  2.9476e-03],
        [ 3.5040e-03,  5.6975e-03,  8.3258e-03],
        [ 6.3633e-03, -7.3361e-03, -7.0351e-04],
        [ 7.0749e-03, -1.8126e-02,  2.4187e-03],
        [ 6.3338e-03, -5.7429e-04, -2.5164e-03],
        [ 4.8180e-03, -5.2346e-03,  1.5445e-03],
        [ 1.0768e-02, -3.0307e-03,  6.1578e-03],
        [ 5.0834e-03, -5.2304e-03, -1.6252e-03],
        [ 5.8237e-03, -5.4644e-03,  2.5078e-04],
        [ 6.7239e-03,  4.0361e-03, -6.3222e-03],
        [-5.2370e-03, -7.0323e-03,  7.2694e-03],
        [ 3.2194e-03, -4.9961e-03, -7.1121e-03],
        [ 3.6427e-03,  4.6055e-05,  7.9930e-03],
        [ 1.3293e-02, -8.6910e-05,  2.3435e-03],
        [ 5.7639e-03, -6.7565e-03,  4.6505e-03],
        [ 6.5334e-03, -1.3846e-02,  6.3286e-03],
        [ 2.5165e-03, -6.8132e-03, -7.2938e-04],
        [ 1.2773e-02,  6.2085e-03,  7.1483e-03],
        [ 8.7011e-03, -3.5197e-03, -3.8095e-04],
        [ 1.2323e-02,  1.1777e-03,  3.7009e-04],
        [ 4.0002e-03,  1.0590e-03, -4.6305e-03],
        [ 7.8069e-03, -8.8205e-03,  3.2591e-03],
        [ 1.0432e-02, -1.0604e-02, -1.6542e-03],
        [ 1.3063e-02, -1.3445e-02, -1.1703e-02],
        [-1.1101e-03, -1.3790e-02, -3.1619e-03],
        [ 1.5140e-02, -1.6351e-02,  7.2025e-03],
        [ 8.7636e-03,  3.3626e-03,  1.0357e-02],
        [ 6.9170e-03,  1.1088e-03, -4.9670e-03],
        [ 9.7721e-03, -6.8483e-03, -7.2715e-03],
        [ 2.0031e-02, -2.6631e-03,  2.4744e-03],
        [-3.3212e-03, -1.2228e-02, -1.0421e-02],
        [ 2.9070e-02, -1.5896e-02, -5.4103e-03],
        [ 1.2270e-02, -6.6466e-03,  6.5900e-04],
        [-8.0519e-03, -9.6053e-03, -4.2745e-03],
        [ 1.2595e-02, -4.2940e-03, -2.5579e-03],
        [ 5.1398e-03, -6.5654e-03, -1.0688e-06],
        [ 1.2220e-02, -1.6324e-03, -1.5281e-03],
        [ 1.8594e-02, -4.7876e-03, -1.0469e-02],
        [ 1.1402e-02, -8.1563e-03, -2.4947e-03],
        [ 1.3912e-02, -7.7828e-03, -1.3855e-02],
        [-4.1249e-03, -2.0698e-02, -2.2626e-03],
        [ 1.7937e-02, -2.4309e-03, -8.5122e-03],
        [ 1.7044e-02, -1.0076e-02, -5.6259e-03],
        [ 7.7859e-03, -1.0493e-02, -1.2451e-04],
        [ 9.5594e-03,  5.5099e-03, -2.1275e-03],
        [ 7.0588e-03, -1.8612e-02,  2.8482e-03],
        [ 1.0431e-02, -7.8203e-03,  1.3911e-03],
        [ 1.0804e-02, -3.7311e-03,  6.8825e-03],
        [ 3.0851e-04, -1.1049e-02, -2.7084e-03],
        [ 1.1791e-02, -3.1298e-03,  2.8280e-04],
        [ 6.5820e-03, -9.8750e-03, -3.3363e-03],
        [-2.6932e-03,  3.3868e-03, -8.5831e-03],
        [ 9.2927e-03, -1.0441e-02,  8.0741e-03],
        [ 1.1930e-02, -1.4538e-02, -4.8708e-03],
        [-1.3750e-02, -1.6054e-02,  6.1436e-03],
        [ 1.8191e-03, -1.0323e-02,  5.4550e-04],
        [ 3.7678e-03, -1.6224e-02, -5.8846e-03],
        [ 1.0856e-02, -2.8681e-03,  4.1389e-03],
        [ 8.2583e-03, -5.4468e-03, -3.1756e-03],
        [ 2.1438e-02, -2.3202e-03, -2.1363e-03],
        [ 2.4563e-03,  5.2492e-05,  6.5484e-03],
        [ 1.1270e-02, -1.4411e-02,  8.7561e-03],
        [ 1.4701e-02, -2.6402e-03,  2.4117e-03],
        [ 1.9351e-02, -5.6082e-03, -1.6240e-03],
        [ 6.2913e-03, -1.7823e-03,  7.3459e-04],
        [ 7.6401e-03, -1.3098e-02, -6.8349e-03],
        [ 2.8025e-03, -1.1016e-02, -1.1331e-02],
        [ 4.8015e-03, -4.0703e-03,  9.9747e-03],
        [ 3.4982e-03,  5.0832e-03, -1.2425e-02],
        [ 2.1256e-02,  6.3988e-03, -2.0479e-03],
        [ 9.0099e-03,  4.7891e-03, -1.5584e-02],
        [ 6.8261e-03, -2.2343e-03, -3.7095e-03],
        [ 1.7460e-02, -8.0294e-04, -4.9144e-03],
        [-1.9443e-02, -1.1377e-02,  1.1902e-02],
        [ 1.6532e-02, -7.0729e-03, -1.2938e-02],
        [ 5.2235e-03, -1.8290e-03, -7.7785e-03],
        [ 7.2413e-03, -1.2814e-02, -1.0818e-02],
        [ 3.0860e-03, -7.5237e-03, -2.0045e-03],
        [ 8.0228e-03, -2.1913e-02,  2.5595e-03],
        [ 9.8023e-03, -5.9672e-04,  2.0678e-03],
        [-3.1091e-03,  1.1794e-03,  2.6617e-03],
        [ 3.0974e-03,  4.9397e-03,  6.8033e-03],
        [ 1.3291e-02, -4.2885e-03,  6.6721e-03],
        [ 1.6298e-02,  3.2130e-03, -7.5281e-03],
        [ 1.7155e-02,  2.2403e-03, -7.4134e-04],
        [ 9.8673e-03, -4.3694e-03, -5.9832e-03],
        [-7.0913e-03, -1.2093e-02,  1.0293e-02],
        [ 1.4566e-02,  4.5457e-03, -1.1161e-02],
        [ 4.6622e-03, -6.6468e-03, -2.8561e-04],
        [ 1.4778e-02, -6.6507e-03, -1.3828e-02],
        [ 1.1077e-02, -7.6207e-05, -7.0614e-04],
        [ 1.9030e-02, -4.9344e-03, -2.3717e-03],
        [-2.7533e-04, -1.0625e-02,  2.0331e-03],
        [ 1.1052e-02, -6.5091e-05, -8.5602e-04],
        [ 6.7651e-03, -8.2250e-03,  7.3790e-04],
        [ 5.6019e-03, -1.9346e-02, -1.9473e-03],
        [ 8.2350e-04, -7.8651e-03, -6.1454e-03],
        [ 3.9247e-03, -1.1993e-02, -3.6325e-03],
        [ 1.0178e-02, -2.4988e-03,  1.1208e-02],
        [ 9.8060e-03, -6.9574e-03,  2.5538e-03],
        [-2.6695e-03, -4.2640e-03,  2.4715e-03],
        [ 9.6691e-03, -1.1184e-02, -8.0701e-03],
        [ 1.1973e-02, -1.5375e-02,  3.2113e-03],
        [ 1.1807e-02, -4.4124e-03,  4.2649e-03],
        [ 1.3013e-02, -5.3448e-03, -6.3076e-04],
        [ 2.5713e-02,  1.2587e-03, -9.0133e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000029
penultimate_layer.0.bias: grad mean = 0.000357
output_layer.0.weight: grad mean = 0.000834
output_layer.0.bias: grad mean = 0.012604
[correct] no_actions is False
task_labels:  tensor([0, 2, 0, 0, 0, 1, 2, 1, 0, 0, 2, 0, 0, 2, 2, 1, 1, 1, 1, 1, 0, 2, 0, 1,
        0, 2, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 2, 2, 2, 1, 2, 1, 1, 2, 0, 0, 2, 2,
        1, 1, 1, 2, 2, 2, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 2, 0, 2, 0,
        1, 1, 2, 0, 1, 0, 2, 1, 1, 0, 1, 2, 2, 0, 2, 1, 0, 1, 2, 1, 2, 1, 1, 1,
        1, 1, 1, 2, 1, 0, 2, 0, 1, 1, 2, 2, 0, 2, 0, 0, 1, 1, 0, 2, 1, 2, 2, 1,
        2, 2, 0, 2, 0, 0, 1, 2]) , task_preds: tensor([[ 2.4567e-02, -1.2539e-02, -1.5075e-03],
        [ 7.1779e-03, -7.6736e-03, -5.0102e-04],
        [ 1.5227e-02, -1.0513e-02,  5.9548e-03],
        [ 4.4196e-03, -6.4898e-03,  6.7582e-03],
        [ 5.8155e-03, -6.7790e-03,  4.6337e-03],
        [ 1.6536e-02, -7.0784e-03, -1.2936e-02],
        [-9.4993e-03, -1.3827e-02, -3.9362e-03],
        [ 7.4363e-03, -5.7960e-03, -1.0093e-03],
        [ 4.3467e-03, -1.9764e-02,  7.9242e-03],
        [ 1.1090e-02, -1.6478e-03, -9.5167e-03],
        [ 8.4296e-04, -1.2086e-02,  1.7430e-03],
        [ 1.3225e-02, -4.1219e-03, -2.8394e-03],
        [ 8.4728e-03,  5.1710e-03, -6.2457e-04],
        [ 2.2690e-02, -3.5391e-03,  2.1973e-03],
        [ 6.4733e-03, -7.3822e-03, -6.9067e-04],
        [-2.6657e-03, -4.2997e-03,  2.5058e-03],
        [ 1.3910e-02,  1.3852e-03, -8.4667e-04],
        [ 5.9409e-03, -6.2523e-03,  5.8007e-03],
        [ 9.7173e-03, -1.5847e-02, -3.0406e-03],
        [-8.2718e-03, -4.5283e-03, -6.1849e-03],
        [ 1.1085e-02, -9.8965e-05, -6.8838e-04],
        [ 9.7979e-03, -6.9845e-03,  2.5883e-03],
        [ 7.0820e-03, -1.8157e-02,  2.4454e-03],
        [ 5.8140e-03,  8.2279e-04,  5.7086e-03],
        [ 1.0582e-03, -3.6537e-03, -3.3887e-03],
        [ 2.0529e-03, -5.0467e-03, -9.3623e-03],
        [-5.4884e-03, -1.0105e-02, -9.8614e-04],
        [ 2.1622e-02, -1.4576e-02, -6.2309e-04],
        [ 9.9649e-03, -5.8230e-03, -6.7123e-03],
        [ 9.8186e-03, -1.7756e-03,  2.4483e-03],
        [ 8.9903e-03, -4.4389e-03, -1.1176e-04],
        [ 1.7131e-02, -2.5286e-02,  8.7475e-04],
        [ 1.8908e-02, -1.2165e-02, -1.1345e-02],
        [ 1.4883e-02,  4.8536e-03,  4.1869e-04],
        [ 7.9403e-03, -6.4081e-03, -3.0429e-03],
        [ 1.0760e-02, -1.5778e-02,  1.3036e-02],
        [ 2.6426e-03, -5.9263e-03,  2.5585e-04],
        [ 2.9623e-03, -5.6551e-04,  6.6305e-03],
        [ 1.3925e-02, -7.8110e-03, -1.3835e-02],
        [ 1.2765e-02,  6.2043e-03,  7.1586e-03],
        [ 8.0531e-03, -1.1288e-02,  2.5400e-03],
        [ 1.5896e-02, -7.8807e-03,  1.3040e-03],
        [-1.0995e-02, -1.0713e-02,  1.0273e-02],
        [ 3.6786e-03,  5.4878e-03,  2.4166e-03],
        [ 4.0620e-03, -8.0333e-03,  5.0111e-03],
        [ 6.9472e-03, -2.1629e-03,  3.7854e-03],
        [ 6.9066e-03, -7.6076e-03,  1.2599e-02],
        [-3.2987e-03, -1.2252e-02, -1.0414e-02],
        [ 1.5525e-03, -8.5096e-03, -9.9258e-03],
        [ 7.2196e-03, -3.8315e-03, -7.5641e-04],
        [ 8.1292e-03, -5.2071e-03, -9.6991e-04],
        [ 1.4789e-02, -5.5410e-03,  3.0882e-03],
        [ 1.0216e-02, -9.2584e-03,  7.0850e-04],
        [ 1.1940e-02, -1.2468e-02, -2.2606e-03],
        [ 1.8706e-02, -1.4383e-02,  6.2221e-04],
        [ 1.2958e-02, -1.3374e-02, -1.4413e-02],
        [ 4.6146e-04, -1.1138e-02,  6.2356e-04],
        [-3.5257e-03,  9.4048e-04,  4.5020e-03],
        [ 8.3723e-03, -5.1687e-03,  4.6711e-03],
        [ 5.2391e-03, -4.3052e-03,  9.1025e-04],
        [ 4.6740e-03, -2.2283e-03,  1.0079e-02],
        [ 1.1785e-02, -2.2722e-03,  1.4180e-04],
        [ 1.0920e-02, -4.5961e-03, -4.5491e-03],
        [ 1.1030e-02, -7.2363e-03,  7.3736e-03],
        [ 6.0053e-03,  2.4017e-03, -1.0403e-02],
        [-1.6729e-03,  3.9946e-03,  3.7060e-03],
        [ 2.8335e-02, -5.0484e-03, -4.8845e-03],
        [ 2.8846e-04, -1.2615e-02, -7.5493e-03],
        [ 1.9037e-02, -4.9706e-03, -2.3398e-03],
        [ 2.1212e-02, -4.9462e-03,  5.3945e-04],
        [ 3.4831e-03,  5.0650e-03, -1.2394e-02],
        [ 4.9572e-03, -3.6372e-03,  2.6348e-04],
        [-7.2703e-03, -1.1614e-02, -4.3126e-03],
        [ 1.6625e-02, -1.1043e-02, -4.6671e-03],
        [ 8.3723e-03, -5.1687e-03,  4.6711e-03],
        [ 6.2790e-03, -1.1144e-02, -4.8193e-03],
        [ 1.3786e-03, -2.0057e-03, -1.3840e-02],
        [ 8.5009e-03, -1.1842e-02,  4.2497e-03],
        [ 9.0091e-03, -7.9023e-03,  2.0361e-03],
        [ 6.3329e-03, -5.8373e-04, -2.5061e-03],
        [ 9.5629e-03,  5.5060e-03, -2.1263e-03],
        [ 1.2936e-02, -1.2218e-02, -6.8650e-03],
        [ 6.5327e-03, -1.3868e-02,  6.3523e-03],
        [-4.7919e-05,  7.0506e-03,  5.2128e-03],
        [ 8.6834e-03,  4.0436e-04,  6.2424e-03],
        [ 3.2563e-04,  5.5588e-03,  5.9049e-03],
        [ 7.2970e-03, -6.2422e-03,  2.1651e-03],
        [ 4.7617e-03, -3.0815e-04,  8.5737e-03],
        [-5.5302e-04, -6.4138e-03,  5.7166e-03],
        [ 5.3147e-03,  5.4954e-03, -1.1846e-02],
        [ 1.2238e-02,  9.6805e-03,  4.6010e-03],
        [-4.2265e-04, -4.7529e-03,  1.5970e-03],
        [ 1.3530e-02,  1.4542e-03,  9.6662e-04],
        [ 1.2103e-02, -1.1160e-02,  1.0461e-03],
        [ 8.4059e-03, -7.4922e-03, -5.9166e-04],
        [ 6.3653e-03,  7.2058e-03, -8.5693e-04],
        [ 3.0298e-03, -6.9682e-03,  1.2722e-02],
        [ 1.4525e-02,  1.1497e-02, -2.2241e-03],
        [ 9.2986e-03, -5.0672e-03,  1.8212e-03],
        [ 3.3996e-03, -3.7270e-03,  5.7115e-04],
        [ 1.1185e-02, -1.8622e-03,  1.9119e-03],
        [ 6.4214e-03, -7.2218e-03, -5.0488e-03],
        [ 5.1342e-03, -6.5889e-03,  2.7822e-05],
        [ 5.8627e-03, -7.9597e-03, -6.7449e-03],
        [ 3.6503e-03, -2.3277e-05, -8.1936e-03],
        [ 1.7356e-02, -1.2301e-02, -3.3350e-03],
        [-4.2949e-03,  9.9945e-04,  7.5279e-03],
        [ 2.2787e-03, -1.0945e-02, -1.1349e-02],
        [ 1.0214e-02, -1.8600e-02,  1.1941e-03],
        [ 9.8053e-03, -1.5405e-02,  7.1413e-05],
        [-2.0675e-03, -3.0914e-03, -5.7225e-03],
        [ 2.4208e-02,  7.5974e-04, -1.7837e-03],
        [-7.6580e-03, -9.7544e-03, -3.6312e-03],
        [ 2.1333e-02, -1.2680e-02, -1.2279e-02],
        [ 1.5840e-03, -8.3266e-04,  1.0439e-03],
        [-5.2373e-03, -7.0632e-03,  7.3019e-03],
        [ 1.4459e-02, -9.6035e-03,  3.4903e-03],
        [ 7.4797e-03,  1.2559e-04, -8.9500e-03],
        [ 8.3723e-03, -5.1687e-03,  4.6711e-03],
        [-7.7211e-03, -1.0882e-02, -1.1510e-02],
        [ 8.0637e-03,  4.3028e-03, -3.7858e-03],
        [ 1.3669e-02, -5.2156e-03, -2.1534e-03],
        [ 8.3723e-03, -5.1687e-03,  4.6711e-03],
        [ 1.3303e-02, -9.7416e-03, -1.7263e-02],
        [ 1.9191e-02, -8.5679e-03,  4.0075e-03],
        [ 1.0541e-03, -4.2626e-03,  4.5213e-03],
        [ 1.5258e-02,  3.9479e-03, -9.6174e-04],
        [ 4.0004e-03, -1.8950e-03,  4.0623e-04]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000029
penultimate_layer.0.bias: grad mean = 0.000582
output_layer.0.weight: grad mean = 0.000886
output_layer.0.bias: grad mean = 0.017531
[correct] no_actions is False
task_labels:  tensor([2, 0, 2, 0, 1, 1, 1, 2, 0, 2, 0, 1, 1, 0, 0, 1, 2, 1, 1, 1, 2, 2, 1, 0,
        2, 2, 1, 0, 2, 0, 1, 2, 1, 0, 1, 0, 1, 1, 1, 0, 0, 2, 1, 2, 0, 0, 2, 0,
        1, 1, 2, 1, 2, 0, 0, 2, 0, 2, 1, 0, 2, 1, 2, 1, 1, 2, 2, 0, 0, 0, 0, 2,
        1, 1, 1, 2, 2, 1, 1, 0, 1, 1, 0, 1, 1, 2, 1, 1, 2, 0, 1, 2, 1, 1, 2, 1,
        0, 0, 0, 0, 1, 1, 1, 2, 0, 2, 1, 1, 2, 0, 1, 0, 1, 0, 2, 1, 2, 2, 1, 0,
        1, 2, 0, 0, 1, 1, 2, 1]) , task_preds: tensor([[ 1.9186e-02, -9.1791e-04,  5.2163e-03],
        [ 7.5679e-04, -1.1297e-02, -6.2345e-03],
        [ 1.3298e-02, -8.2419e-03, -3.6797e-03],
        [ 9.3036e-03,  4.5382e-04, -1.8854e-03],
        [-3.1371e-03, -2.4004e-03,  3.7286e-03],
        [ 1.4066e-02, -1.8797e-03,  9.8811e-03],
        [-1.1006e-03, -1.3820e-02, -3.1381e-03],
        [-1.2258e-02, -3.4896e-03, -3.1674e-03],
        [ 2.0471e-03, -1.0050e-02,  3.0649e-04],
        [ 1.0433e-02, -1.0668e-02, -1.5872e-03],
        [ 1.5324e-02, -2.9340e-03, -1.1777e-02],
        [ 3.7814e-03,  4.7514e-03,  4.3197e-03],
        [-2.4706e-03,  2.0577e-03, -9.6578e-03],
        [ 2.2807e-02, -7.2315e-03, -1.0810e-02],
        [-6.3842e-03, -6.6418e-03, -7.9731e-03],
        [ 7.6472e-03, -2.0821e-03, -9.0755e-03],
        [ 6.3747e-03, -5.1589e-04,  7.5106e-04],
        [ 1.5286e-02, -4.3779e-03, -6.1280e-03],
        [-5.9264e-03, -5.2012e-04,  2.6060e-03],
        [ 1.4264e-02, -1.3320e-02,  1.8366e-03],
        [ 2.1488e-03, -8.2286e-03,  7.6663e-03],
        [-6.6442e-04,  6.2420e-03,  1.0115e-02],
        [ 5.0994e-03, -5.2770e-03, -1.5890e-03],
        [ 3.9352e-03, -9.3235e-03, -1.6064e-03],
        [ 3.5608e-03, -8.0258e-03,  1.2155e-03],
        [-1.1265e-03, -2.3472e-02, -1.3548e-03],
        [ 5.3873e-03,  9.3937e-04,  7.4673e-03],
        [ 8.4071e-03, -9.3773e-03, -8.5262e-04],
        [ 1.4656e-03, -8.0195e-03,  2.5450e-03],
        [ 5.6001e-03, -1.8302e-03, -1.2291e-02],
        [ 7.2145e-03, -1.2830e-02, -1.0781e-02],
        [ 8.3921e-03, -5.1546e-03,  4.6409e-03],
        [-1.6159e-03,  2.4858e-03,  3.5226e-04],
        [ 8.3921e-03, -5.1546e-03,  4.6409e-03],
        [ 1.3094e-02, -1.3069e-02, -8.0809e-04],
        [ 9.7248e-03,  5.7938e-04,  1.8455e-03],
        [ 1.8728e-02, -2.4370e-03,  1.7780e-03],
        [ 4.6857e-03, -4.1722e-03,  4.8059e-03],
        [-3.0327e-03, -7.7239e-03, -1.3782e-04],
        [ 4.6606e-03,  8.1880e-03,  9.2624e-03],
        [ 1.1232e-02,  3.2009e-03,  8.3527e-03],
        [ 2.1955e-02, -8.0954e-03, -4.5557e-03],
        [ 1.0626e-02, -5.9527e-03,  1.5404e-03],
        [ 9.9076e-03, -3.5378e-03,  4.0168e-03],
        [ 2.3458e-02, -9.5732e-03,  1.6942e-03],
        [ 1.3754e-02, -3.5171e-03, -1.6486e-03],
        [ 1.6306e-02, -9.8100e-03,  9.2280e-03],
        [ 7.5854e-03, -8.2484e-03,  5.7290e-03],
        [-1.1637e-03, -1.4855e-03, -6.1226e-03],
        [ 1.1345e-02, -1.2151e-02,  4.4286e-03],
        [ 1.1892e-02, -1.6387e-02,  2.5003e-03],
        [ 1.4636e-02, -7.5831e-04,  3.8193e-03],
        [ 2.3274e-02, -1.0066e-02,  2.4620e-04],
        [ 1.1155e-03, -1.8255e-02, -2.7143e-03],
        [ 1.2485e-02, -8.8453e-03,  4.4480e-03],
        [-9.1808e-04, -2.8493e-03,  9.2568e-03],
        [ 2.2755e-02, -9.7685e-03, -1.8554e-02],
        [-2.1033e-04, -4.6999e-03, -2.9482e-03],
        [ 6.3768e-04, -3.9654e-03,  1.0615e-04],
        [-8.0022e-03, -9.6466e-03, -4.2702e-03],
        [-3.3484e-03, -1.2533e-02, -1.0307e-02],
        [-1.1110e-03, -1.4453e-03, -8.2153e-03],
        [ 5.8113e-03, -5.5237e-03,  3.2239e-04],
        [ 1.9976e-02, -1.8952e-03,  4.5987e-03],
        [-2.3545e-04, -7.2976e-03, -4.5826e-03],
        [-3.8283e-03, -2.1786e-02, -1.5382e-03],
        [ 7.0575e-03, -1.4329e-03,  5.0057e-03],
        [ 1.5611e-02, -7.3531e-03, -3.7861e-03],
        [ 1.4747e-02, -2.6744e-03,  2.4114e-03],
        [ 2.1989e-02, -8.3513e-03, -6.3689e-03],
        [ 4.7097e-03, -6.6874e-03, -2.8051e-04],
        [ 2.2247e-02, -4.1542e-04, -1.1434e-02],
        [ 8.6139e-03, -1.1509e-02, -1.5759e-03],
        [ 6.8100e-03,  5.0137e-03, -1.3818e-02],
        [ 6.8215e-03, -8.0664e-03,  6.3530e-03],
        [ 3.0868e-03,  4.9219e-03,  6.8301e-03],
        [ 1.3971e-02, -4.3079e-03, -1.6968e-03],
        [ 7.7906e-03, -2.6306e-03, -6.0102e-03],
        [ 2.3358e-02,  7.6739e-03, -7.1400e-03],
        [ 9.7883e-03, -1.0204e-02, -1.0949e-02],
        [ 1.4298e-02,  9.7906e-05, -1.2857e-03],
        [ 8.5040e-03, -3.1265e-03,  7.7961e-04],
        [-7.7428e-04,  3.7023e-03, -2.0311e-03],
        [ 1.3185e-02, -5.0055e-03, -1.4466e-02],
        [ 1.7499e-02,  9.4441e-04,  8.1834e-03],
        [-4.9008e-04, -1.2273e-02, -1.7305e-03],
        [-9.1671e-03, -3.9672e-03, -9.7517e-04],
        [ 2.3891e-02, -2.0774e-02,  4.3906e-03],
        [ 6.8438e-03, -1.4144e-02, -6.9850e-03],
        [ 6.8585e-03, -7.0712e-03,  5.4263e-05],
        [ 7.5794e-03, -6.3296e-03, -4.2203e-03],
        [ 1.3858e-02,  7.2519e-05, -6.9721e-03],
        [ 2.2254e-02, -9.2565e-04, -4.0772e-03],
        [ 1.3048e-02,  1.8639e-03, -1.1675e-02],
        [ 1.8108e-02, -4.1627e-03, -3.0192e-04],
        [ 8.5582e-03,  3.2968e-04, -7.8571e-03],
        [ 2.0910e-02, -5.9112e-03,  5.0138e-03],
        [ 1.6360e-02,  3.1707e-03, -7.5328e-03],
        [ 1.3373e-02, -7.1330e-03, -2.6843e-03],
        [ 7.2514e-03, -1.1555e-02, -3.8863e-03],
        [ 1.1148e-02, -7.2181e-03, -8.3760e-03],
        [ 1.1672e-02,  3.1723e-03, -1.5237e-02],
        [ 3.9360e-03, -4.4433e-04,  2.9456e-03],
        [ 7.7774e-03, -3.4909e-03,  4.7789e-03],
        [ 8.3921e-03, -5.1546e-03,  4.6409e-03],
        [ 7.2049e-03, -3.3313e-03, -8.5353e-03],
        [ 1.8077e-02, -4.4438e-03, -3.7186e-03],
        [-1.0379e-02, -1.6404e-03,  6.5180e-03],
        [-4.2149e-03, -1.1721e-02,  9.6901e-03],
        [ 1.1820e-02, -1.8383e-03,  6.5411e-03],
        [ 1.3031e-02, -4.5110e-03,  2.5691e-03],
        [ 6.9824e-03, -1.1366e-02, -1.0012e-02],
        [-3.7132e-04, -2.8096e-03, -6.0395e-03],
        [ 7.9552e-03, -7.3151e-04,  3.9102e-03],
        [ 9.2675e-03, -7.3494e-03, -2.2598e-03],
        [ 5.2601e-03, -2.0332e-04, -1.1331e-03],
        [ 8.3921e-03, -5.1546e-03,  4.6409e-03],
        [-1.2838e-02, -8.1704e-03, -5.7744e-03],
        [ 1.4156e-02, -6.6108e-03, -6.4620e-04],
        [ 1.2356e-02, -9.0401e-03,  1.0899e-02],
        [-1.0764e-03, -1.5470e-02, -4.3397e-03],
        [-2.1367e-03,  2.0631e-03,  3.7940e-03],
        [ 1.7283e-02, -1.2730e-02,  7.8777e-03],
        [ 1.2236e-02,  1.3522e-03, -3.0539e-03],
        [ 4.9327e-03, -7.0559e-03,  4.8132e-03],
        [ 1.9307e-02,  5.3007e-03, -1.0562e-02],
        [ 1.2093e-02, -3.8118e-03,  3.8416e-03],
        [ 8.8081e-03,  2.3354e-03, -1.7511e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000026
penultimate_layer.0.bias: grad mean = 0.000131
output_layer.0.weight: grad mean = 0.000813
output_layer.0.bias: grad mean = 0.003903
[correct] no_actions is False
task_labels:  tensor([0, 0, 0, 1, 2, 2, 2, 2, 1, 1, 0, 0, 0, 1, 2, 1, 0, 0, 2, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 1, 1, 2, 1, 1, 2, 1, 0, 2, 0, 0,
        1, 1, 1, 0, 1, 1, 2, 0, 1, 2, 1, 2, 0, 1, 1, 0, 0, 2, 0, 2, 0, 0, 1, 2,
        1, 1, 0, 2, 2, 2, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 2, 2, 0, 1, 2, 2, 2, 1,
        1, 1, 2, 0, 1, 0, 1, 0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 2, 2, 0, 0, 1, 2, 0,
        2, 2, 1, 0, 0, 2, 2, 0]) , task_preds: tensor([[-5.1511e-03,  6.6209e-03, -1.1754e-02],
        [ 2.4243e-02,  7.3849e-04, -1.7891e-03],
        [ 8.4016e-03, -5.1443e-03,  4.6228e-03],
        [ 1.5835e-02, -1.2865e-02, -4.7852e-03],
        [ 5.1128e-04, -2.7075e-03, -1.5045e-03],
        [-1.2854e-03,  5.6979e-04, -4.4035e-03],
        [ 1.3901e-02,  6.2130e-03, -8.9904e-03],
        [-1.2827e-03, -2.2736e-03, -2.3859e-03],
        [-2.7859e-04, -2.0575e-02,  2.9833e-03],
        [ 1.0178e-02, -1.7041e-02, -3.4881e-03],
        [ 2.0701e-02, -1.3090e-02, -6.4238e-03],
        [ 1.3559e-02,  1.5646e-03,  3.9435e-04],
        [ 2.0775e-02, -2.7650e-03, -5.5179e-03],
        [ 1.8720e-02, -1.4380e-02,  6.0864e-04],
        [ 7.0250e-03, -2.1276e-02, -3.6158e-03],
        [ 5.0981e-03, -5.2818e-03, -1.5830e-03],
        [ 1.8181e-03, -1.0375e-02,  6.0080e-04],
        [ 7.0115e-03, -4.0312e-03, -1.0966e-02],
        [-4.3033e-03,  9.7592e-04,  7.5591e-03],
        [ 5.3619e-04, -1.0496e-02,  4.9898e-03],
        [ 7.5840e-03, -8.2544e-03,  5.7363e-03],
        [ 3.7244e-03, -1.0533e-02,  4.4231e-05],
        [ 5.8018e-03, -9.0306e-04,  1.0770e-02],
        [-1.0990e-03, -1.3822e-02, -3.1372e-03],
        [ 1.0723e-02, -1.9223e-03,  3.4355e-03],
        [ 1.8393e-03,  6.3948e-04, -8.0311e-03],
        [ 8.4147e-03, -9.3810e-03, -8.5460e-04],
        [ 3.9398e-03, -2.8236e-03,  1.4620e-02],
        [ 1.4697e-02, -2.7944e-03, -9.3805e-03],
        [ 9.1997e-03, -4.7854e-03, -2.6899e-03],
        [ 4.5974e-03, -3.8891e-03,  4.9507e-04],
        [-9.2196e-03, -1.4249e-02,  9.9242e-03],
        [ 1.4171e-02, -4.7482e-03,  1.5487e-03],
        [ 1.5371e-02, -3.8368e-03,  1.3259e-02],
        [ 5.8054e-03, -5.5327e-03,  3.3640e-04],
        [ 1.6539e-02, -7.0774e-03, -1.2939e-02],
        [-7.5450e-04, -2.8024e-02, -1.0639e-02],
        [-4.2137e-03, -9.4380e-03,  8.9552e-03],
        [ 1.3095e-02, -1.3072e-02, -8.0574e-04],
        [ 2.7895e-02, -4.6794e-04, -7.2552e-03],
        [-2.4785e-03,  2.0673e-03, -9.6617e-03],
        [ 1.5259e-02,  3.9480e-03, -9.6298e-04],
        [ 1.2233e-02,  9.6625e-03,  4.6242e-03],
        [-7.6019e-03, -3.7583e-03,  8.3557e-04],
        [ 1.0901e-02, -5.5586e-03, -6.1016e-03],
        [ 1.4622e-03, -8.0210e-03,  2.5493e-03],
        [ 5.5994e-03,  5.2275e-03, -2.6446e-03],
        [ 1.6984e-03, -1.6023e-03,  2.7438e-03],
        [ 1.8209e-02, -1.0108e-03, -8.0771e-04],
        [ 1.1519e-02, -2.6159e-03,  6.6128e-03],
        [ 1.4615e-02,  4.5068e-03, -1.1160e-02],
        [ 1.5363e-02, -4.8312e-04, -3.3276e-03],
        [-4.4502e-03, -6.1495e-03, -5.9843e-03],
        [ 7.8096e-03, -1.0582e-03,  7.2370e-03],
        [ 1.8132e-02, -2.7754e-04, -4.3181e-03],
        [ 2.8780e-02,  3.8814e-03, -6.6250e-03],
        [ 3.9844e-03,  2.0854e-03, -2.3740e-04],
        [ 3.9456e-03,  3.5450e-03,  4.1452e-03],
        [ 9.1524e-03, -4.7680e-03,  8.2838e-05],
        [ 4.3026e-04, -1.0826e-02, -1.1974e-02],
        [ 6.0701e-03, -5.2684e-03,  6.2431e-03],
        [-3.1548e-03, -4.7404e-03,  7.4536e-03],
        [ 2.4424e-02, -6.8484e-03, -1.5252e-03],
        [ 2.1493e-02, -2.3646e-03, -2.1339e-03],
        [ 1.8003e-03, -1.6571e-03,  8.3964e-03],
        [ 1.1700e-02, -8.3480e-04,  5.9736e-03],
        [ 8.9158e-03, -1.7149e-02, -5.3139e-03],
        [ 5.6439e-03, -1.3711e-03,  4.1431e-03],
        [ 1.0132e-02, -6.6028e-03, -9.9837e-03],
        [ 9.2363e-03, -5.6346e-03, -1.9942e-03],
        [ 3.5843e-03, -2.5564e-03, -5.6340e-03],
        [ 2.1957e-02, -8.0996e-03, -4.5524e-03],
        [ 1.5962e-02,  7.1513e-04, -6.1707e-04],
        [ 2.3577e-02, -2.3109e-03, -8.4369e-03],
        [ 1.3205e-02,  3.8263e-03,  6.7098e-03],
        [ 1.2260e-02, -6.6539e-03,  6.7380e-04],
        [ 1.9035e-02, -5.0001e-03, -2.3073e-03],
        [ 1.3934e-02, -7.8336e-03, -1.3819e-02],
        [ 8.5698e-03, -3.9526e-03,  3.0619e-03],
        [ 1.6755e-02,  5.1745e-03, -7.6181e-03],
        [ 8.4016e-03, -5.1443e-03,  4.6228e-03],
        [ 8.4016e-03, -5.1443e-03,  4.6228e-03],
        [ 3.9764e-03, -1.1899e-02, -1.6933e-02],
        [ 6.3539e-03, -4.3541e-03, -1.6861e-03],
        [ 1.3518e-02, -1.1916e-02, -7.5090e-03],
        [ 1.0160e-02, -1.6745e-02, -5.6463e-03],
        [ 1.7328e-02, -5.4408e-03,  8.0993e-04],
        [ 1.1473e-02, -3.9104e-03,  5.4975e-03],
        [-6.6853e-04, -8.3657e-03,  4.7715e-03],
        [ 8.4016e-03, -5.1443e-03,  4.6228e-03],
        [ 5.6037e-03,  5.8589e-03,  3.4616e-03],
        [ 1.0159e-02, -3.2300e-03,  2.2455e-03],
        [ 8.4016e-03, -5.1443e-03,  4.6228e-03],
        [ 1.3300e-02, -8.2436e-03, -3.6792e-03],
        [ 4.4710e-03, -6.2424e-03,  6.5924e-03],
        [-1.2093e-02,  2.7991e-03,  4.1948e-03],
        [ 1.8538e-03, -2.3541e-03, -7.3297e-04],
        [ 6.8159e-03, -8.0728e-03,  6.3640e-03],
        [ 1.1406e-02,  5.2591e-03,  5.8832e-03],
        [ 1.6597e-03, -1.8126e-03, -1.3571e-04],
        [ 2.1000e-03, -6.3483e-03,  4.8269e-04],
        [-3.6025e-04,  1.8810e-03,  1.1443e-03],
        [ 5.2366e-03, -1.2861e-03, -2.3411e-03],
        [-2.1638e-03,  6.4704e-03, -2.1302e-03],
        [ 4.0922e-03, -5.1240e-03, -1.0001e-02],
        [ 5.1096e-03, -5.2023e-03,  3.6542e-03],
        [-4.2214e-03, -1.1727e-02,  9.7008e-03],
        [ 1.2167e-02, -6.6030e-03,  4.2603e-03],
        [-2.2853e-03, -4.9669e-03, -5.3298e-03],
        [ 9.1013e-03, -4.8692e-03, -9.0053e-04],
        [ 3.6200e-03, -9.0986e-04, -2.4747e-03],
        [ 1.4369e-02, -1.0193e-02,  6.7235e-03],
        [ 1.2203e-02, -1.4660e-02, -4.9706e-03],
        [ 1.0454e-02, -3.4356e-03, -4.5205e-03],
        [ 8.9172e-03, -6.2488e-03,  4.9650e-03],
        [ 1.0040e-02, -3.2579e-03,  5.5474e-03],
        [ 5.4097e-03, -3.8296e-03,  1.1680e-02],
        [ 1.2472e-02, -1.1206e-02, -9.1284e-04],
        [-6.5551e-03, -1.2901e-02, -1.2482e-02],
        [ 1.6613e-02, -3.2102e-03,  1.2310e-02],
        [-6.5931e-04,  6.2417e-03,  1.0111e-02],
        [ 3.8849e-03, -8.0385e-03, -6.7788e-03],
        [ 8.1535e-03, -5.1787e-03, -1.0187e-03],
        [ 2.2005e-02, -8.3478e-03, -6.3849e-03],
        [ 1.6373e-02,  3.1664e-03, -7.5380e-03],
        [ 2.2420e-02,  1.8072e-03,  8.9812e-03],
        [ 1.2857e-02, -8.6309e-03, -1.5331e-03],
        [ 1.9494e-02, -1.1073e-02, -3.8170e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000025
penultimate_layer.0.bias: grad mean = 0.000350
output_layer.0.weight: grad mean = 0.000773
output_layer.0.bias: grad mean = 0.012149
[correct] no_actions is False
task_labels:  tensor([1, 2, 2, 0, 2, 2, 2, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 1, 0, 1,
        1, 0, 2, 2, 0, 0, 0, 1, 2, 0, 2, 1, 1, 0, 2, 1, 1, 2, 1, 2, 0, 0, 2, 1,
        1, 0, 0, 1, 2, 1, 2, 1, 2, 0, 2, 2, 2, 0, 2, 1, 1, 1, 2, 0, 2, 2, 1, 0,
        1, 1, 0, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0,
        2, 1, 2, 2, 0, 2, 0, 0, 0, 1, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 2, 0, 1, 1,
        0, 1, 1, 1, 2, 0, 2, 2]) , task_preds: tensor([[ 3.2282e-03, -5.0032e-03, -7.1118e-03],
        [ 9.0083e-03, -7.9261e-03,  2.0615e-03],
        [ 1.4762e-02, -9.5681e-03, -3.9644e-03],
        [ 7.8938e-03, -7.2046e-03,  3.7051e-03],
        [ 6.4733e-03, -7.4056e-03, -6.6607e-04],
        [ 3.6835e-03,  5.4872e-03,  2.4136e-03],
        [ 2.1143e-02,  3.5530e-03, -2.4175e-03],
        [-1.0068e-02, -5.5938e-03,  3.4761e-03],
        [ 8.5465e-03,  1.1179e-03, -4.7776e-03],
        [ 1.5411e-02,  2.6710e-03,  6.0778e-03],
        [ 8.4120e-03, -5.1416e-03,  4.6118e-03],
        [ 1.3473e-02, -6.7003e-03,  7.3181e-03],
        [ 1.7231e-02, -1.2292e-03, -5.3573e-03],
        [ 1.6598e-02, -1.3174e-02,  2.8404e-03],
        [ 9.6575e-03, -1.0923e-02, -2.8172e-03],
        [ 1.3779e-02, -3.5253e-03, -1.6597e-03],
        [ 1.3392e-02, -7.1372e-03, -2.6955e-03],
        [ 2.5722e-02,  8.3408e-03, -4.7210e-03],
        [ 9.3240e-03, -6.7848e-03, -1.1168e-03],
        [ 2.1443e-02, -5.3387e-03,  8.9272e-04],
        [-3.3447e-03, -1.2539e-02, -1.0304e-02],
        [ 1.0371e-02,  1.1686e-03, -7.7450e-03],
        [ 8.0246e-03, -4.9504e-03,  9.6836e-03],
        [ 1.1659e-02, -1.2982e-03,  4.2155e-04],
        [-2.6241e-03, -1.6556e-02,  1.7892e-03],
        [ 7.0875e-03, -1.8188e-02,  2.4729e-03],
        [ 9.4882e-03, -8.2962e-03,  1.6333e-03],
        [ 1.1773e-02, -9.7783e-03,  3.4986e-03],
        [ 2.1085e-02, -1.1574e-02, -4.8680e-03],
        [ 2.2773e-02, -9.7795e-03, -1.8557e-02],
        [ 4.6303e-03, -1.8764e-03,  4.0847e-03],
        [-7.6793e-03, -1.2824e-02, -2.9314e-03],
        [ 6.9249e-03, -7.6303e-03,  1.2609e-02],
        [ 3.5999e-03, -2.0688e-02, -8.1177e-03],
        [ 5.1192e-03, -6.6121e-03,  6.3836e-05],
        [ 1.0571e-02, -1.6890e-02,  3.3216e-03],
        [ 9.9653e-03,  7.8209e-03, -8.1287e-03],
        [ 1.7363e-02,  7.0400e-03, -3.4386e-03],
        [ 1.5586e-02, -5.3729e-03, -4.4804e-03],
        [ 1.2588e-02,  9.5014e-05, -5.8031e-03],
        [ 2.3909e-02, -2.0780e-02,  4.3827e-03],
        [ 9.3119e-03, -1.0488e-02,  8.1072e-03],
        [ 3.9974e-03,  1.0419e-03, -4.6107e-03],
        [ 1.5387e-02, -2.3897e-03,  5.5220e-03],
        [ 1.0945e-02, -1.1681e-03,  1.2113e-02],
        [ 2.4606e-02, -1.2555e-02, -1.5205e-03],
        [ 9.8410e-03, -3.3025e-03,  1.1362e-02],
        [ 6.9011e-03, -1.0556e-02, -9.6838e-04],
        [ 4.6893e-03, -4.1750e-03,  4.8060e-03],
        [ 1.0983e-02, -4.2935e-03, -9.6743e-04],
        [-4.2448e-03, -1.6019e-03, -7.8062e-03],
        [ 3.9929e-03, -2.2330e-03,  9.2280e-04],
        [ 8.4120e-03, -5.1416e-03,  4.6118e-03],
        [ 9.9205e-03, -4.3926e-04,  5.7743e-03],
        [-2.2144e-03, -1.0653e-02,  5.1069e-04],
        [ 6.2119e-03, -2.6131e-03, -8.6888e-03],
        [ 6.3755e-03, -5.2320e-04,  7.5812e-04],
        [ 4.4290e-03, -6.5301e-03,  6.7930e-03],
        [ 2.2799e-03, -1.1611e-02,  4.6826e-03],
        [-1.0423e-02, -1.2298e-02, -6.3885e-03],
        [ 6.5023e-03,  1.1603e-02,  6.2599e-04],
        [ 1.3003e-02, -1.3396e-02, -1.4426e-02],
        [ 1.5279e-02, -7.7191e-03, -4.1115e-03],
        [ 4.4733e-03, -1.1417e-02,  1.5500e-06],
        [ 1.2831e-04, -2.7178e-03,  9.0645e-03],
        [ 2.3240e-03, -2.0720e-03, -1.4436e-02],
        [ 5.7137e-03, -1.2920e-02, -1.5629e-02],
        [ 1.3844e-02,  2.5900e-04, -5.6408e-03],
        [ 5.8688e-03, -6.9769e-04,  8.3539e-04],
        [ 8.5126e-03,  4.3534e-03,  1.2162e-03],
        [ 5.0569e-03, -5.6676e-03,  6.0093e-04],
        [ 2.3117e-02,  9.7588e-04, -7.0857e-03],
        [ 1.2134e-02, -4.9386e-03,  2.7050e-03],
        [ 5.7841e-03, -2.9519e-03,  1.3646e-03],
        [ 1.5262e-02, -5.1499e-03,  8.7426e-03],
        [ 7.0692e-03,  1.1518e-03,  4.8857e-03],
        [-3.8286e-03, -2.1793e-02, -1.5303e-03],
        [ 7.2561e-03, -3.8007e-03, -8.1719e-04],
        [ 2.0211e-02, -1.3665e-02, -1.2739e-03],
        [ 1.3058e-02, -8.2670e-03,  2.6327e-03],
        [-1.0294e-02, -1.4940e-02, -8.2702e-03],
        [ 3.6356e-03, -6.6071e-03, -3.5786e-03],
        [ 2.6680e-03, -5.9486e-03,  2.5930e-04],
        [ 3.2445e-03, -1.3519e-02,  1.2350e-03],
        [ 6.1796e-03,  1.2655e-03, -2.4883e-03],
        [ 1.4511e-02, -2.0973e-03, -1.2205e-03],
        [-2.2896e-03, -4.2570e-03,  5.2407e-03],
        [ 2.4554e-02, -1.4481e-02, -4.1948e-03],
        [ 7.2989e-04, -6.9902e-03, -1.6108e-03],
        [-5.1093e-03, -2.4814e-03, -1.6335e-03],
        [-6.6182e-04,  1.8321e-03, -2.1877e-03],
        [ 1.3522e-02,  6.5011e-04,  1.0852e-02],
        [ 6.3700e-03,  7.2136e-03, -8.6882e-04],
        [ 1.1549e-02, -2.0627e-03, -1.1969e-03],
        [ 6.1468e-03,  4.5683e-04, -7.3454e-03],
        [ 1.1773e-02, -1.0586e-02, -2.9026e-03],
        [ 1.2331e-02,  1.1311e-03,  4.1186e-04],
        [ 4.4181e-03, -1.0432e-02, -3.3368e-03],
        [ 1.1928e-02, -5.0427e-03,  1.5755e-03],
        [ 1.1788e-02, -1.1611e-02, -3.1930e-04],
        [ 6.8802e-03, -7.0671e-03,  3.3070e-05],
        [ 8.4120e-03, -5.1416e-03,  4.6118e-03],
        [ 9.9203e-03, -1.5640e-02, -1.2574e-03],
        [ 1.0678e-02, -7.1169e-03,  1.4028e-03],
        [ 1.4359e-02, -1.8050e-03, -8.9772e-03],
        [ 8.7468e-03, -1.2448e-02, -4.4123e-03],
        [ 5.1141e-03, -1.1048e-02, -4.1446e-03],
        [ 8.6838e-04, -1.2073e-02,  1.7095e-03],
        [ 4.4917e-03, -1.6284e-03, -5.2079e-03],
        [ 8.0515e-03, -1.1322e-02,  2.5768e-03],
        [ 8.5415e-03, -1.3224e-03,  6.4370e-04],
        [ 5.9281e-03, -3.5283e-03,  2.5188e-03],
        [-3.4265e-04,  8.7749e-04, -1.1411e-02],
        [ 1.3315e-02, -9.7535e-03, -1.7260e-02],
        [ 1.5746e-02, -8.7077e-03,  6.5564e-03],
        [ 4.3105e-03, -6.7932e-03,  7.2831e-04],
        [ 1.6642e-02, -6.1586e-03,  2.9385e-04],
        [ 1.1102e-02, -1.2046e-04, -6.7918e-04],
        [ 1.3550e-02,  1.2487e-03, -9.7160e-04],
        [-2.6604e-03, -4.3296e-03,  2.5331e-03],
        [-1.0078e-04, -4.8030e-03,  7.1178e-03],
        [ 1.8140e-02, -8.8163e-03,  1.4454e-03],
        [ 1.2371e-02, -3.6976e-03,  2.4714e-03],
        [ 1.4539e-02,  1.1497e-02, -2.2349e-03],
        [ 6.2101e-03, -2.6051e-03,  6.7222e-03],
        [ 1.7452e-02, -1.5458e-02,  3.2560e-03],
        [-3.4004e-03, -1.6451e-02, -1.7905e-02],
        [ 1.5305e-02, -8.4897e-03,  3.4947e-03]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000037
penultimate_layer.0.bias: grad mean = 0.001345
output_layer.0.weight: grad mean = 0.001151
output_layer.0.bias: grad mean = 0.040733
[correct] no_actions is False
class_weights:  [1.0833334 0.8666667 1.0833334]
task_preds:  tensor([[ 9.0814e-03, -4.4804e-03, -3.4630e-05],
        [-3.0310e-03, -2.1054e-02,  2.6590e-03],
        [ 1.9083e-02, -9.2091e-03, -9.0252e-03],
        [-4.4464e-03, -6.6338e-03,  1.3938e-02],
        [ 1.2307e-02, -7.6951e-03, -7.7420e-04],
        [ 4.9504e-03, -8.5974e-03, -1.3306e-03],
        [-2.6907e-03,  4.1957e-04,  2.2461e-02],
        [ 3.7353e-03, -1.4424e-02,  1.9315e-03],
        [-6.0670e-03, -4.4561e-03,  1.2036e-02],
        [ 5.9083e-03, -4.6012e-03, -8.6142e-03],
        [-1.7996e-03, -2.8031e-03, -2.2227e-03],
        [ 1.4603e-02, -7.4248e-03, -1.5842e-03],
        [ 1.3656e-02, -1.2113e-03,  4.1310e-03],
        [ 4.2674e-03, -5.6156e-03, -1.2870e-02],
        [ 1.3649e-02, -3.6526e-03, -6.1450e-03],
        [ 1.1144e-02,  1.8377e-04, -5.7080e-03],
        [ 1.0977e-02, -1.1117e-02, -2.0457e-03],
        [ 1.1162e-02, -2.8040e-04,  4.3774e-03],
        [ 1.3921e-02, -1.1663e-02, -6.1141e-04],
        [ 2.8722e-02,  2.8191e-03, -3.8095e-03],
        [ 2.1228e-02, -2.2254e-03,  2.3883e-03],
        [ 8.4133e-03, -5.1412e-03,  4.6103e-03],
        [ 1.3632e-02, -9.6031e-04, -3.2183e-04],
        [ 1.4729e-02, -2.0399e-03, -1.2750e-03],
        [ 3.0557e-03,  6.6304e-03, -2.5058e-03],
        [-1.6436e-02, -1.3188e-02,  1.5315e-02],
        [ 5.0719e-03, -1.6172e-02,  9.7782e-05],
        [ 1.2846e-02, -1.2350e-03, -4.0808e-03],
        [ 9.7558e-03, -1.1511e-02, -6.7499e-03],
        [ 1.0296e-02, -4.6517e-03,  6.6402e-03],
        [ 1.4649e-02, -3.1092e-03,  2.8236e-03],
        [ 1.9320e-02, -1.4568e-02, -6.0406e-03],
        [ 3.2099e-03, -1.4724e-02,  7.8206e-03],
        [ 1.6281e-02, -4.7816e-03, -2.3924e-03],
        [ 4.1417e-04, -1.2146e-02, -2.6132e-03],
        [ 1.0056e-02, -4.4645e-03, -1.2861e-03],
        [-2.0455e-03, -1.2321e-02, -3.6544e-03],
        [ 1.1462e-02,  7.6415e-05, -1.2060e-02],
        [ 8.3525e-03, -3.9690e-03, -3.1901e-03],
        [ 1.2244e-02, -2.1869e-03,  4.4432e-03],
        [-3.3600e-03, -1.0307e-02, -5.7394e-03],
        [ 6.2104e-03,  4.0971e-03,  8.8010e-03],
        [ 1.2024e-02, -6.6458e-03, -1.1854e-02],
        [ 5.5128e-03, -8.0583e-03,  9.9859e-03],
        [ 1.3076e-02, -5.7605e-03,  2.9140e-05],
        [ 2.2314e-03, -1.0050e-02,  1.0671e-02],
        [-1.1792e-03, -1.0851e-02, -7.2686e-03],
        [ 7.9963e-03,  4.4537e-03,  1.5198e-03],
        [ 8.3413e-03, -3.0293e-03,  7.2859e-04],
        [-9.7386e-03, -8.3439e-03, -3.0376e-03],
        [ 7.4490e-03, -5.0622e-03, -1.7502e-02],
        [ 6.5887e-03, -3.0854e-03, -5.3103e-03],
        [-5.2472e-03, -8.0995e-03,  5.5758e-03],
        [ 6.0623e-03, -9.9048e-03, -1.4838e-03],
        [-3.6611e-03, -9.5193e-03,  1.1810e-02],
        [ 2.0918e-02, -2.1004e-03, -3.2311e-04],
        [ 6.1567e-03, -1.5120e-02, -7.5450e-04],
        [ 9.7452e-03, -7.4907e-03,  1.2653e-03],
        [-3.0347e-03, -5.7117e-03,  3.0032e-03],
        [ 1.2713e-02, -4.0047e-03,  2.6160e-03],
        [ 4.6845e-03, -6.5102e-03,  8.6422e-03],
        [ 1.3270e-02, -9.7601e-03, -4.6630e-03],
        [ 8.4376e-03, -1.1441e-02,  1.1081e-02],
        [ 4.2025e-03, -3.2787e-03,  5.7648e-03],
        [ 1.5232e-02, -1.2877e-02,  3.7796e-03],
        [ 7.4821e-03,  1.1741e-03,  7.4998e-03],
        [ 2.2966e-03, -1.3331e-02,  2.8938e-03],
        [ 5.1226e-03, -3.4597e-03, -2.9853e-03],
        [-5.7870e-03, -8.8125e-03, -2.0211e-04],
        [ 8.4133e-03, -5.1412e-03,  4.6103e-03],
        [ 1.7188e-04, -1.8468e-02,  3.6771e-03],
        [ 1.0660e-02, -1.2905e-02,  1.3435e-03],
        [ 6.3936e-03, -1.2448e-04, -2.5988e-03],
        [ 6.2120e-03, -5.3535e-03, -2.4451e-03],
        [ 1.3507e-02, -1.0696e-02, -2.1249e-03],
        [ 9.0069e-03, -9.0921e-03, -7.5438e-03],
        [ 2.7323e-03, -9.8656e-03,  1.0988e-03],
        [ 8.5818e-03, -4.0093e-03,  1.3798e-02],
        [ 1.1510e-02, -7.1332e-03,  1.5935e-03],
        [ 3.4432e-03, -3.8515e-03, -1.5503e-03],
        [ 1.0743e-02,  2.7243e-03, -1.1188e-02],
        [ 3.6584e-03, -1.0782e-02,  2.3412e-03],
        [ 1.4604e-02, -3.4853e-03, -1.1646e-04],
        [ 8.4133e-03, -5.1412e-03,  4.6103e-03],
        [ 7.2559e-03, -3.8056e-03, -8.1823e-04],
        [ 1.5712e-02,  6.3272e-03,  1.6991e-03],
        [ 3.9102e-04, -7.3268e-03, -2.2198e-03],
        [ 9.1883e-03, -8.9229e-03, -2.8801e-03],
        [ 1.9319e-02,  4.0135e-03,  4.1422e-03],
        [ 1.0759e-02, -5.8243e-03,  7.6593e-03],
        [ 1.2428e-02, -7.6790e-03,  1.1213e-02],
        [-4.8215e-03, -9.2617e-03,  9.6221e-03],
        [ 8.6445e-03, -6.2323e-03,  2.5263e-03],
        [ 1.9106e-02, -7.0493e-03,  2.4113e-03],
        [-9.4417e-03,  8.4583e-03, -9.6655e-03],
        [ 1.2094e-02,  3.0126e-04, -1.1522e-02],
        [-1.4281e-03,  5.2289e-03, -2.9474e-03],
        [ 1.9338e-02, -3.0710e-03, -6.4086e-04],
        [ 2.2722e-02, -2.5326e-03,  2.3188e-03],
        [ 1.8626e-02, -9.1526e-03,  1.1423e-03],
        [ 1.4228e-02, -1.1059e-02, -9.5560e-03],
        [ 1.1615e-02, -1.0507e-02,  1.4828e-03],
        [ 1.3219e-02, -6.3598e-03, -1.3044e-02],
        [ 4.3844e-03, -6.8064e-03, -1.2046e-02],
        [-4.3231e-03,  7.9453e-04, -2.8304e-03],
        [ 7.4831e-03,  1.0395e-03, -5.4305e-03],
        [ 1.2638e-02, -9.8828e-03, -1.3462e-03],
        [ 1.3163e-02, -2.9873e-03,  9.5589e-04],
        [-9.0631e-03, -6.0608e-03, -4.5127e-03],
        [ 1.4733e-02, -6.3162e-03,  5.6980e-03],
        [ 1.0127e-02,  8.8865e-06,  6.9616e-03],
        [ 8.0996e-03, -1.1520e-02, -1.8646e-03],
        [ 8.9117e-03, -2.1363e-03,  3.4788e-04],
        [ 7.0518e-03,  6.4409e-04,  1.1637e-02],
        [ 5.3372e-04, -1.8940e-03,  2.0668e-03],
        [ 8.8790e-03, -2.3230e-03,  1.7859e-03],
        [ 5.7127e-03, -3.9746e-03,  2.3967e-03],
        [ 6.9913e-03, -8.9796e-03, -1.4627e-02],
        [ 1.8005e-02, -3.3292e-03,  4.6059e-03],
        [ 7.4024e-03, -7.5886e-03,  5.7853e-03],
        [ 1.2576e-02,  2.2479e-03,  8.9322e-03],
        [ 1.8284e-02, -7.8808e-03, -1.8537e-03],
        [ 1.8226e-02,  2.2165e-03,  1.2120e-02],
        [ 1.1740e-02,  4.0041e-03,  5.0957e-03],
        [ 8.2198e-03, -1.7244e-03,  5.7828e-03],
        [ 1.3903e-02, -3.1808e-03,  2.7989e-03],
        [-9.8856e-05, -7.0000e-03,  2.7534e-03],
        [ 4.6998e-03,  1.7448e-03,  4.2964e-03]])
task_pred: tensor([0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0,
        0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 0, 2, 0,
        0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 2, 0]), task_id: tensor([1, 0, 0, 2, 0, 1, 1, 2, 1, 1, 0, 0, 2, 0, 0, 0, 2, 0, 2, 1, 0, 1, 0, 1,
        1, 2, 0, 1, 1, 0, 0, 0, 1, 2, 1, 0, 1, 0, 2, 2, 0, 2, 2, 1, 2, 1, 2, 2,
        1, 1, 0, 1, 2, 1, 2, 2, 1, 0, 0, 1, 0, 2, 0, 1, 1, 2, 2, 0, 1, 1, 1, 0,
        1, 2, 0, 1, 0, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 0, 2, 1, 2, 2, 0, 1, 2, 0,
        2, 0, 1, 2, 0, 2, 1, 1, 0, 0, 2, 1, 0, 2, 1, 0, 1, 2, 2, 1, 1, 1, 2, 1,
        2, 0, 2, 0, 1, 0, 2, 1])
[correct] no_actions is False
class_weights:  [1.0833334 0.8666667 1.0833334]
task_preds:  tensor([[ 0.0052,  0.0017,  0.0069],
        [ 0.0035, -0.0036, -0.0069]])
task_pred: tensor([2, 0]), task_id: tensor([0, 2])
[correct] no_actions is False
class_weights:  [1.0833334 0.8666667 1.0833334]
task_preds:  tensor([[ 5.2355e-03, -5.2221e-04, -6.4906e-03],
        [ 2.1465e-02,  3.1300e-03, -8.3089e-03],
        [ 1.3583e-02, -4.3249e-03,  4.1102e-03],
        [ 1.6154e-02,  4.5380e-03, -1.9503e-03],
        [-4.9185e-03, -1.3172e-02,  9.1071e-03],
        [ 7.4821e-03,  1.1741e-03,  7.4998e-03],
        [ 8.6968e-03, -4.3017e-03, -1.4713e-02],
        [ 2.2864e-03, -6.2375e-03,  2.4590e-03],
        [ 1.0896e-02, -1.2109e-03, -2.7562e-03],
        [ 4.9677e-03, -2.1706e-04, -5.3093e-03],
        [ 5.0719e-03, -1.6172e-02,  9.7782e-05],
        [ 6.1543e-03,  2.2928e-03, -7.7077e-03],
        [ 9.0814e-03, -4.4804e-03, -3.4630e-05],
        [ 1.8911e-02, -9.3572e-03, -5.6163e-03],
        [ 9.0069e-03, -9.0921e-03, -7.5438e-03],
        [ 2.9954e-03, -9.7171e-03,  7.9822e-03],
        [ 6.8848e-04, -1.8566e-02, -1.6745e-02],
        [ 6.9072e-03,  3.6075e-03, -7.3216e-03],
        [ 1.0158e-02, -8.1659e-03, -1.0400e-02],
        [ 2.8722e-02,  2.8191e-03, -3.8095e-03],
        [ 1.7659e-02, -1.5885e-03,  1.4668e-03],
        [ 3.0137e-03, -3.9889e-03, -3.2950e-03],
        [ 6.7460e-03, -1.1848e-02, -5.7413e-03],
        [-8.3294e-03, -9.8446e-03, -1.0002e-02],
        [ 1.2963e-02, -1.0330e-02, -1.6630e-03],
        [ 8.4133e-03, -5.1412e-03,  4.6103e-03],
        [ 8.5878e-03, -2.4157e-03,  9.2127e-03],
        [-4.2042e-03, -1.1490e-02, -6.6240e-03],
        [ 3.9844e-03, -3.7378e-04, -5.5672e-03],
        [ 8.4133e-03, -5.1412e-03,  4.6103e-03],
        [ 1.7164e-02,  9.4724e-03,  5.0874e-04],
        [ 4.3844e-03, -6.8064e-03, -1.2046e-02],
        [ 1.6922e-02, -9.9092e-03,  5.3175e-04],
        [ 1.2576e-02,  2.2479e-03,  8.9322e-03],
        [ 1.6645e-02, -5.3480e-03, -3.2062e-03],
        [ 7.3489e-03, -2.5137e-03,  2.9117e-04],
        [ 9.7452e-03, -7.4907e-03,  1.2653e-03],
        [ 1.3134e-02, -2.6061e-03, -7.2166e-04],
        [ 4.0766e-03, -1.2657e-02,  7.0507e-04],
        [ 4.3560e-03, -4.2247e-03,  4.5450e-03],
        [ 4.3811e-03, -7.3527e-03,  6.1043e-03],
        [ 2.2069e-03, -5.7380e-04,  7.4510e-03],
        [ 1.9718e-02,  5.5708e-03,  3.1825e-03],
        [ 8.4133e-03, -5.1412e-03,  4.6103e-03],
        [ 1.4815e-02, -4.1514e-03, -7.8420e-04],
        [ 8.0996e-03, -1.1520e-02, -1.8646e-03],
        [ 6.1567e-03, -1.5120e-02, -7.5450e-04],
        [-2.6907e-03,  4.1957e-04,  2.2461e-02],
        [-4.1742e-03, -4.6217e-03, -2.4896e-04],
        [-6.8743e-03, -2.7260e-03, -6.7479e-03],
        [ 2.1070e-02, -1.0404e-03, -7.5874e-03],
        [ 8.4133e-03, -5.1412e-03,  4.6103e-03],
        [ 3.6549e-03,  5.1428e-04,  1.1063e-02],
        [ 1.8626e-02, -9.1526e-03,  1.1423e-03],
        [ 1.6286e-02, -7.2219e-03, -2.6745e-03],
        [ 8.1754e-03,  1.6730e-03,  1.5236e-04],
        [ 1.1896e-02, -3.0961e-03,  3.2953e-03],
        [ 2.0449e-02, -5.8527e-03, -2.3881e-03],
        [ 8.5382e-03, -5.8401e-04, -1.7927e-02],
        [ 1.8070e-02, -1.1877e-02,  2.7462e-03],
        [ 2.1828e-03, -5.0070e-03, -1.5629e-03],
        [ 1.5921e-02, -8.5713e-03, -4.7975e-03],
        [ 3.4432e-03, -3.8515e-03, -1.5503e-03],
        [ 5.4307e-03, -1.1657e-02,  2.7193e-03],
        [ 6.5422e-03,  4.9627e-03, -7.2216e-03],
        [ 6.3936e-03, -1.2448e-04, -2.5988e-03],
        [ 1.3507e-02, -1.0696e-02, -2.1249e-03],
        [-1.4031e-03, -3.5351e-03, -1.3877e-03],
        [ 1.1337e-02, -1.1098e-02, -1.8278e-02],
        [ 8.8790e-03, -2.3230e-03,  1.7859e-03],
        [ 9.7969e-03, -7.1576e-03,  5.7369e-03],
        [ 1.9320e-02, -1.4568e-02, -6.0406e-03],
        [-2.3702e-03, -1.0443e-02, -1.0906e-02],
        [ 9.9604e-03, -3.3576e-03,  8.0750e-05],
        [-4.8351e-03, -2.3385e-02,  1.2308e-02],
        [ 1.5580e-02, -5.5583e-03, -2.0833e-03],
        [ 1.8108e-02, -2.9492e-03, -9.3630e-03],
        [-3.6611e-03, -9.5193e-03,  1.1810e-02],
        [ 8.8800e-03, -5.4661e-03, -1.1609e-03],
        [ 2.3599e-02, -4.7393e-03, -8.2559e-03],
        [ 1.3643e-02, -1.8095e-03,  5.6492e-03],
        [-3.3171e-03,  5.6008e-03, -9.8930e-03],
        [ 1.8284e-02, -7.8808e-03, -1.8537e-03],
        [ 1.5719e-02, -1.1497e-02,  1.8730e-03],
        [ 9.9274e-03, -4.9673e-03,  2.0242e-03],
        [ 1.1193e-02, -8.3530e-03,  3.4869e-03],
        [ 1.3903e-02, -3.1808e-03,  2.7989e-03],
        [ 8.4376e-03, -1.1441e-02,  1.1081e-02],
        [ 7.0787e-03, -2.5292e-03, -2.1110e-03],
        [ 2.0619e-02, -1.6274e-02,  3.0172e-03],
        [ 1.7799e-02, -6.0748e-03, -1.4813e-02],
        [ 1.6555e-02, -5.6882e-03, -1.0639e-03],
        [ 7.9142e-03, -7.6546e-03,  4.2310e-03],
        [ 1.1581e-02, -3.2431e-03,  4.0134e-03],
        [ 5.6160e-03, -3.8843e-03,  1.1267e-03],
        [-3.1811e-03, -1.3503e-02, -4.6447e-03],
        [-1.1041e-03, -5.6582e-03,  9.7582e-03],
        [-5.7870e-03, -8.8125e-03, -2.0211e-04],
        [ 1.4087e-02, -3.2142e-03,  8.5629e-03],
        [ 6.9399e-03, -1.5930e-02,  1.5693e-04],
        [ 8.2441e-03, -6.4897e-03, -1.1451e-02],
        [ 1.3324e-02, -6.3751e-03,  4.4049e-03],
        [ 2.2722e-02, -2.5326e-03,  2.3188e-03],
        [ 1.2307e-02, -7.6951e-03, -7.7420e-04],
        [ 1.3570e-02,  5.4419e-03,  9.0616e-05],
        [ 1.6869e-02,  1.1971e-03, -2.2163e-03],
        [ 8.3413e-03, -3.0293e-03,  7.2859e-04],
        [ 6.9449e-03,  4.9534e-03, -8.4689e-03],
        [ 2.1556e-02, -1.0879e-03,  6.4118e-03],
        [ 1.2713e-02, -4.0047e-03,  2.6160e-03],
        [ 1.8226e-02,  2.2165e-03,  1.2120e-02],
        [ 5.4546e-03, -1.5228e-02,  1.4101e-02],
        [ 1.0286e-02,  7.1794e-04,  8.7105e-03],
        [ 1.2250e-02, -7.1726e-04, -2.2153e-03],
        [ 1.4729e-02, -2.0399e-03, -1.2750e-03],
        [ 2.2966e-03, -1.3331e-02,  2.8938e-03],
        [ 4.9899e-03, -5.3220e-03, -2.4975e-03],
        [ 2.0988e-02, -2.4113e-03, -3.8624e-04],
        [ 8.3525e-03, -3.9690e-03, -3.1901e-03],
        [ 8.0141e-03, -1.0732e-02, -7.2159e-03],
        [ 1.2899e-02,  1.1264e-02,  1.1129e-02],
        [ 3.6584e-03, -1.0782e-02,  2.3412e-03],
        [ 1.9971e-02,  3.5747e-03,  1.2045e-03],
        [ 1.7529e-04, -1.1354e-02,  7.2439e-03],
        [ 1.8005e-02, -3.3292e-03,  4.6059e-03],
        [-2.6554e-03, -7.7638e-03, -1.5668e-02],
        [ 1.4518e-02,  2.5002e-03, -9.0356e-03],
        [ 2.0221e-02, -8.5513e-03,  1.7580e-03]])
task_pred: tensor([0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2,
        2, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,
        0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,
        2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0,
        0, 0, 0, 2, 0, 0, 0, 0]), task_id: tensor([2, 0, 0, 1, 1, 2, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 2, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 2, 0, 1, 0, 1, 0, 2, 1, 2, 0, 2, 2, 0, 2, 2, 0, 1, 2, 0, 1, 1,
        2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 0, 1, 0, 1, 2, 2, 1, 0, 1, 2, 1, 1, 0,
        1, 1, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 1, 2, 0, 1, 2, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 2, 1, 1, 2, 1, 0, 2, 1, 2, 2, 0, 2, 2,
        0, 2, 2, 0, 2, 2, 1, 2])
[correct] no_actions is False
class_weights:  [1.0833334 0.8666667 1.0833334]
task_preds:  tensor([[-0.0052, -0.0081,  0.0056],
        [ 0.0073, -0.0038, -0.0008]])
task_pred: tensor([2, 0]), task_id: tensor([2, 1])
[correct] no_actions is False
class_weights:  [1.0833334 0.8666667 1.0833334]
task_preds:  tensor([[ 1.4675e-02, -1.0116e-02,  4.6368e-03],
        [ 5.6160e-03, -3.8843e-03,  1.1267e-03],
        [ 1.7122e-02,  8.0718e-03,  8.3678e-03],
        [ 1.3282e-02,  3.6600e-03, -3.8854e-03],
        [ 1.0180e-02, -5.8614e-04, -4.2744e-03],
        [ 5.8172e-03, -3.6380e-03, -2.2970e-03],
        [ 8.6445e-03, -6.2323e-03,  2.5263e-03],
        [ 1.3313e-02, -3.7087e-03, -1.1642e-02],
        [-9.2801e-03, -1.0970e-02, -1.0172e-02],
        [ 2.0449e-02, -5.8527e-03, -2.3881e-03],
        [ 4.0766e-03, -1.2657e-02,  7.0507e-04],
        [ 9.7579e-04, -1.1216e-02, -3.2487e-03],
        [ 2.6779e-02, -1.1634e-02, -4.8446e-03],
        [ 8.0141e-03, -1.0732e-02, -7.2159e-03],
        [ 2.3598e-03,  1.1864e-04,  1.0045e-02],
        [ 3.0137e-03, -3.9889e-03, -3.2950e-03],
        [ 8.6006e-03, -1.8469e-03,  1.3028e-02],
        [ 1.3632e-02, -9.6031e-04, -3.2183e-04],
        [ 3.3234e-03, -1.5939e-02,  1.0003e-02],
        [-4.8351e-03, -2.3385e-02,  1.2308e-02],
        [ 1.2359e-02, -6.8209e-03,  4.5644e-03],
        [ 1.6579e-02, -4.3735e-03, -5.2493e-03],
        [ 2.0093e-02, -6.5809e-03,  5.8355e-03],
        [ 4.2674e-03, -5.6156e-03, -1.2870e-02],
        [-1.7996e-03, -2.8031e-03, -2.2227e-03],
        [ 4.3844e-03, -6.8064e-03, -1.2046e-02],
        [ 1.8613e-03,  7.3604e-03, -7.9665e-03],
        [-2.6907e-03,  4.1957e-04,  2.2461e-02],
        [ 1.5128e-02,  1.2422e-03, -5.0648e-03],
        [ 1.0286e-02,  7.1794e-04,  8.7105e-03],
        [ 3.3298e-03, -7.7395e-03,  1.0425e-02],
        [ 1.5719e-02, -1.1497e-02,  1.8730e-03],
        [ 1.9168e-02, -2.2761e-03, -7.5515e-03],
        [ 1.4433e-02,  5.9292e-04, -1.9911e-03],
        [ 1.9083e-02, -9.2091e-03, -9.0252e-03],
        [ 8.4133e-03, -5.1412e-03,  4.6103e-03],
        [ 1.0788e-02, -9.5719e-03,  7.2359e-04],
        [ 1.3120e-02, -2.4096e-03, -8.2715e-04],
        [ 1.3893e-02, -1.0227e-02, -8.6397e-03],
        [-4.6044e-03, -1.7297e-02, -9.8878e-03],
        [ 9.1883e-03, -8.9229e-03, -2.8801e-03],
        [ 7.0082e-03, -3.4244e-03,  4.5079e-03],
        [ 1.4087e-02, -3.2142e-03,  8.5629e-03],
        [ 2.2966e-03, -1.3331e-02,  2.8938e-03],
        [ 1.8108e-02, -2.9492e-03, -9.3630e-03],
        [ 1.6778e-02, -1.2568e-02,  4.4293e-03],
        [ 5.0719e-03, -1.6172e-02,  9.7782e-05],
        [ 1.4407e-02, -9.5716e-03, -5.8915e-03],
        [ 1.2931e-02, -3.8374e-03,  1.9854e-04],
        [-4.4464e-03, -6.6338e-03,  1.3938e-02],
        [ 7.0518e-03,  6.4409e-04,  1.1637e-02],
        [ 3.4229e-04, -1.2377e-02, -3.1920e-03],
        [-1.4114e-04, -4.7155e-03, -3.3645e-03],
        [ 4.9616e-03, -2.4994e-02,  8.1763e-03],
        [ 6.1567e-03, -1.5120e-02, -7.5450e-04],
        [ 1.3324e-02, -6.3751e-03,  4.4049e-03],
        [ 1.3779e-02, -5.4897e-03, -1.8363e-03],
        [ 1.5921e-02, -8.5713e-03, -4.7975e-03],
        [ 1.2829e-02, -6.0576e-03, -2.3680e-03],
        [ 6.1543e-03,  2.2928e-03, -7.7077e-03],
        [ 7.9275e-03, -7.8552e-03, -1.5720e-03],
        [ 7.6442e-03, -3.1430e-03, -4.8630e-03],
        [ 7.4821e-03,  1.1741e-03,  7.4998e-03],
        [ 1.5187e-02, -6.1206e-03, -3.3237e-03],
        [ 9.4454e-03, -3.2427e-03, -7.9786e-04],
        [ 1.3134e-02,  3.9322e-03, -9.9402e-03],
        [ 6.3936e-03, -1.2448e-04, -2.5988e-03],
        [ 1.0987e-02, -1.2868e-02, -4.6659e-04],
        [ 9.4066e-03, -4.4585e-03, -4.5795e-04],
        [ 5.8537e-03, -1.7604e-05, -7.8826e-04],
        [ 1.3959e-02, -1.2441e-02, -3.4549e-03],
        [-5.7870e-03, -8.8125e-03, -2.0211e-04],
        [ 8.4376e-03, -1.1441e-02,  1.1081e-02],
        [ 1.2668e-02, -4.6008e-03,  5.9991e-03],
        [ 1.8005e-02, -3.3292e-03,  4.6059e-03],
        [ 4.5349e-03, -1.2481e-02,  1.2547e-03],
        [ 1.7446e-02, -7.0957e-04,  1.6486e-03],
        [ 1.5548e-02, -2.4460e-03, -9.8775e-03],
        [ 8.3413e-03, -3.0293e-03,  7.2859e-04],
        [ 7.9816e-03, -3.5397e-03, -9.4453e-03],
        [ 2.4926e-03, -1.0355e-03, -3.3817e-03],
        [ 4.8406e-03, -1.0574e-02, -4.1600e-03],
        [ 1.1574e-02, -1.7431e-02,  2.2023e-03],
        [ 1.7790e-02, -5.7989e-03,  3.7511e-03],
        [ 1.8552e-02, -2.7708e-03,  6.6071e-03],
        [ 2.0793e-03, -4.9472e-03,  2.3463e-03],
        [ 8.4133e-03, -5.1412e-03,  4.6103e-03],
        [-3.0708e-03,  3.1505e-03, -2.3527e-03],
        [ 1.5511e-02, -4.5262e-03,  3.7475e-03],
        [ 3.7353e-03, -1.4424e-02,  1.9315e-03],
        [ 1.0287e-02, -2.5869e-03,  1.8094e-03],
        [ 1.4859e-02, -4.9188e-03,  6.3572e-05],
        [ 3.9102e-04, -7.3268e-03, -2.2198e-03],
        [ 4.6172e-03, -4.8889e-03, -1.4238e-03],
        [ 1.3032e-02, -9.2970e-03, -4.5951e-03],
        [ 1.4309e-02, -5.6672e-03,  1.8414e-03],
        [ 2.0619e-02, -1.6274e-02,  3.0172e-03],
        [ 1.7238e-02,  8.6846e-03, -6.0547e-03],
        [ 5.1624e-03, -1.6151e-03, -1.0586e-02],
        [ 2.0796e-02,  2.9189e-03,  1.1081e-02],
        [ 2.1070e-02, -1.0404e-03, -7.5874e-03],
        [ 7.6393e-03,  9.4370e-04,  8.3293e-03],
        [-3.8162e-03, -6.3870e-03,  4.5048e-03],
        [ 1.6795e-02, -4.9671e-03, -5.1496e-03],
        [ 8.9450e-03,  7.9660e-03, -1.2579e-02],
        [ 1.6142e-02, -1.8284e-03, -9.2867e-04],
        [ 1.5921e-02, -5.5957e-03,  1.0480e-03],
        [ 2.1828e-03, -5.0070e-03, -1.5629e-03],
        [ 5.2313e-03,  1.2436e-04,  3.4423e-03],
        [ 1.5196e-02, -1.7872e-03,  1.8176e-03],
        [ 7.5007e-03,  1.1484e-02,  3.7849e-03],
        [ 8.3525e-03, -3.9690e-03, -3.1901e-03],
        [ 8.0921e-04, -8.1668e-03, -1.8075e-02],
        [-3.6611e-03, -9.5193e-03,  1.1810e-02],
        [ 8.7172e-03, -2.2760e-03, -3.7482e-03],
        [ 1.0939e-02, -3.6208e-03, -4.3338e-03],
        [-1.7198e-03, -6.2805e-03, -6.8278e-03],
        [ 1.3270e-02, -9.7601e-03, -4.6630e-03],
        [ 1.2154e-02, -6.1940e-03,  3.7770e-03],
        [ 1.4249e-02, -1.1865e-03, -5.9971e-04],
        [ 1.4570e-02,  6.2577e-03,  1.1054e-03],
        [ 8.4133e-03, -5.1412e-03,  4.6103e-03],
        [ 1.0018e-02, -9.7460e-03,  1.0238e-02],
        [ 1.3921e-02, -1.1663e-02, -6.1141e-04],
        [ 1.2713e-02, -4.0047e-03,  2.6160e-03],
        [-7.7202e-03, -1.3996e-02,  1.8421e-03],
        [ 1.2963e-02, -1.0330e-02, -1.6630e-03],
        [ 8.8790e-03, -2.3230e-03,  1.7859e-03]])
task_pred: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 0, 0, 0, 0,
        0, 0, 1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,
        0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2,
        2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0,
        0, 0, 2, 0, 0, 2, 0, 0]), task_id: tensor([0, 2, 1, 0, 1, 1, 0, 0, 0, 2, 2, 1, 1, 2, 1, 1, 0, 0, 2, 2, 2, 2, 2, 0,
        0, 1, 2, 1, 2, 0, 2, 0, 2, 1, 0, 1, 1, 1, 0, 1, 0, 2, 0, 2, 0, 1, 0, 1,
        1, 2, 2, 0, 1, 0, 1, 1, 1, 0, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 1, 0, 0, 1,
        0, 2, 2, 2, 1, 0, 1, 2, 0, 0, 1, 2, 2, 0, 1, 0, 1, 2, 1, 2, 1, 1, 2, 1,
        1, 1, 0, 2, 1, 2, 0, 1, 0, 1, 0, 1, 2, 0, 0, 2, 1, 2, 0, 1, 0, 2, 1, 0,
        0, 1, 1, 2, 1, 1, 0, 1])
[correct] no_actions is False
class_weights:  [1.0833334 0.8666667 1.0833334]
task_preds:  tensor([[ 0.0172,  0.0095,  0.0005],
        [ 0.0139, -0.0045,  0.0018]])
task_pred: tensor([0, 0]), task_id: tensor([0, 1])
==== [Test Summary] ====
rtg loss: 0.0002
Task loss: 1.1050
Task accuracy: 0.3436
- Task 0: 0.8500 (102/120)
- Task 1: 0.0133 (2/150)
- Task 2: 0.2500 (30/120)
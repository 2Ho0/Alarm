Class Weights: [1.08333333 0.86666667 1.08333333]
✅ Will update: action_embedding.0.weight
✅ Will update: time_embedding.weight
✅ Will update: state_embedding.weight
✅ Will update: transformer.pos_embed.W_pos
✅ Will update: transformer.blocks.0.attn.W_Q
✅ Will update: transformer.blocks.0.attn.W_K
✅ Will update: transformer.blocks.0.attn.W_V
✅ Will update: transformer.blocks.0.attn.W_O
✅ Will update: transformer.blocks.0.attn.b_Q
✅ Will update: transformer.blocks.0.attn.b_K
✅ Will update: transformer.blocks.0.attn.b_V
✅ Will update: transformer.blocks.0.attn.b_O
✅ Will update: transformer.blocks.0.mlp.W_in
✅ Will update: transformer.blocks.0.mlp.b_in
✅ Will update: transformer.blocks.0.mlp.W_out
✅ Will update: transformer.blocks.0.mlp.b_out
✅ Will update: transformer.blocks.1.attn.W_Q
✅ Will update: transformer.blocks.1.attn.W_K
✅ Will update: transformer.blocks.1.attn.W_V
✅ Will update: transformer.blocks.1.attn.W_O
✅ Will update: transformer.blocks.1.attn.b_Q
✅ Will update: transformer.blocks.1.attn.b_K
✅ Will update: transformer.blocks.1.attn.b_V
✅ Will update: transformer.blocks.1.attn.b_O
✅ Will update: transformer.blocks.1.mlp.W_in
✅ Will update: transformer.blocks.1.mlp.b_in
✅ Will update: transformer.blocks.1.mlp.W_out
✅ Will update: transformer.blocks.1.mlp.b_out
✅ Will update: action_predictor.weight
✅ Will update: action_predictor.bias
✅ Will update: state_predictor.weight
✅ Will update: state_predictor.bias
✅ Will update: reward_embedding.0.weight
✅ Will update: reward_predictor.weight
✅ Will update: reward_predictor.bias
✅ Will update: penultimate_layer.0.weight
✅ Will update: penultimate_layer.0.bias
✅ Will update: output_layer.0.weight
✅ Will update: output_layer.0.bias
===== 태스크별 데이터셋 크기 =====
Task task_0: 133 샘플
Task task_1: 165 샘플
Task task_2: 131 샘플
===== 원본 데이터셋 태스크별 분포 =====
전체 데이터 수: 429
Task 0: 133 샘플 (31.00%)
Task 1: 165 샘플 (38.46%)
Task 2: 131 샘플 (30.54%)
===== 학습/테스트 데이터 분할 (태스크 균형 유지) =====
학습 데이터: 299 샘플
테스트 데이터: 130 샘플
===== 첫 배치에서의 태스크 분포 확인 =====
Task 0: 42 샘플 (32.81%)
Task 1: 44 샘플 (34.38%)
Task 2: 42 샘플 (32.81%)
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
Training DT: 0.0001:   0%|          | 0/1 [00:01<?, ?it/s]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
Training DT: 0.0001:   0%|          | 0/1 [00:02<?, ?it/s]/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning:
[33mWARN: Overwriting existing videos at /home/hail/DT/src/videos/dt_eval_videos_0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)
Evaluating DT:   0%|          | 0/10 [00:00<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [42, 42, 42, 42, 42, 42, 42, 42] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:02<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [90, 90, 90, 90, 90, 90, 90, 90] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:05<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [137, 137, 137, 137, 137, 137, 137, 137] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:07<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [183, 183, 183, 183, 183, 183, 183, 183] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:09<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [200, 200, 200, 200, 200, 200, 200, 200] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:10<?, ?it/s]
Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [8, 8, 8, 8, 8, 8, 8, 8] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|█         | 1/10 [00:11<01:31, 10.14s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [67, 67, 67, 67, 67, 67, 67, 67] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|█         | 1/10 [00:13<01:31, 10.14s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [135, 135, 135, 135, 135, 135, 135, 135] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|█         | 1/10 [00:15<01:31, 10.14s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [185, 185, 185, 185, 185, 185, 185, 185] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|█         | 1/10 [00:16<01:31, 10.14s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
🔒 Freezing all layers except MLP (penultimate_layer, output_layer)
❌ action_embedding.0.weight is frozen.
❌ time_embedding.weight is frozen.
❌ state_embedding.weight is frozen.
❌ transformer.pos_embed.W_pos is frozen.
❌ transformer.blocks.0.attn.W_Q is frozen.
❌ transformer.blocks.0.attn.W_K is frozen.
❌ transformer.blocks.0.attn.W_V is frozen.
❌ transformer.blocks.0.attn.W_O is frozen.
❌ transformer.blocks.0.attn.b_Q is frozen.
❌ transformer.blocks.0.attn.b_K is frozen.
❌ transformer.blocks.0.attn.b_V is frozen.
❌ transformer.blocks.0.attn.b_O is frozen.
❌ transformer.blocks.0.mlp.W_in is frozen.
❌ transformer.blocks.0.mlp.b_in is frozen.
❌ transformer.blocks.0.mlp.W_out is frozen.
❌ transformer.blocks.0.mlp.b_out is frozen.
❌ transformer.blocks.1.attn.W_Q is frozen.
❌ transformer.blocks.1.attn.W_K is frozen.
❌ transformer.blocks.1.attn.W_V is frozen.
❌ transformer.blocks.1.attn.W_O is frozen.
❌ transformer.blocks.1.attn.b_Q is frozen.
❌ transformer.blocks.1.attn.b_K is frozen.
❌ transformer.blocks.1.attn.b_V is frozen.
❌ transformer.blocks.1.attn.b_O is frozen.
❌ transformer.blocks.1.mlp.W_in is frozen.
❌ transformer.blocks.1.mlp.b_in is frozen.
❌ transformer.blocks.1.mlp.W_out is frozen.
❌ transformer.blocks.1.mlp.b_out is frozen.
❌ action_predictor.weight is frozen.
❌ action_predictor.bias is frozen.
❌ state_predictor.weight is frozen.
❌ state_predictor.bias is frozen.
❌ reward_embedding.0.weight is frozen.
❌ reward_predictor.weight is frozen.
❌ reward_predictor.bias is frozen.
✅ penultimate_layer.0.weight will be updated.
✅ penultimate_layer.0.bias will be updated.
✅ output_layer.0.weight will be updated.
✅ output_layer.0.bias will be updated.
Training DT: 0.0001: 100%|██████████| 1/1 [00:20<00:00, 20.09s/it]t timestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|████████  | 8/10 [00:17<00:03,  1.88s/it]
MLP Fine-Tuning:   0%|          | 0/15 [00:00<?, ?it/s]
task_labels:  tensor([1, 2, 1, 2, 1, 0, 0, 2, 0, 2, 2, 2, 1, 2, 2, 0, 1, 0, 0, 0, 2, 1, 0, 2,
        1, 2, 2, 2, 0, 2, 2, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 2, 0, 1, 0, 2, 1, 0, 0, 0, 0, 2, 2, 0, 2, 1, 1, 1, 0, 1, 2, 2, 0,
        1, 1, 2, 2, 0, 1, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 1, 1, 1, 2, 2, 0,
        1, 1, 0, 2, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 2, 1, 0, 0, 1, 2]) , task_preds: tensor([[ 5.5303e-03, -3.5749e-02, -5.7275e-03],
        [ 1.7404e-02, -2.5734e-02, -3.5371e-02],
        [ 3.3737e-02, -2.9735e-02, -4.2474e-02],
        [ 6.3778e-03, -1.4742e-02, -1.3632e-02],
        [ 1.1420e-02, -1.9326e-02, -1.0679e-02],
        [ 1.1973e-02, -6.8404e-03, -1.7082e-02],
        [ 3.7639e-02,  3.1298e-02,  2.4087e-02],
        [-2.7501e-02,  1.6320e-02,  1.4383e-02],
        [ 2.3528e-02, -1.4970e-02, -1.7487e-03],
        [ 2.7515e-02, -3.7330e-02, -5.0238e-03],
        [ 1.4736e-02,  2.3998e-03,  1.0965e-02],
        [ 6.0137e-02, -3.3125e-02, -4.1339e-02],
        [-8.5914e-03, -3.0738e-02, -2.8231e-02],
        [ 2.8076e-02, -9.4591e-03, -3.3670e-02],
        [-3.3993e-03, -4.6279e-03,  3.7482e-02],
        [-8.2959e-03, -1.0563e-02, -1.1501e-02],
        [ 4.5069e-02, -1.2942e-02, -1.7408e-03],
        [-2.2097e-02, -3.0440e-02, -1.2870e-02],
        [-7.3962e-03, -2.1484e-02, -3.2831e-02],
        [-1.6646e-02, -1.8768e-02, -2.1050e-03],
        [ 1.1819e-02, -3.4206e-02,  1.1156e-04],
        [ 3.7624e-03, -2.4472e-02, -6.9150e-03],
        [ 1.8698e-02, -1.8581e-02, -4.6540e-02],
        [ 2.9233e-02,  1.2578e-02,  1.2477e-02],
        [ 7.3394e-03, -4.2097e-02, -8.7870e-03],
        [-4.0458e-03, -1.9588e-02, -2.7861e-02],
        [ 1.8433e-02,  1.0692e-02,  2.4781e-02],
        [ 4.1304e-03, -3.4557e-02,  3.1087e-03],
        [ 6.3026e-03, -4.3176e-02,  4.3806e-03],
        [ 5.1690e-03, -2.9509e-02, -4.5780e-03],
        [ 2.4861e-02, -3.3366e-02, -8.6598e-04],
        [ 4.5373e-03, -4.0635e-02, -2.0626e-02],
        [ 3.2758e-02,  7.0601e-03,  2.4530e-02],
        [ 1.3045e-03, -1.7391e-02, -5.6279e-02],
        [-4.3576e-02,  5.9967e-03,  2.3822e-02],
        [ 8.2973e-03,  1.3634e-02,  4.2631e-02],
        [ 2.9287e-02, -1.9354e-02, -3.2992e-02],
        [ 4.1130e-02, -4.1931e-02, -4.0914e-02],
        [ 1.3712e-02, -2.5216e-02, -1.0839e-02],
        [ 6.5344e-03, -3.7609e-02,  5.7801e-03],
        [ 1.7083e-03, -2.1409e-02, -1.6075e-02],
        [ 3.9500e-02, -4.7700e-02, -2.4065e-02],
        [ 1.5670e-02, -2.8531e-02, -5.1884e-02],
        [ 4.2440e-03, -3.8172e-02,  2.9854e-03],
        [ 2.1271e-02, -2.3440e-02, -1.8513e-02],
        [-1.0125e-02,  7.4934e-03,  2.7226e-02],
        [-5.0619e-03, -3.3357e-02, -1.1523e-02],
        [-3.2490e-03,  1.2293e-02,  2.5583e-02],
        [ 4.5404e-02, -4.2870e-02, -2.2082e-02],
        [ 1.9176e-02,  3.2921e-02,  2.5814e-03],
        [ 2.9357e-02, -1.1871e-03,  2.7260e-02],
        [ 4.0232e-02, -1.6147e-02, -3.1487e-02],
        [ 9.4774e-03, -3.0213e-02, -3.5126e-02],
        [-5.3135e-03, -3.2399e-02,  6.6024e-04],
        [ 4.1141e-02, -2.4150e-03, -1.9973e-02],
        [ 2.8388e-02,  1.2237e-02,  4.1899e-03],
        [ 1.9071e-02, -1.9382e-02,  1.9108e-02],
        [-5.2196e-04, -2.5105e-02, -9.6405e-03],
        [ 1.9338e-02,  5.7453e-04,  3.4600e-02],
        [ 1.9262e-02, -7.8439e-03, -3.9972e-02],
        [ 1.6252e-02, -2.7886e-02, -1.2272e-03],
        [-7.3360e-03, -1.5047e-02, -2.5642e-02],
        [-1.0728e-03, -3.6940e-02,  7.2334e-03],
        [ 5.8592e-03, -3.5696e-02, -2.0904e-02],
        [-2.8612e-02,  9.6481e-03, -2.9515e-02],
        [ 1.0663e-02,  1.3806e-02,  3.2244e-02],
        [ 3.4317e-03, -4.0403e-02, -5.4178e-03],
        [-2.3829e-03, -2.3093e-02, -2.5939e-02],
        [ 1.0880e-02, -4.1410e-02, -8.0425e-03],
        [-5.4443e-03, -1.5162e-02,  8.6692e-03],
        [-1.0360e-02,  4.1970e-04,  2.7505e-02],
        [ 4.2532e-03, -3.8172e-02, -2.2314e-02],
        [ 2.0825e-02, -1.3886e-02, -1.3966e-02],
        [ 2.5247e-03,  2.2155e-02,  1.9216e-02],
        [ 2.1688e-02, -2.8006e-03, -2.4957e-02],
        [-1.8198e-02, -4.2335e-03, -5.7342e-03],
        [ 1.1744e-02, -4.3633e-02, -6.4544e-04],
        [ 1.5956e-03, -3.7773e-02,  3.7924e-03],
        [ 6.3869e-03, -2.0085e-02,  2.1370e-02],
        [ 1.0706e-02, -5.0501e-04, -2.3025e-02],
        [ 3.0720e-02, -2.3060e-02, -2.8227e-02],
        [ 3.9821e-02, -2.9975e-02, -5.9172e-02],
        [ 4.0237e-02, -2.0663e-02, -1.9080e-02],
        [ 4.6629e-03, -4.0084e-02, -1.3498e-02],
        [-6.6104e-03, -2.2709e-02, -2.5847e-02],
        [ 5.2661e-02, -1.0673e-03, -3.9482e-02],
        [-3.7357e-04, -3.5998e-02, -6.6604e-03],
        [ 2.6192e-02, -2.8637e-02, -3.7339e-02],
        [-2.1241e-02,  1.7037e-02,  6.3614e-02],
        [ 1.0032e-02, -3.9134e-02, -8.6527e-03],
        [ 1.6199e-02, -2.5816e-02, -6.8341e-03],
        [ 3.5725e-02, -3.6129e-02, -6.5104e-02],
        [ 2.0814e-02,  3.3684e-03,  2.1805e-02],
        [-6.6393e-03, -2.3889e-02,  1.7369e-03],
        [-1.0788e-03,  2.7154e-02,  4.3354e-02],
        [ 1.0884e-02, -7.8455e-05,  2.1985e-02],
        [ 1.2917e-03, -2.8214e-02, -1.1560e-02],
        [ 1.7281e-02, -1.1387e-02,  2.5053e-02],
        [-2.0834e-02, -1.7177e-02, -2.1753e-02],
        [ 4.1742e-03, -3.1962e-02, -9.8041e-03],
        [ 3.8647e-02, -1.2189e-02, -2.0687e-02],
        [-4.0856e-03, -2.3136e-02, -1.4227e-02],
        [ 1.8024e-02, -2.2457e-02,  7.5451e-03],
        [-5.6905e-04, -1.1565e-02, -4.1922e-02],
        [ 2.4885e-02, -3.9725e-02, -2.5836e-02],
        [-2.0563e-02, -1.3217e-02,  1.0962e-02],
        [ 8.4739e-03, -2.9095e-02, -9.1246e-03],
        [ 1.6357e-02,  2.6704e-03, -3.8880e-02],
        [ 1.0279e-02, -2.5743e-02, -1.5975e-02],
        [ 9.4842e-03, -3.8992e-02, -7.8446e-03],
        [ 2.0518e-02,  2.2034e-02, -3.2524e-03],
        [ 1.2809e-02, -3.9735e-02, -1.4257e-02],
        [ 5.3174e-03, -3.8595e-02, -5.2891e-03],
        [ 3.0948e-02, -5.1780e-03,  2.5982e-03],
        [ 3.5858e-02, -1.6854e-02, -5.5234e-02],
        [ 4.1304e-03, -3.4557e-02,  3.1087e-03],
        [ 5.4663e-02, -5.1852e-03, -4.8600e-02],
        [ 3.9960e-03, -4.6927e-02, -4.5758e-02],
        [ 1.4449e-02, -2.7463e-02, -3.3350e-02],
        [-1.9253e-02,  1.9550e-02,  1.4581e-02],
        [ 6.7325e-03, -1.9068e-02, -3.8332e-02],
        [ 8.7614e-03,  1.1126e-02, -4.3785e-02],
        [ 1.4226e-02, -2.4033e-02, -1.3265e-02],
        [ 1.2031e-02, -3.5014e-02, -2.1081e-03],
        [ 2.6792e-02,  5.6387e-03, -7.8533e-03],
        [ 2.1654e-02, -9.7812e-03, -5.1468e-02],
        [ 9.2021e-03, -1.9013e-02, -2.7578e-02],
        [-7.1048e-03,  1.6295e-02, -2.6382e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]


MLP Fine-Tuning: task_loss = 1.0931:  13%|█▎        | 2/15 [00:51<04:38, 21.38s/it]
task_labels:  tensor([1, 1, 2, 2, 2, 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 2, 2, 0, 0, 2, 0, 2, 2, 0,
        2, 2, 0, 0, 1, 0, 2, 0, 1, 1, 1, 0, 1, 2, 2, 2, 1, 1, 2, 0, 0, 1, 1, 2,
        0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 0, 1, 2, 2,
        2, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 2, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 2, 1, 2, 2, 2, 1, 1, 0, 0, 0, 2, 1, 2, 0, 1, 1, 1, 0, 2, 0, 0, 2,
        2, 1, 2, 1, 1, 0, 1, 2]) , task_preds: tensor([[ 0.0701, -0.0171, -0.0872],
        [ 0.0228,  0.0005,  0.0164],
        [ 0.0487, -0.0374, -0.0533],
        [ 0.0643, -0.0332, -0.0102],
        [ 0.0698, -0.0477, -0.0813],
        [ 0.0455, -0.0361, -0.0557],
        [-0.0421,  0.0191,  0.0131],
        [ 0.0496, -0.0262, -0.0350],
        [ 0.0463, -0.0426, -0.0518],
        [ 0.0418, -0.0330, -0.0541],
        [ 0.0460, -0.0349, -0.0691],
        [ 0.0605, -0.0276, -0.0703],
        [ 0.0611, -0.0301, -0.0592],
        [ 0.0541, -0.0371, -0.0013],
        [ 0.0383, -0.0392, -0.0435],
        [ 0.0425, -0.0101, -0.0705],
        [ 0.0290, -0.0335, -0.0420],
        [ 0.0389, -0.0093, -0.0356],
        [ 0.0409, -0.0027,  0.0022],
        [ 0.0661, -0.0403, -0.0615],
        [ 0.0537, -0.0316, -0.0550],
        [ 0.0715,  0.0007, -0.0247],
        [ 0.0444, -0.0366, -0.0515],
        [ 0.0582, -0.0497, -0.0873],
        [ 0.0584, -0.0079, -0.0213],
        [ 0.0612, -0.0200, -0.0535],
        [ 0.0667, -0.0453, -0.0843],
        [ 0.0402, -0.0233, -0.0655],
        [ 0.0432, -0.0386, -0.0556],
        [ 0.0522, -0.0442, -0.0660],
        [ 0.0233, -0.0049, -0.0095],
        [ 0.0548, -0.0260, -0.0796],
        [ 0.0645, -0.0308, -0.0131],
        [ 0.0570, -0.0393, -0.0723],
        [ 0.0447, -0.0361, -0.0610],
        [ 0.0399, -0.0321, -0.0676],
        [ 0.0547, -0.0281, -0.0830],
        [ 0.0462, -0.0426, -0.0678],
        [ 0.0472, -0.0351, -0.0490],
        [ 0.0474, -0.0372, -0.0486],
        [ 0.0653, -0.0602, -0.1115],
        [ 0.0618, -0.0128, -0.0775],
        [ 0.0535, -0.0391, -0.0639],
        [ 0.0236, -0.0247,  0.0319],
        [ 0.0529, -0.0403, -0.0549],
        [ 0.0171, -0.0316,  0.0163],
        [ 0.0750, -0.0196, -0.0709],
        [ 0.0616, -0.0308, -0.0597],
        [ 0.0427, -0.0311, -0.0600],
        [ 0.0388, -0.0326, -0.0579],
        [ 0.0675, -0.0285, -0.0856],
        [ 0.0391, -0.0172,  0.0390],
        [ 0.0448, -0.0353, -0.0472],
        [ 0.0594, -0.0145, -0.0469],
        [ 0.0500, -0.0408, -0.0576],
        [ 0.0481, -0.0329, -0.0693],
        [ 0.0716, -0.0140, -0.0688],
        [ 0.0603, -0.0316, -0.0535],
        [ 0.0619, -0.0295, -0.0359],
        [ 0.0388, -0.0279, -0.0473],
        [ 0.0504, -0.0352, -0.0554],
        [ 0.0509, -0.0269, -0.0715],
        [-0.0011, -0.0193, -0.0263],
        [ 0.0386, -0.0394, -0.0582],
        [ 0.0437, -0.0249, -0.0473],
        [ 0.0648, -0.0328, -0.0148],
        [ 0.0670, -0.0114, -0.0537],
        [ 0.0526, -0.0479, -0.0530],
        [ 0.0456, -0.0209, -0.0499],
        [ 0.0534, -0.0368, -0.0566],
        [ 0.0465, -0.0416, -0.0501],
        [ 0.0003, -0.0097, -0.0102],
        [ 0.0550,  0.0312, -0.0280],
        [ 0.0674, -0.0501, -0.0626],
        [ 0.0512, -0.0352, -0.0552],
        [ 0.0407, -0.0283, -0.0709],
        [ 0.0393, -0.0076,  0.0114],
        [ 0.0413, -0.0429, -0.0468],
        [ 0.0697, -0.0298, -0.0734],
        [ 0.0096, -0.0040,  0.0105],
        [ 0.0483, -0.0399, -0.0643],
        [ 0.0649, -0.0309, -0.0269],
        [ 0.0565, -0.0341, -0.0776],
        [ 0.0297,  0.0113, -0.0137],
        [ 0.0727,  0.0044, -0.0048],
        [ 0.0448, -0.0353, -0.0472],
        [ 0.0566, -0.0285, -0.0542],
        [ 0.0473, -0.0260, -0.0271],
        [ 0.0443, -0.0359, -0.0498],
        [ 0.0557, -0.0226, -0.0691],
        [ 0.0588, -0.0608, -0.0705],
        [ 0.0656, -0.0453, -0.1013],
        [ 0.0241, -0.0086, -0.0339],
        [ 0.0574, -0.0257, -0.0641],
        [ 0.0353, -0.0207, -0.0462],
        [ 0.0471, -0.0413, -0.0531],
        [ 0.0439, -0.0415, -0.0550],
        [ 0.0484, -0.0324,  0.0078],
        [ 0.0444, -0.0282, -0.0427],
        [ 0.0614, -0.0214, -0.0075],
        [ 0.0506, -0.0331, -0.0739],
        [ 0.0473, -0.0389, -0.0498],
        [ 0.0320, -0.0237, -0.0718],
        [ 0.0464, -0.0402, -0.0634],
        [ 0.0419, -0.0172, -0.0303],
        [ 0.0644, -0.0467, -0.0637],
        [ 0.0429, -0.0141, -0.0071],
        [ 0.0530, -0.0400, -0.0568],
        [ 0.0594, -0.0370, -0.0930],
        [ 0.0750, -0.0455, -0.0576],
        [ 0.0518, -0.0158,  0.0017],
        [ 0.0562,  0.0059, -0.0501],
        [ 0.0301,  0.0016, -0.0339],
        [ 0.0407, -0.0050,  0.0053],
        [ 0.0142, -0.0052,  0.0549],
        [ 0.0591, -0.0311, -0.0591],
        [ 0.0588, -0.0502, -0.0585],
        [ 0.0664, -0.0308, -0.0921],
        [ 0.0501, -0.0341, -0.0619],
        [ 0.0576, -0.0136,  0.0033],
        [ 0.0477, -0.0073, -0.0324],
        [ 0.0538, -0.0388, -0.0640],
        [ 0.0787, -0.0238, -0.0605],
        [ 0.0683, -0.0315, -0.0466],
        [ 0.0272, -0.0358,  0.0187],
        [ 0.0413, -0.0065, -0.0639],
        [ 0.0172, -0.0186,  0.0060],
        [ 0.0103,  0.0268,  0.0089]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000038
penultimate_layer.0.bias: grad mean = 0.001352
output_layer.0.weight: grad mean = 0.003850
output_layer.0.bias: grad mean = 0.039672
[correct] no_actions is False
task_labels:  tensor([1, 0, 0, 1, 2, 2, 2, 1, 0, 2, 0, 2, 1, 2, 2, 1, 0, 1, 0, 0, 1, 2, 1, 2,
        2, 1, 2, 0, 1, 0, 2, 2, 0, 0, 0, 2, 2, 1, 2, 0, 1, 1, 2, 2, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 0, 2, 1, 1, 0, 2, 1, 1, 1, 0, 0, 2, 2, 2,
        0, 0, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 0, 1, 1, 2, 1, 1, 1, 1, 0, 1, 2, 2,
        1, 1, 2, 2, 2, 1, 0, 2, 2, 1, 0, 2, 2, 0, 2, 1, 2, 1, 2, 0, 1, 1, 1, 1,
        1, 0, 2, 1, 2, 0, 1, 0]) , task_preds: tensor([[ 4.2361e-02, -1.0456e-02, -1.3183e-01],
        [ 4.7692e-02, -1.7766e-02, -8.7585e-02],
        [ 5.9183e-02, -1.1849e-02, -1.0590e-01],
        [ 2.9054e-02, -3.0661e-02,  1.2055e-02],
        [ 6.8262e-02, -3.1164e-02, -3.0299e-02],
        [ 6.4057e-02, -2.2745e-02, -9.5588e-02],
        [ 5.7138e-02,  3.1644e-02, -3.1610e-02],
        [ 6.6250e-02, -8.2925e-03, -8.4579e-02],
        [ 5.8429e-02, -1.5800e-02, -7.8174e-02],
        [ 6.6980e-02, -3.7823e-03, -1.0152e-01],
        [ 7.2300e-02,  4.5651e-03, -1.0338e-03],
        [ 4.9193e-02, -1.3780e-02, -9.8715e-02],
        [ 5.3300e-02, -1.3663e-02, -8.2838e-02],
        [ 5.8140e-02, -3.4911e-02, -1.1090e-01],
        [ 2.5099e-02, -8.6112e-03, -2.5383e-02],
        [ 4.5053e-02, -2.1085e-02, -8.5086e-02],
        [ 6.9168e-02, -1.7349e-02, -1.0621e-01],
        [ 4.6995e-02, -2.7650e-02, -8.7621e-02],
        [ 5.9225e-02, -3.0033e-02, -1.0297e-01],
        [ 6.8917e-02, -8.0366e-03,  2.1575e-03],
        [ 4.5378e-02, -1.8712e-02, -9.3723e-02],
        [ 2.3836e-02, -4.4342e-03, -1.1051e-02],
        [ 8.4282e-02, -2.9591e-02, -1.0486e-01],
        [ 3.4003e-02, -1.1364e-02, -2.9499e-02],
        [ 8.2276e-02, -2.7904e-02, -8.0880e-02],
        [ 1.7946e-02, -1.3614e-02, -2.2008e-03],
        [ 4.9324e-02, -2.2442e-02, -1.0267e-01],
        [ 4.1465e-02, -9.6835e-03, -8.9945e-02],
        [ 4.5805e-02, -1.9621e-02, -8.3272e-02],
        [ 6.1806e-02, -1.9619e-02, -8.8283e-02],
        [ 4.5314e-02, -2.3327e-03,  1.1268e-02],
        [ 4.4929e-02, -9.3183e-03, -8.4493e-02],
        [ 4.2278e-02, -1.2376e-02, -7.6175e-02],
        [ 1.3639e-02,  1.9397e-03, -2.7432e-02],
        [ 7.0320e-02, -1.3741e-02, -6.3303e-02],
        [ 5.2420e-02, -4.3900e-05, -9.4848e-02],
        [ 4.8619e-02, -2.1835e-02, -8.8243e-02],
        [ 4.3039e-02, -1.3410e-02, -7.7152e-02],
        [ 5.1536e-02, -1.5151e-02, -8.6023e-02],
        [ 4.5393e-02, -2.2209e-02, -7.5824e-02],
        [ 4.0618e-02, -1.2640e-02, -7.6782e-02],
        [ 4.1572e-02, -2.7438e-02, -1.0874e-01],
        [ 4.9120e-02, -2.9450e-02, -9.0784e-02],
        [ 4.6205e-02, -1.1484e-02, -9.3165e-02],
        [ 4.3187e-02, -2.1707e-02, -8.8683e-02],
        [-3.3553e-02,  1.7293e-02,  5.0615e-03],
        [ 7.6588e-02, -2.4352e-02, -7.2732e-02],
        [ 8.0450e-02, -3.4453e-02, -1.0450e-01],
        [ 7.6277e-02, -1.5698e-02, -6.1848e-02],
        [ 4.3039e-02, -1.3410e-02, -7.7152e-02],
        [ 4.2775e-02, -1.6712e-02, -7.5575e-02],
        [ 4.0877e-02, -1.9783e-02, -8.5369e-02],
        [ 3.1743e-02,  1.3346e-02, -1.9406e-02],
        [ 6.3893e-02, -9.6460e-03, -9.2036e-02],
        [ 4.4400e-02,  3.1765e-03, -8.9423e-02],
        [ 6.3313e-02, -2.3463e-02, -9.6670e-02],
        [ 3.3541e-02, -1.5620e-02, -9.9476e-02],
        [ 6.3330e-02, -3.4662e-02, -7.0166e-02],
        [ 6.2288e-02, -3.3185e-02, -1.1654e-01],
        [ 6.3034e-02, -3.3789e-02, -1.0368e-01],
        [ 4.4943e-02, -1.5933e-02, -9.0851e-02],
        [ 6.2938e-02, -2.7021e-02, -1.2242e-01],
        [ 5.4958e-02, -1.5839e-02, -9.0194e-02],
        [ 3.3891e-02, -1.8095e-02, -7.4152e-02],
        [ 4.3731e-02, -8.7554e-03, -9.2499e-02],
        [ 3.4709e-02, -2.5900e-02, -1.0155e-01],
        [ 4.3691e-02, -1.3770e-02, -1.0765e-01],
        [ 1.3859e-02,  1.2444e-03,  1.8634e-02],
        [ 3.9110e-02,  5.3625e-04, -8.7520e-02],
        [ 6.6464e-02,  1.9440e-03, -7.8465e-02],
        [ 5.3240e-02, -1.6922e-02, -7.7527e-02],
        [ 3.5318e-02,  1.0832e-02,  7.5193e-03],
        [ 1.6915e-02, -4.8810e-03,  2.5921e-03],
        [ 6.2756e-02,  1.8453e-02, -6.9951e-02],
        [ 6.5519e-02, -3.6771e-02, -9.0416e-02],
        [ 3.2988e-02, -8.0903e-03, -1.0756e-01],
        [ 6.1828e-02, -1.0448e-02, -9.4346e-02],
        [ 7.6378e-02, -8.6303e-03, -7.4525e-02],
        [ 4.1673e-02, -1.8983e-02, -8.7220e-02],
        [ 6.5346e-02, -1.1410e-02, -5.0712e-02],
        [ 4.2246e-02, -1.3879e-02, -9.7646e-02],
        [ 7.1570e-02,  1.1954e-03, -8.4553e-02],
        [ 8.0702e-02, -3.1898e-02, -5.4752e-02],
        [ 7.7430e-02, -4.0205e-02, -9.0413e-02],
        [ 6.2829e-02, -1.9995e-02, -1.4615e-02],
        [ 4.2630e-02, -1.0117e-02, -9.9950e-02],
        [ 4.1785e-02, -1.8404e-02, -8.2900e-02],
        [ 5.1412e-02, -6.5830e-03, -8.2415e-02],
        [ 4.4888e-02, -1.5075e-02, -9.3141e-02],
        [ 4.3039e-02, -1.3410e-02, -7.7152e-02],
        [ 4.5690e-02, -1.3116e-02, -1.2477e-01],
        [ 5.6706e-02, -8.9485e-03, -2.2126e-02],
        [ 5.3246e-02, -1.9912e-02, -7.7088e-02],
        [ 5.5287e-02,  1.7291e-03, -1.3228e-02],
        [ 1.4361e-02, -2.6447e-02, -6.5044e-02],
        [ 4.3042e-02, -1.3394e-02, -7.7011e-02],
        [ 6.2411e-02, -3.4351e-02,  1.2888e-02],
        [ 7.6482e-02, -7.8963e-03, -1.0869e-01],
        [-6.6670e-03,  1.4811e-02, -3.8050e-02],
        [ 5.2140e-02, -2.4470e-02, -5.5093e-02],
        [ 3.3577e-03, -1.9457e-02, -1.1431e-02],
        [ 8.7812e-03, -1.1746e-02, -2.8671e-02],
        [ 4.8566e-02, -2.3981e-02, -8.4710e-02],
        [ 6.9736e-02, -1.8482e-02, -3.6720e-02],
        [ 6.4730e-02, -9.2763e-03, -8.0812e-02],
        [ 7.9417e-02, -9.6863e-03, -1.2052e-01],
        [ 9.3716e-02, -8.6713e-03, -9.2274e-02],
        [ 7.3227e-02, -1.7656e-02, -2.7813e-02],
        [ 6.3803e-02, -1.0068e-02, -7.1365e-02],
        [ 5.8535e-02, -1.7935e-02, -8.6833e-02],
        [ 3.4475e-02, -2.1143e-02, -7.3858e-02],
        [ 4.4162e-02, -2.5853e-02, -8.7534e-02],
        [ 7.0561e-02, -3.5064e-02, -1.0084e-01],
        [ 4.5722e-02, -1.3494e-02, -8.8928e-02],
        [ 6.5622e-02, -1.0582e-02, -2.3368e-02],
        [ 6.2203e-02,  1.9712e-03, -9.5312e-02],
        [ 7.0510e-02, -4.8874e-03, -3.8398e-02],
        [ 1.9836e-02, -2.8895e-02,  8.9958e-03],
        [ 3.8816e-02, -1.5139e-02, -9.5440e-02],
        [ 5.6778e-02, -1.4117e-02, -8.9391e-02],
        [ 7.1609e-02,  1.8802e-03, -2.7539e-02],
        [ 6.9430e-02, -3.7297e-02, -8.3857e-02],
        [ 6.0397e-02, -2.1376e-03, -1.7946e-02],
        [ 3.9540e-02, -2.7072e-02, -9.1117e-02],
        [ 5.1981e-02, -2.2163e-02, -9.1520e-02],
        [ 8.2895e-02, -3.2432e-02, -1.0987e-01],
        [ 4.0714e-02, -5.0160e-03, -8.7153e-02],
        [ 6.3831e-02, -2.5594e-04, -7.3371e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000027
penultimate_layer.0.bias: grad mean = 0.000846
output_layer.0.weight: grad mean = 0.002734
output_layer.0.bias: grad mean = 0.024926
[correct] no_actions is False
task_labels:  tensor([0, 0, 1, 0, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 0, 0, 1, 1, 0, 2, 0,
        0, 1, 0, 1, 1, 1, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 1, 1, 0, 1, 1, 2,
        2, 0, 1, 1, 2, 1, 2, 2, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 2, 0, 0, 0, 2, 0,
        1, 0, 1, 2, 0, 1, 0, 0, 1, 2, 0, 1, 0, 2, 2, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        2, 1, 2, 0, 2, 0, 1, 0, 2, 1, 0, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 0, 1,
        1, 2, 1, 2, 2, 1, 0, 2]) , task_preds: tensor([[ 6.1615e-02, -1.9760e-02, -9.0703e-02],
        [ 2.7944e-02, -1.8731e-03, -8.6660e-02],
        [ 4.1048e-02,  2.1107e-03, -9.0349e-02],
        [ 2.4733e-02,  6.1736e-03, -7.9219e-02],
        [ 6.2034e-02, -5.7362e-03, -8.0388e-02],
        [ 6.5011e-02, -1.0704e-02, -2.0466e-02],
        [ 2.0578e-02,  1.6551e-02,  5.0611e-03],
        [ 7.2078e-02, -3.0350e-02, -9.6899e-02],
        [ 3.5626e-02, -1.6179e-02,  1.6989e-02],
        [ 6.8648e-02,  1.5705e-03,  3.9782e-03],
        [ 2.3463e-02,  4.5997e-03, -7.9351e-02],
        [ 2.5960e-02,  9.8844e-03, -6.5643e-03],
        [ 1.9932e-02,  3.1290e-03, -8.1714e-02],
        [ 2.7435e-02, -1.5716e-03, -8.4617e-02],
        [ 4.8516e-02, -1.5209e-03, -5.6536e-02],
        [ 1.4773e-02,  5.9990e-03, -1.1779e-01],
        [ 6.5958e-02,  1.6185e-03, -7.9512e-02],
        [ 2.3814e-02,  2.3719e-04, -8.0100e-02],
        [ 3.2307e-02, -4.4086e-03,  3.9612e-02],
        [ 2.6159e-02, -3.2148e-02, -5.5136e-02],
        [ 4.2592e-02,  1.4579e-02, -2.0087e-02],
        [ 3.7130e-02, -1.0327e-02, -8.6442e-02],
        [ 4.0232e-02, -5.0012e-02, -1.0211e-01],
        [ 3.9636e-02, -8.8196e-03, -8.6929e-02],
        [ 4.5927e-02,  1.6607e-02, -8.7722e-02],
        [ 1.8884e-02, -4.4334e-02, -1.7576e-03],
        [ 1.0182e-02, -4.3585e-02, -4.9491e-02],
        [ 5.6969e-02, -2.7214e-02, -8.3490e-02],
        [ 4.9424e-02, -2.4301e-02, -9.5201e-02],
        [ 2.0278e-02, -3.1428e-02,  1.9439e-02],
        [ 3.0049e-02,  7.6747e-03, -8.6163e-02],
        [ 4.7376e-03, -9.3434e-04,  5.8138e-02],
        [ 3.0598e-02,  5.7407e-03, -9.0113e-02],
        [ 2.6664e-02, -4.7484e-03, -8.7252e-02],
        [ 4.5642e-02, -4.2513e-02,  7.9477e-03],
        [ 4.7107e-02, -8.7157e-03, -6.1584e-02],
        [ 3.9767e-02,  2.2404e-03, -1.0043e-01],
        [ 1.8506e-02,  9.9511e-03, -1.1895e-01],
        [ 2.8629e-02, -4.3741e-02,  1.8718e-03],
        [ 5.6523e-02,  2.9174e-03, -8.8963e-02],
        [ 2.8430e-02,  1.1309e-03, -7.9953e-02],
        [ 3.1766e-02, -9.2544e-03, -9.3402e-02],
        [ 3.2660e-02,  4.7913e-03, -1.0392e-01],
        [ 2.5914e-02,  3.7013e-03, -8.5579e-02],
        [ 1.6228e-02, -1.4734e-02,  5.3155e-03],
        [ 2.6708e-02, -1.2129e-02, -8.7282e-02],
        [ 4.9635e-02, -3.1707e-02,  1.7508e-02],
        [ 2.8315e-02, -2.7815e-02, -4.3007e-02],
        [ 3.9685e-02, -2.0992e-02, -9.0163e-02],
        [ 4.4470e-02,  1.7448e-03, -1.0105e-01],
        [ 2.3463e-02,  4.5997e-03, -7.9351e-02],
        [ 2.8738e-02,  5.7323e-03, -9.3053e-02],
        [ 2.3203e-02, -2.9271e-02, -3.8903e-03],
        [ 1.2975e-02, -1.7795e-02,  3.0439e-02],
        [ 4.3445e-02, -1.7462e-02, -1.0942e-01],
        [ 4.3110e-02, -4.1821e-03, -1.0456e-01],
        [ 4.1102e-02, -1.1013e-02, -6.4107e-02],
        [ 5.4696e-02,  8.7778e-03, -4.2904e-03],
        [ 4.0144e-02, -4.4165e-02, -9.1895e-02],
        [ 6.2907e-02, -2.4383e-02, -4.5220e-02],
        [ 3.9563e-02, -1.5755e-02, -8.5441e-02],
        [ 3.0085e-02,  7.9347e-03, -7.2246e-02],
        [ 6.7699e-02, -1.6815e-02, -1.0042e-01],
        [ 2.9602e-02,  1.2996e-03,  2.7337e-02],
        [ 4.9383e-02,  5.1024e-03, -7.6905e-02],
        [ 4.3667e-02, -1.3337e-02, -5.6655e-03],
        [ 5.9025e-02, -3.7782e-02,  2.0035e-02],
        [ 5.2663e-02, -4.0141e-02,  3.1146e-03],
        [ 5.8971e-02,  7.9260e-03, -5.6742e-02],
        [ 6.2292e-02,  3.5258e-04, -5.7570e-02],
        [ 4.9572e-02, -4.2859e-03, -7.2853e-02],
        [ 4.3437e-02, -1.3917e-02, -7.6845e-02],
        [ 1.8824e-02, -7.3672e-03, -2.3950e-02],
        [ 4.4765e-02, -2.7098e-02, -6.9985e-02],
        [ 6.6162e-02,  3.1549e-03, -7.3328e-02],
        [ 3.0188e-02, -8.8593e-03, -9.5440e-02],
        [ 2.8977e-02,  5.1982e-03, -9.2942e-02],
        [ 6.0505e-02, -1.9080e-02, -9.5608e-02],
        [ 2.9667e-02,  8.4404e-03, -8.6996e-02],
        [ 6.7960e-02,  4.4378e-04, -6.1610e-02],
        [ 5.6629e-02, -3.1940e-02, -1.9255e-03],
        [ 3.5984e-02, -1.1267e-03, -7.1060e-02],
        [ 5.5361e-02, -2.1511e-03, -1.2068e-01],
        [ 3.4200e-02, -2.3575e-02,  2.2207e-02],
        [-2.3181e-03,  1.7009e-02,  5.4608e-03],
        [ 4.9523e-02, -7.4569e-03, -9.3835e-02],
        [ 2.3095e-02,  2.4817e-03, -8.9045e-02],
        [ 4.2218e-02,  6.5833e-03, -1.0391e-01],
        [ 2.4483e-02, -1.2553e-02, -8.8397e-02],
        [ 3.0523e-02, -1.1544e-02, -3.7566e-02],
        [ 3.0650e-02,  6.8766e-03, -9.6483e-02],
        [ 1.7205e-02, -4.5827e-03, -1.1553e-01],
        [ 2.8982e-02, -1.0867e-02, -8.9385e-02],
        [ 6.5982e-02, -3.1737e-02, -1.2229e-01],
        [ 2.3459e-02, -7.8302e-04, -9.0047e-02],
        [ 5.4916e-02, -3.7396e-02,  2.7039e-02],
        [-5.2671e-03, -1.9515e-03,  1.4045e-02],
        [ 5.9486e-02, -1.8330e-02, -9.1994e-02],
        [ 3.7565e-02,  3.2683e-02, -1.0126e-01],
        [ 6.2215e-02,  1.2177e-04, -1.0345e-01],
        [ 4.8047e-02,  1.2446e-04, -2.6511e-02],
        [ 2.7353e-02, -1.2787e-02, -7.1903e-02],
        [ 2.2228e-02,  3.5505e-03, -8.0596e-02],
        [ 2.9015e-03, -1.1253e-02,  3.6398e-03],
        [ 2.4186e-02,  3.7419e-03, -4.3029e-02],
        [ 5.5694e-02, -4.1453e-02, -1.2739e-01],
        [ 5.7937e-02, -1.3422e-02, -8.0384e-02],
        [ 3.1675e-02, -5.4398e-03,  3.7359e-02],
        [ 4.3610e-02,  3.6006e-04, -9.9236e-02],
        [ 1.1351e-02, -1.8156e-02,  2.8311e-02],
        [ 4.8436e-02, -1.4637e-02,  1.5844e-02],
        [ 6.6563e-02,  9.3096e-05, -7.4149e-02],
        [ 6.2878e-02, -4.8368e-02, -5.2451e-02],
        [ 5.3874e-02, -7.2240e-03, -1.0520e-01],
        [ 3.9624e-02, -5.1463e-02, -1.0200e-01],
        [ 4.9110e-02, -4.9002e-02, -5.5031e-02],
        [ 6.1850e-02, -3.1899e-02, -9.5338e-03],
        [ 4.8638e-02,  1.5297e-03, -1.0173e-01],
        [ 3.9043e-02, -2.9510e-02,  1.5926e-02],
        [ 1.0707e-02,  2.8420e-03,  3.0933e-02],
        [ 3.6372e-02,  1.7720e-03, -1.0107e-01],
        [ 2.6880e-02,  5.7100e-03, -8.8555e-02],
        [ 6.5816e-02, -1.1776e-02, -1.3917e-01],
        [ 3.7009e-02, -2.2274e-02,  2.1718e-02],
        [ 3.1320e-02, -1.1944e-04, -1.2159e-01],
        [ 3.9101e-02, -2.4581e-02, -7.2648e-02],
        [ 1.8763e-02,  7.0252e-03, -7.7753e-02],
        [ 2.2793e-02,  6.8131e-03, -9.1190e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000041
penultimate_layer.0.bias: grad mean = 0.001622
output_layer.0.weight: grad mean = 0.004496

MLP Fine-Tuning: task_loss = 1.0958:  27%|██▋       | 4/15 [00:53<01:25,  7.81s/it]
        [ 0.0034,  0.0276, -0.0890],
        [ 0.0254,  0.0262, -0.0970],
        [ 0.0497, -0.0374,  0.0332],
        [ 0.0102,  0.0236, -0.0982],
        [ 0.0400, -0.0403,  0.0140],
        [ 0.0364, -0.0147,  0.0043],
        [ 0.0046,  0.0150, -0.0912],
        [ 0.0185,  0.0272, -0.0860],
        [-0.0013,  0.0195, -0.0965],
        [ 0.0604, -0.0300, -0.0233],
        [ 0.0557, -0.0073, -0.0199],
        [ 0.0098,  0.0243, -0.0943],
        [ 0.0552, -0.0220, -0.0841],
        [ 0.0089, -0.0147,  0.0216],
        [ 0.0408,  0.0164, -0.0988],
        [ 0.0074,  0.0256, -0.0952],
        [ 0.0178,  0.0120,  0.0156],
        [ 0.0027,  0.0172, -0.1062],
        [ 0.0263, -0.0322, -0.0056],
        [ 0.0118, -0.0081,  0.0051],
        [ 0.0501, -0.0071,  0.0153],
        [ 0.0123, -0.0151, -0.0377],
        [-0.0051,  0.0175, -0.0957],
        [-0.0021,  0.0210, -0.0805],
        [ 0.0505,  0.0154, -0.0947],
        [ 0.0065,  0.0174, -0.0890],
        [-0.0131, -0.0050,  0.0042],
        [ 0.0334, -0.0648, -0.0123],
        [ 0.0060,  0.0273, -0.0900],
        [ 0.0489, -0.0239, -0.0779],
        [ 0.0604,  0.0074, -0.0922],
        [ 0.0373,  0.0172,  0.0028],
        [ 0.0518, -0.0090, -0.1129],
        [ 0.0162, -0.0072, -0.0789],
        [ 0.0062,  0.0110, -0.0902],
        [ 0.0187,  0.0203, -0.1182],
        [ 0.0224,  0.0040,  0.0165],
        [ 0.0147,  0.0158, -0.1116],
        [ 0.0178,  0.0226, -0.0992],
        [-0.0109, -0.0027,  0.0217],
        [ 0.0465, -0.0057,  0.0040],
        [ 0.0391, -0.0164,  0.0218],
        [ 0.0151, -0.0065, -0.0357],
        [ 0.0522, -0.0253,  0.0058],
        [ 0.0546, -0.0311, -0.0021],
        [ 0.0134, -0.0373,  0.0154],
        [ 0.0034, -0.0036,  0.0382],
        [ 0.0404, -0.0193,  0.0378],
        [ 0.0064,  0.0182, -0.0996],
        [ 0.0317, -0.0120, -0.0588],
        [ 0.0539, -0.0018, -0.0549],
        [ 0.0007,  0.0247, -0.0863],
        [ 0.0072,  0.0169, -0.0910],
        [ 0.0123,  0.0101, -0.0379],
        [ 0.0138, -0.0303,  0.0147],
        [ 0.0205,  0.0238, -0.0993],
        [ 0.0525, -0.0157, -0.0817],
        [ 0.0374, -0.0259, -0.0353],
        [ 0.0113,  0.0129, -0.0986],
        [ 0.0238,  0.0047, -0.0797],
        [ 0.0655,  0.0005, -0.0688],
        [ 0.0109, -0.0146,  0.0116],
        [ 0.0400,  0.0014, -0.0592],
        [ 0.0190, -0.0004, -0.0713],
        [ 0.0009, -0.0303,  0.0092],
        [ 0.0320,  0.0024, -0.0727],
        [ 0.0161,  0.0163, -0.1119],
        [ 0.0185,  0.0272, -0.0906],
        [ 0.0550, -0.0122, -0.0753],
        [-0.0210,  0.0127, -0.0175],
        [ 0.0292, -0.0058,  0.0358],
        [ 0.0457,  0.0255, -0.0091],
        [ 0.0494, -0.0048, -0.0554],
        [ 0.0007,  0.0192, -0.0831],
        [ 0.0237, -0.0074, -0.1005],
        [ 0.0626, -0.0027, -0.0537],
        [ 0.0092,  0.0285, -0.1009],
        [ 0.0299, -0.0091, -0.0734],
        [ 0.0132,  0.0181, -0.0915],
        [ 0.0159, -0.0293, -0.0004],
        [ 0.0033,  0.0225, -0.0874],
        [ 0.0349,  0.0063, -0.1323],
        [ 0.0413,  0.0053, -0.0112],
        [ 0.0052,  0.0167, -0.0880],
        [ 0.0249, -0.0151, -0.0798],
        [ 0.0465,  0.0151, -0.0810],
        [ 0.0173,  0.0199,  0.0221],
        [-0.0025,  0.0202, -0.1052],
        [ 0.0268,  0.0120, -0.1108],
        [ 0.0444,  0.0133, -0.0687],
        [ 0.0229,  0.0026,  0.0336],
        [ 0.0744, -0.0309, -0.0629],
        [ 0.0166,  0.0235, -0.0981],
        [-0.0026,  0.0204, -0.0839],
        [ 0.0035, -0.0272, -0.0287],
        [ 0.0129,  0.0286, -0.1147],
        [ 0.0036,  0.0239, -0.0832],
        [ 0.0356,  0.0029, -0.1022],
        [ 0.0287, -0.0187, -0.0768],
        [ 0.0067,  0.0261, -0.0953],
        [ 0.0619, -0.0057, -0.0754],
        [ 0.0421, -0.0158, -0.0820],
        [ 0.0205, -0.0127, -0.1121],
        [ 0.0452, -0.0325,  0.0040],
        [ 0.0209,  0.0159, -0.0760],
        [ 0.0093, -0.0089,  0.0661],
        [ 0.0379, -0.0374,  0.0189],
        [ 0.0185, -0.0346,  0.0273],
        [ 0.0374, -0.0115, -0.0141],
        [-0.0020,  0.0193, -0.0942]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000021
penultimate_layer.0.bias: grad mean = 0.000324
output_layer.0.weight: grad mean = 0.002149
output_layer.0.bias: grad mean = 0.010969
[correct] no_actions is False
task_labels:  tensor([0, 2, 1, 1, 1, 2, 0, 0, 1, 2, 0, 1, 1, 2, 1, 2, 1, 0, 2, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 0, 1, 0, 1, 0,
        0, 2, 0, 2, 0, 2, 0, 1, 1, 2, 2, 2, 1, 1, 0, 2, 1, 0, 0, 1, 0, 2, 1, 1,
        2, 2, 0, 2, 2, 1, 2, 0, 0, 2, 1, 1, 2, 1, 0, 0, 2, 0, 0, 2, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 2, 0, 1, 0, 2, 2, 2, 2, 0, 1, 1, 2, 1, 0, 0, 2, 2, 0, 1,
        2, 1, 2, 1, 2, 0, 0, 2]) , task_preds: tensor([[-6.6173e-03,  2.7234e-02, -8.9895e-02],
        [ 2.1656e-02, -6.3845e-03, -1.9579e-02],
        [-1.0852e-02,  5.1876e-02, -7.5699e-02],
        [ 2.0549e-03,  1.9454e-02, -9.0217e-02],
        [ 5.9669e-03,  1.8968e-02, -1.0909e-01],
        [-2.0740e-02,  5.0120e-02, -8.2475e-02],
        [ 4.6116e-02,  1.3101e-02, -5.3453e-02],
        [ 9.1321e-03, -3.0035e-03, -6.4038e-02],
        [-1.3912e-02,  2.2110e-02, -9.1817e-02],
        [ 4.6417e-02, -1.5183e-03, -4.3444e-03],
        [-7.6332e-03,  3.2344e-02, -1.0428e-01],
        [-8.5163e-03,  3.2237e-02, -9.6646e-02],
        [ 2.9288e-02, -2.3378e-02, -1.2153e-01],
        [ 1.8526e-02, -3.3550e-02,  3.7790e-02],
        [-7.9665e-03,  4.2570e-02, -1.0009e-01],
        [ 3.0299e-02, -3.2695e-02,  3.2011e-02],
        [-1.2094e-02,  3.5649e-02, -9.6010e-02],
        [-9.7462e-03,  3.7087e-02, -8.8894e-02],
        [-6.4775e-03,  2.8443e-02, -9.9658e-02],
        [-1.8661e-02,  3.7356e-02, -8.9942e-02],
        [ 2.2608e-02,  1.9597e-02, -9.5508e-02],
        [-2.4199e-03,  4.1822e-02, -1.0304e-01],
        [-1.4817e-02,  4.2684e-02, -9.3862e-02],
        [-1.8274e-02,  3.8174e-02, -1.0047e-01],
        [-7.1165e-03,  5.0132e-02, -9.4513e-02],
        [ 3.3357e-02, -1.7296e-02, -5.9033e-02],
        [ 2.1256e-02,  8.3792e-03, -7.9872e-02],
        [ 1.9618e-02,  7.7139e-04, -3.6563e-02],
        [ 1.9041e-02, -2.2200e-02,  3.8581e-02],
        [-3.6567e-03,  3.2013e-02, -8.5806e-02],
        [-1.1159e-02,  7.4398e-03, -1.2797e-01],
        [ 3.0298e-03,  5.0888e-02, -9.5921e-02],
        [-2.2718e-02,  4.4157e-02, -9.4502e-02],
        [-9.2723e-03,  3.2089e-02, -1.0511e-01],
        [ 1.6612e-03,  3.6163e-02, -1.0321e-01],
        [ 2.0436e-02, -3.1535e-02, -1.4393e-02],
        [ 4.2005e-02, -3.0712e-02,  2.8202e-02],
        [ 1.1559e-02, -2.4976e-02, -2.6840e-02],
        [-7.1512e-03,  4.2784e-02, -8.6232e-02],
        [-1.8296e-02,  7.3443e-03, -3.0572e-02],
        [ 1.5479e-02,  1.1895e-02, -1.1103e-01],
        [-1.4118e-02,  2.0782e-02, -1.1521e-01],
        [ 4.1737e-02, -4.0140e-02, -3.8639e-02],
        [ 1.7253e-02,  2.5674e-02, -7.9485e-02],
        [ 9.1643e-03, -2.1679e-02, -8.6848e-02],
        [ 4.8529e-02,  9.6007e-03, -4.6616e-02],
        [ 1.9899e-02, -5.1522e-03,  5.1033e-02],
        [ 1.7072e-02,  1.8522e-02, -7.5270e-02],
        [ 3.3782e-02, -4.3726e-02,  2.3966e-02],
        [-3.4528e-03,  5.2549e-02, -9.6088e-02],
        [ 2.2090e-02,  2.6091e-02, -1.2117e-01],
        [ 3.5887e-02,  1.8523e-02, -9.6910e-02],
        [ 3.1302e-02,  2.5957e-02, -1.0336e-01],
        [-3.6591e-02, -1.8238e-02, -2.2839e-02],
        [ 3.0781e-02,  2.1080e-02, -8.5026e-02],
        [ 2.9201e-02,  1.0005e-02, -1.1852e-01],
        [ 8.6784e-03, -2.5224e-02, -4.3982e-02],
        [-1.6601e-02,  1.9198e-02, -1.0439e-01],
        [-7.9204e-03,  3.3480e-02, -9.8192e-02],
        [ 1.5816e-02,  1.6694e-02, -9.2496e-02],
        [ 3.7417e-02,  2.1646e-02, -1.1502e-01],
        [-7.0159e-03, -3.0850e-02,  3.1482e-02],
        [ 6.7875e-02,  4.7955e-04, -7.4204e-02],
        [-1.4698e-02,  3.8683e-02, -8.4244e-02],
        [ 4.9146e-02, -3.3911e-02,  3.9288e-02],
        [ 5.3228e-02, -1.4741e-02,  3.0742e-02],
        [-1.6146e-02,  2.4221e-02, -1.0478e-01],
        [ 3.1524e-02,  1.7357e-02, -1.3969e-01],
        [-1.8431e-02,  4.5178e-02, -9.6366e-02],
        [-4.5912e-03,  3.4618e-02, -8.1658e-02],
        [ 2.3386e-03,  3.3259e-02, -9.0279e-02],
        [ 6.3055e-02, -1.0789e-02,  1.0981e-02],
        [-1.2672e-02,  4.1901e-02, -9.5984e-02],
        [ 3.6508e-05,  4.6045e-02, -9.6921e-02],
        [ 4.1050e-02, -6.2366e-03, -9.0457e-02],
        [ 1.8219e-02, -2.4775e-02,  1.9087e-02],
        [ 2.8407e-02, -1.0491e-02, -8.5298e-02],
        [-1.6625e-02,  3.6295e-02, -1.0184e-01],
        [ 3.1991e-02, -1.4919e-03, -4.6681e-03],
        [-1.4887e-02,  3.1459e-02, -9.3169e-02],
        [ 6.9918e-03,  3.3867e-02, -8.5874e-02],
        [-1.8162e-02,  4.0775e-02, -8.3177e-02],
        [-9.9874e-04,  2.6635e-02, -7.9530e-02],
        [ 1.6389e-02, -9.8486e-03, -6.6460e-02],
        [ 1.4930e-02, -2.1879e-02, -9.5402e-02],
        [ 4.5985e-03,  5.1519e-03, -7.3875e-02],
        [ 2.4946e-02, -1.9057e-03, -8.3779e-02],
        [ 2.2822e-02,  3.1771e-02, -7.0083e-02],
        [-1.7358e-02,  4.2443e-02, -8.4427e-02],
        [-8.6542e-03,  4.2766e-02, -9.0588e-02],
        [ 5.1867e-02,  3.5667e-03,  6.4434e-03],
        [ 5.8074e-02, -2.0423e-02, -3.7775e-02],
        [ 2.9500e-02, -5.5592e-03, -5.9627e-02],
        [ 6.0739e-02, -2.2586e-02, -7.3592e-02],
        [ 9.8523e-03,  4.0676e-02, -8.0257e-02],
        [ 3.7484e-02, -1.2696e-02, -2.7323e-02],
        [-1.4067e-02,  3.8995e-02, -8.2252e-02],
        [-1.9813e-02,  3.6068e-02, -9.2235e-02],
        [ 1.8402e-04,  4.4996e-02, -9.3670e-02],
        [ 2.7313e-02,  2.1559e-02, -6.4164e-02],
        [-1.5091e-02,  4.1249e-02, -8.6394e-02],
        [ 3.9725e-02, -1.7524e-02, -7.5265e-02],
        [ 2.8274e-02,  1.4436e-03, -7.9058e-02],
        [ 9.8608e-03,  2.8304e-02, -1.1068e-01],
        [ 1.3519e-03,  3.2497e-02, -7.6595e-02],
        [ 2.3348e-02, -2.7111e-02, -1.1732e-02],
        [-4.0167e-03,  2.2811e-04, -5.3494e-02],
        [ 7.7848e-03, -3.5759e-02, -1.2869e-02],
        [ 3.9079e-02, -2.2211e-02,  4.6038e-02],
        [ 2.2409e-02, -3.2809e-02,  4.4449e-02],
        [ 1.2778e-02, -2.7781e-02, -1.9681e-02],
        [ 5.3151e-02, -8.7563e-04, -7.9944e-02],
        [ 8.3571e-03,  1.2957e-02,  2.5379e-02],
        [-2.4940e-03,  4.7346e-02, -9.2106e-02],
        [-5.5384e-03,  3.2928e-02, -9.1722e-02],
        [ 1.3018e-03, -6.3595e-03, -5.9956e-04],
        [ 4.2982e-02, -1.9400e-02, -7.7729e-02],
        [-1.7358e-02,  4.2443e-02, -8.4427e-02],
        [ 1.7620e-03,  9.7975e-03, -9.7689e-02],
        [ 1.1576e-02,  1.1572e-02,  8.3349e-03],
        [ 3.4140e-02, -4.1129e-02, -4.1447e-02],
        [ 3.3469e-02,  3.4224e-03, -5.1716e-02],
        [-1.2283e-03, -5.0617e-03, -5.3822e-02],
        [ 3.1256e-02,  2.9501e-04, -8.4728e-02],
        [ 5.1438e-02, -1.8617e-02,  1.3868e-04],
        [-1.0687e-02,  3.0640e-02, -9.1589e-02],
        [ 1.5916e-02,  6.9374e-03, -1.1826e-01],
        [ 1.7823e-03, -2.0946e-02, -3.6786e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000022
penultimate_layer.0.bias: grad mean = 0.000562
output_layer.0.weight: grad mean = 0.002170
output_layer.0.bias: grad mean = 0.015792
[correct] no_actions is False
task_labels:  tensor([2, 0, 0, 2, 0, 1, 1, 1, 0, 2, 1, 0, 1, 1, 0, 2, 0, 2, 0, 2, 1, 2, 2, 1,
        1, 2, 0, 2, 2, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 2, 2, 2, 0, 1, 2, 0, 2, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 2, 1,
        1, 2, 2, 2, 1, 0, 2, 0, 1, 0, 1, 0, 0, 1, 2, 0, 2, 1, 2, 2, 0, 2, 2, 2,
        2, 0, 0, 1, 2, 2, 1, 1, 0, 2, 0, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 0,
        1, 0, 1, 2, 2, 0, 0, 0]) , task_preds: tensor([[-0.0200,  0.0322, -0.0888],
        [-0.0212,  0.0522, -0.0902],
        [-0.0232,  0.0451, -0.0839],
        [ 0.0205,  0.0138, -0.0907],
        [ 0.0302, -0.0359,  0.0274],
        [-0.0005, -0.0339,  0.0026],
        [-0.0129,  0.0275, -0.1066],
        [ 0.0314,  0.0190, -0.0374],
        [ 0.0203, -0.0687,  0.0095],
        [-0.0236,  0.0465, -0.0833],
        [-0.0186,  0.0499, -0.0735],
        [ 0.0292,  0.0282, -0.0946],
        [-0.0116,  0.0534, -0.0895],
        [-0.0279,  0.0437, -0.0876],
        [ 0.0358, -0.0121, -0.0429],
        [ 0.0228,  0.0055, -0.0599],
        [ 0.0062, -0.0079,  0.0213],
        [ 0.0349, -0.0132,  0.0430],
        [ 0.0408, -0.0301,  0.0268],
        [ 0.0113, -0.0445,  0.0377],
        [-0.0156,  0.0373, -0.1128],
        [ 0.0009, -0.0389,  0.0003],
        [-0.0082,  0.0527, -0.1017],
        [ 0.0199,  0.0362, -0.0596],
        [ 0.0111,  0.0345, -0.0635],
        [ 0.0471,  0.0061, -0.0543],
        [-0.0124,  0.0527, -0.0937],
        [-0.0002, -0.0304,  0.0213],
        [-0.0138,  0.0323, -0.0961],
        [-0.0032, -0.0463,  0.0281],
        [-0.0094,  0.0547, -0.0967],
        [ 0.0308,  0.0189,  0.0190],
        [-0.0112,  0.0262, -0.1066],
        [ 0.0260, -0.0064,  0.0115],
        [-0.0247,  0.0466, -0.0896],
        [ 0.0149, -0.0079,  0.0658],
        [-0.0186,  0.0230, -0.0833],
        [-0.0131,  0.0008,  0.0629],
        [-0.0234,  0.0476, -0.0929],
        [-0.0007,  0.0508, -0.0882],
        [-0.0262,  0.0451, -0.0822],
        [ 0.0207, -0.0077, -0.0732],
        [-0.0108,  0.0340, -0.1069],
        [ 0.0371, -0.0074, -0.0837],
        [ 0.0085,  0.0228, -0.0777],
        [-0.0149,  0.0505, -0.1095],
        [ 0.0416, -0.0174, -0.0739],
        [ 0.0100, -0.0002,  0.0575],
        [-0.0168, -0.0273, -0.0123],
        [-0.0266,  0.0461, -0.0914],
        [-0.0264,  0.0470, -0.0866],
        [ 0.0212, -0.0145, -0.0034],
        [ 0.0304,  0.0041, -0.0450],
        [ 0.0155,  0.0270, -0.0585],
        [-0.0152,  0.0454, -0.0883],
        [ 0.0184, -0.0005, -0.0595],
        [-0.0049,  0.0348, -0.0784],
        [ 0.0183,  0.0355, -0.0554],
        [-0.0205,  0.0513, -0.0981],
        [ 0.0065, -0.0118,  0.0535],
        [ 0.0053, -0.0454,  0.0069],
        [ 0.0069,  0.0354, -0.0630],
        [-0.0302,  0.0428, -0.0961],
        [-0.0242,  0.0493, -0.0938],
        [ 0.0037,  0.0164, -0.0935],
        [ 0.0172,  0.0330, -0.0649],
        [ 0.0243,  0.0258, -0.0692],
        [ 0.0151,  0.0295, -0.1173],
        [-0.0016, -0.0115,  0.0106],
        [ 0.0093,  0.0103, -0.1148],
        [-0.0064,  0.0070, -0.0717],
        [ 0.0425, -0.0242, -0.0695],
        [ 0.0024,  0.0296, -0.1243],
        [ 0.0108, -0.0371,  0.0527],
        [-0.0035, -0.0127,  0.0345],
        [ 0.0340,  0.0209, -0.0664],
        [ 0.0074, -0.0013, -0.0898],
        [ 0.0211, -0.0383,  0.0541],
        [-0.0111,  0.0513, -0.0844],
        [-0.0275,  0.0476, -0.0848],
        [ 0.0117,  0.0062,  0.0435],
        [-0.0191,  0.0431, -0.0886],
        [ 0.0300,  0.0119, -0.0134],
        [-0.0295,  0.0476, -0.0812],
        [-0.0215,  0.0495, -0.0859],
        [ 0.0066,  0.0355, -0.1079],
        [ 0.0042,  0.0387, -0.0623],
        [ 0.0504, -0.0096,  0.0462],
        [ 0.0320, -0.0152, -0.0811],
        [ 0.0029, -0.0272, -0.0338],
        [ 0.0499, -0.0191,  0.0103],
        [ 0.0145, -0.0171,  0.0398],
        [-0.0165,  0.0385, -0.1022],
        [ 0.0103,  0.0235, -0.1271],
        [-0.0038, -0.0097,  0.0268],
        [-0.0091, -0.0221,  0.0590],
        [-0.0140,  0.0405, -0.0785],
        [ 0.0399, -0.0439, -0.0378],
        [ 0.0107,  0.0179, -0.1061],
        [-0.0043,  0.0171, -0.0852],
        [ 0.0293, -0.0381,  0.0116],
        [ 0.0394, -0.0222, -0.0174],
        [ 0.0051,  0.0096,  0.0193],
        [ 0.0233, -0.0062, -0.0509],
        [ 0.0079, -0.0048, -0.0282],
        [ 0.0335, -0.0173, -0.0677],
        [ 0.0081, -0.0179,  0.0151],
        [ 0.0114, -0.0019,  0.0388],
        [ 0.0372, -0.0211, -0.0680],
        [-0.0119,  0.0350, -0.0944],
        [-0.0281,  0.0524, -0.0846],
        [ 0.0315, -0.0042,  0.0575],
        [-0.0266,  0.0465, -0.0880],
        [-0.0070,  0.0486, -0.0912],
        [-0.0237,  0.0467, -0.0915],
        [-0.0239,  0.0442, -0.0896],
        [-0.0265,  0.0487, -0.0820],
        [-0.0360,  0.0085,  0.0072],
        [ 0.0140, -0.0113,  0.0562],
        [ 0.0272, -0.0473,  0.0374],
        [-0.0216, -0.0220,  0.0028],
        [-0.0360,  0.0478, -0.0993],
        [-0.0034,  0.0041, -0.0353],
        [ 0.0199,  0.0074, -0.0430],
        [ 0.0419, -0.0335, -0.0424],
        [-0.0265,  0.0487, -0.0820],
        [ 0.0263, -0.0093, -0.0468],
        [-0.0290,  0.0466, -0.0946]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000027
penultimate_layer.0.bias: grad mean = 0.000759
output_layer.0.weight: grad mean = 0.002772
output_layer.0.bias: grad mean = 0.023770
[correct] no_actions is False
task_labels:  tensor([0, 0, 0, 2, 1, 0, 1, 0, 1, 0, 1, 2, 0, 2, 1, 1, 2, 0, 2, 1, 1, 1, 2, 1,
        2, 1, 1, 2, 0, 1, 1, 0, 0, 1, 2, 0, 2, 2, 1, 1, 1, 2, 1, 1, 2, 0, 0, 1,
        2, 0, 1, 1, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 2, 1, 0, 2, 1, 1, 1, 2, 0, 2,
        2, 2, 0, 2, 0, 0, 1, 0, 1, 0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 2, 1, 0, 1,
        2, 0, 1, 0, 1, 1, 2, 0, 1, 1, 2, 0, 2, 0, 1, 2, 0, 1, 2, 1, 1, 0, 2, 0,
        2, 2, 2, 2, 2, 1, 1, 0]) , task_preds: tensor([[ 2.5471e-02, -2.0982e-02, -4.3720e-02],
        [ 2.0940e-02,  3.0262e-02, -6.4841e-02],
        [ 4.0605e-02, -1.2708e-02, -8.2368e-02],
        [-2.7134e-02,  2.2291e-02, -8.9305e-02],
        [-3.3470e-02,  5.3025e-02, -8.6153e-02],
        [-6.9020e-03,  2.5845e-02, -6.1422e-02],
        [ 1.7891e-02,  2.3160e-02, -1.3102e-01],
        [ 6.0089e-02, -7.6055e-03,  6.2770e-03],
        [-1.2146e-02,  5.4389e-02, -9.1488e-02],
        [ 3.6494e-02,  1.7766e-02, -7.7828e-02],
        [ 1.8660e-02, -2.0528e-02, -5.2847e-03],
        [ 3.8904e-03,  3.8658e-02, -5.8427e-02],
        [-2.6185e-02,  4.0355e-02, -8.6229e-02],
        [-3.3945e-02,  5.0848e-02, -8.6600e-02],
        [ 3.3034e-02, -2.5691e-02,  2.3240e-03],
        [-3.1339e-02,  4.2464e-02, -9.8636e-02],
        [-4.2971e-03,  4.2465e-02, -7.0225e-02],
        [-2.3283e-02,  4.6536e-02, -8.7579e-02],
        [-2.2409e-02, -3.8714e-02,  4.8062e-02],
        [ 1.8109e-02,  9.7411e-03,  1.7231e-02],
        [ 1.2936e-02, -3.6954e-02,  3.0912e-02],
        [ 2.5031e-02, -2.1718e-02, -8.4225e-02],
        [ 2.3347e-02, -1.1285e-02,  3.1200e-02],
        [ 2.3037e-02,  4.7398e-03, -7.3028e-02],
        [-3.4693e-02,  5.1439e-02, -7.5882e-02],
        [-3.1003e-02,  4.4697e-02, -1.0239e-01],
        [-2.0579e-03, -2.8620e-02, -9.8595e-02],
        [ 3.7301e-02, -1.4139e-02,  1.7066e-03],
        [-2.4261e-02,  4.9365e-02, -9.5805e-02],
        [-1.8875e-02, -4.1128e-02,  6.1232e-02],
        [-1.4801e-02,  5.0597e-02, -8.0742e-02],
        [ 2.7343e-02, -9.9710e-03, -6.9542e-02],
        [-3.0423e-02,  6.2106e-02, -8.4579e-02],
        [-2.9896e-02,  4.8964e-02, -7.6847e-02],
        [-2.1804e-02,  5.3503e-02, -9.3649e-02],
        [ 1.3902e-02,  2.6446e-02, -6.7351e-02],
        [-1.2872e-02, -9.4565e-03, -3.2806e-02],
        [-2.7635e-02,  4.3410e-02, -7.7617e-02],
        [-2.6859e-02,  5.1334e-02, -7.8040e-02],
        [-1.2504e-02,  4.9428e-02, -9.1787e-02],
        [ 2.5995e-02,  2.3270e-02, -1.0308e-01],
        [ 1.0995e-02,  2.4449e-02, -6.3817e-02],
        [ 4.1772e-02, -3.0097e-03, -6.2517e-02],
        [ 9.3174e-03, -2.2623e-02,  1.9948e-02],
        [-2.7940e-02, -2.0214e-02,  4.0537e-02],
        [ 3.6311e-02, -3.3550e-02,  3.5777e-02],
        [ 1.2765e-02, -9.4508e-03, -3.9138e-02],
        [-2.5212e-02,  3.7102e-02, -9.0509e-02],
        [-2.4973e-03, -2.7591e-02, -8.2205e-02],
        [ 1.9391e-02,  4.3761e-02, -8.0033e-02],
        [-1.7900e-02,  4.9467e-02, -9.8292e-02],
        [ 1.5062e-03, -3.9444e-02,  5.5274e-02],
        [ 3.7033e-02,  9.0229e-03, -3.6058e-02],
        [ 2.9887e-02, -5.2225e-02,  3.8309e-02],
        [-1.0073e-02,  3.9072e-02, -1.0987e-01],
        [-3.1096e-02,  5.3578e-02, -8.2955e-02],
        [-7.7774e-03,  3.7190e-02, -9.9770e-02],
        [ 1.4752e-02,  1.1353e-02, -4.4045e-02],
        [-2.3846e-02,  5.3873e-02, -9.9491e-02],
        [-3.3769e-02,  4.0336e-02, -9.4845e-02],
        [ 2.6568e-02, -3.1906e-02,  7.3204e-02],
        [-2.4818e-02,  4.3343e-02, -9.2185e-02],
        [-1.6542e-02,  1.8369e-02, -4.1864e-02],
        [ 2.0290e-02, -4.9898e-02,  5.2264e-02],
        [ 3.4877e-02, -4.6838e-02,  3.9327e-02],
        [-1.2245e-03,  1.4596e-03,  5.4207e-02],
        [ 3.6642e-03, -4.4743e-03,  6.7486e-02],
        [ 3.1629e-02, -3.6307e-02,  3.2526e-02],
        [-1.6228e-02,  4.1094e-02, -1.0121e-01],
        [ 2.1793e-02, -2.6305e-02,  6.5466e-02],
        [ 3.6441e-02, -2.3659e-02, -9.7520e-02],
        [-3.5791e-02,  4.3553e-02, -9.6230e-02],
        [ 7.3238e-03,  5.1722e-03, -8.9967e-02],
        [-3.3366e-02,  5.2458e-02, -7.8869e-02],
        [-1.6301e-02,  2.8666e-02, -1.0382e-01],
        [ 7.9273e-03, -1.0307e-02, -4.4373e-02],
        [-1.9989e-02,  4.9387e-02, -7.7445e-02],
        [ 2.0113e-02, -2.6083e-02,  4.1769e-02],
        [ 2.4692e-02,  1.5909e-02, -9.7884e-02],
        [ 1.6293e-03,  1.8710e-02, -6.3886e-02],
        [ 1.0854e-02, -1.5189e-02,  7.6922e-02],
        [ 1.2471e-02,  7.2724e-03, -6.7255e-02],
        [-1.8693e-02, -9.5686e-03,  9.9302e-02],
        [-7.0766e-03,  5.0466e-02, -7.3243e-02],
        [ 4.0522e-03,  2.5793e-02, -6.0727e-02],
        [ 3.3526e-02, -9.7526e-03, -7.6038e-02],
        [-5.6111e-03, -1.7594e-02,  9.7023e-02],
        [-1.7779e-02, -2.9525e-02,  5.5014e-02],
        [ 3.8074e-02, -4.2125e-02,  6.9311e-02],
        [-3.0099e-02,  3.4593e-02, -8.5145e-02],
        [ 6.5673e-03, -1.1889e-02, -5.1072e-02],
        [-1.7941e-02,  3.5862e-02, -8.5626e-02],
        [ 2.9316e-02, -3.3684e-02,  7.4909e-02],
        [-2.6850e-03,  2.4461e-02, -9.2095e-02],
        [-3.1779e-02,  5.6840e-02, -8.1745e-02],
        [-1.6889e-02,  3.5091e-02, -9.8917e-02],
        [ 2.3041e-02, -1.6690e-02, -6.6072e-02],
        [-3.2572e-02,  5.2592e-02, -8.0083e-02],
        [ 6.1264e-03, -4.7926e-02,  4.2165e-02],
        [ 2.6396e-02,  4.0253e-02, -8.4519e-02],
        [-9.9293e-03, -2.9640e-02,  7.5650e-02],
        [-3.2172e-02,  6.0088e-02, -8.7052e-02],
        [-4.5838e-03, -2.9163e-02, -8.0160e-02],
        [ 2.7990e-03,  1.2569e-02,  8.6751e-02],
        [ 3.4890e-02,  1.7270e-02, -9.6886e-02],
        [-2.9675e-02,  4.4141e-02, -7.9626e-02],
        [ 8.8933e-05,  1.1811e-02, -5.3643e-02],
        [-1.8883e-02,  3.2591e-02, -8.9718e-02],
        [ 1.4480e-02, -1.2912e-02, -6.2654e-02],
        [-2.1520e-02,  4.8526e-02, -8.2746e-02],
        [ 3.4249e-02, -4.7571e-02,  6.7013e-02],
        [ 1.2063e-02, -4.9293e-02, -2.5832e-03],
        [ 7.2427e-03,  2.3071e-02, -9.3076e-02],
        [ 4.1245e-02, -6.1822e-03, -4.9081e-02],
        [ 3.5527e-02, -4.4320e-02,  2.8990e-02],
        [ 4.5484e-02, -3.5670e-02,  2.1676e-02],
        [-1.6357e-02,  4.5659e-02, -1.0418e-01],
        [-2.8204e-02,  5.3898e-02, -8.0506e-02],
        [-3.4429e-02,  4.3991e-02, -8.1816e-02],
        [ 2.3800e-02, -3.6233e-02,  2.2000e-02],
        [-2.4269e-02,  2.7506e-02, -8.6484e-02],
        [-1.3563e-02, -4.6811e-02,  5.8086e-02],
        [ 4.2747e-02, -1.0295e-02, -2.2123e-02],
        [ 3.3444e-02, -4.4875e-02,  4.1143e-02],
        [ 3.1260e-02, -1.6642e-02,  7.7485e-03],
        [ 2.0396e-02,  3.8066e-02, -8.7674e-02],
        [ 3.2657e-03, -1.8960e-03,  3.7771e-02],
        [ 3.7503e-03,  2.7886e-02, -9.1978e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000023
penultimate_layer.0.bias: grad mean = 0.000624
output_layer.0.weight: grad mean = 0.002491

MLP Fine-Tuning: task_loss = 1.1016:  33%|███▎      | 5/15 [00:55<00:54,  5.42s/it]
[correct] no_actions is False
task_labels:  tensor([0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 0, 2, 1, 2, 0, 2, 2, 0, 2, 1, 2, 0, 1,
        2, 1, 2, 0, 2, 2, 0, 0, 0, 1, 0, 2, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 2,
        1, 0, 1, 1, 0, 2, 1, 2, 1, 1, 1, 2, 2, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 1,
        2, 1, 2, 2, 2, 0, 1, 2, 1, 2, 2, 1, 0, 2, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 2, 2, 0, 0, 0, 1, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 0, 0, 2, 2, 1, 0, 2,
        1, 0, 0, 0, 0, 0, 0, 1]) , task_preds: tensor([[ 2.2863e-02, -6.2316e-02,  6.4687e-02],
        [-3.2897e-02,  4.8267e-02, -8.9883e-02],
        [-1.6217e-02,  5.0091e-02, -7.8032e-02],
        [-3.0910e-02,  4.9265e-02, -7.7938e-02],
        [-3.0744e-02,  5.1847e-02, -8.0295e-02],
        [ 7.8529e-03, -4.2158e-02,  4.4814e-02],
        [-1.6365e-02, -9.8543e-03, -2.3239e-02],
        [ 4.4594e-02, -2.4593e-02,  7.5353e-02],
        [ 1.3890e-02, -1.5819e-02,  2.1449e-02],
        [-2.1246e-03,  9.5345e-03, -1.0487e-01],
        [ 1.9666e-02, -1.7008e-02, -5.0846e-02],
        [-1.8047e-02,  5.2928e-02, -8.1437e-02],
        [-1.6289e-02,  5.2107e-02, -8.4398e-02],
        [-8.6707e-03, -1.9193e-02, -3.4387e-02],
        [ 7.7188e-03, -3.1007e-04, -6.5860e-02],
        [ 1.9113e-02, -2.1419e-02,  6.0372e-03],
        [ 3.0678e-02, -2.9721e-02, -4.7179e-02],
        [-1.1259e-02, -2.1229e-02,  5.3050e-02],
        [ 2.2776e-02, -6.9449e-02,  4.6369e-02],
        [ 1.0337e-02,  4.8774e-03, -4.1104e-02],
        [-2.3662e-02,  4.7069e-02, -8.2479e-02],
        [ 1.1378e-02, -1.6917e-02, -3.7312e-02],
        [ 1.3588e-02,  2.8696e-02, -5.3787e-02],
        [-3.0214e-02,  4.5287e-02, -8.7065e-02],
        [-2.4181e-02,  4.8314e-02, -8.4838e-02],
        [ 1.4274e-03, -3.6003e-02,  8.0678e-02],
        [-3.3464e-02,  4.0894e-02, -8.1520e-02],
        [ 2.3280e-02, -3.5493e-02,  6.0606e-03],
        [-1.8428e-02,  5.0637e-02, -8.5324e-02],
        [-2.1582e-02,  5.6363e-02, -7.1730e-02],
        [-3.5953e-02,  4.5153e-02, -8.1168e-02],
        [ 6.3968e-02, -2.6920e-02, -2.0038e-02],
        [ 2.3504e-02,  1.2555e-02, -6.3301e-02],
        [ 1.7518e-02, -1.9467e-02, -6.8985e-02],
        [ 2.8826e-02, -5.7282e-03, -1.0587e-01],
        [-1.2338e-02,  4.2418e-03, -6.0083e-02],
        [ 4.8742e-03, -1.4618e-02,  6.5429e-02],
        [ 6.0967e-03, -1.6139e-02, -4.4131e-02],
        [-2.0794e-02,  4.8749e-02, -7.5290e-02],
        [ 3.3616e-02, -4.8862e-02,  8.0407e-02],
        [-7.5152e-03, -4.8458e-02,  4.7666e-02],
        [-2.3950e-02, -4.6304e-02,  7.5086e-02],
        [ 1.5887e-02, -1.1898e-02, -3.3188e-02],
        [ 5.1308e-02, -1.7298e-02, -2.9239e-02],
        [ 7.1834e-03, -2.0304e-02, -4.7650e-02],
        [ 1.5089e-02,  1.3765e-02, -1.0609e-01],
        [-3.5785e-02,  3.9875e-02, -8.7408e-02],
        [-3.1899e-02,  5.5681e-02, -7.0809e-02],
        [-6.8919e-03, -1.7019e-02, -9.5020e-02],
        [ 7.0306e-03, -3.2253e-02,  5.4658e-02],
        [-9.0419e-03, -1.1146e-03, -4.7489e-02],
        [-2.4682e-03, -4.2704e-02,  6.4986e-02],
        [-3.2239e-02,  4.8817e-02, -8.5240e-02],
        [ 4.4925e-02, -3.3201e-02,  3.7000e-02],
        [ 2.1772e-02, -1.5751e-02, -5.6926e-02],
        [ 1.1805e-02, -1.4322e-02,  8.2341e-02],
        [ 4.4390e-03, -1.6218e-02, -5.2807e-02],
        [-3.1822e-02,  3.9187e-02, -9.2159e-02],
        [-3.6347e-02,  4.5923e-02, -8.6747e-02],
        [ 3.3975e-03, -4.0613e-02,  6.0660e-02],
        [-1.0399e-02,  3.3947e-02, -7.7792e-02],
        [-3.0019e-03, -2.5934e-02,  3.3362e-02],
        [ 2.3165e-02, -4.8559e-02,  5.4536e-02],
        [-3.4232e-02,  6.0891e-02, -9.4475e-02],
        [-1.4455e-02, -3.5097e-02,  8.9230e-02],
        [ 1.2293e-02,  6.6679e-04,  5.7866e-02],
        [ 5.0981e-02, -6.9906e-03, -3.9000e-02],
        [-1.9519e-02,  5.2974e-02, -8.4320e-02],
        [ 3.9376e-02, -4.3503e-02,  4.0626e-02],
        [-1.6460e-02,  1.2679e-02,  6.2515e-02],
        [-8.3102e-03,  5.2815e-02, -6.8416e-02],
        [ 2.2465e-02,  6.3171e-03,  4.4385e-03],
        [ 1.3869e-02, -3.0653e-02,  4.7549e-02],
        [ 2.2892e-02,  1.6779e-02, -9.9410e-02],
        [-1.6443e-02,  5.2807e-02, -7.6081e-02],
        [ 2.7168e-02, -2.3169e-02, -5.1201e-02],
        [-1.1334e-02, -2.4078e-02,  6.0859e-02],
        [ 2.2342e-02,  2.1741e-02, -5.8265e-02],
        [-2.1558e-02,  5.5075e-02, -8.1670e-02],
        [ 2.7261e-03, -2.0081e-02,  8.9381e-02],
        [-3.4356e-02,  4.8959e-02, -8.0456e-02],
        [-6.9965e-03,  1.7478e-02, -4.5169e-02],
        [-6.7451e-03, -4.7861e-02,  6.5710e-02],
        [-3.5184e-02,  5.1480e-02, -7.6489e-02],
        [-1.9952e-02,  5.5921e-02, -8.8668e-02],
        [-1.5373e-02,  7.2491e-03,  6.2649e-04],
        [ 2.8599e-03,  2.9302e-02, -9.2695e-02],
        [-4.0363e-02,  4.8384e-02, -7.7566e-02],
        [-7.6926e-03,  2.4033e-02, -8.4523e-02],
        [ 1.4714e-02,  5.3379e-03,  2.7945e-02],
        [-2.5875e-02,  3.6097e-02, -9.9129e-02],
        [ 9.6690e-03,  2.6456e-02, -8.0304e-02],
        [-3.8980e-02, -1.2661e-02,  5.5021e-02],
        [-2.8620e-02,  4.4736e-02, -9.4050e-02],
        [ 8.2984e-03,  1.7971e-02, -6.4733e-02],
        [ 1.9602e-02,  3.2359e-02, -7.6757e-02],
        [-3.0109e-02,  4.3932e-02, -7.9015e-02],
        [ 1.2359e-02, -5.1720e-02,  7.1233e-02],
        [ 3.3335e-02, -4.7288e-02,  8.2727e-02],
        [ 1.8154e-02,  2.6724e-02, -7.0222e-02],
        [ 4.1723e-02,  8.2765e-04, -1.3208e-02],
        [ 4.5008e-03,  9.2654e-03, -1.0707e-01],
        [ 3.6365e-02, -4.3059e-02,  5.4370e-02],
        [ 2.7895e-02, -4.7635e-02,  5.6226e-02],
        [-3.0360e-02,  4.8091e-02, -7.4783e-02],
        [ 2.7924e-03,  2.0463e-02, -6.3866e-02],
        [ 2.2118e-03,  1.0751e-02, -8.1994e-02],
        [ 1.3259e-02, -3.1373e-02,  5.7262e-02],
        [ 1.0537e-04,  7.0475e-03,  9.8178e-02],
        [-3.9711e-02,  5.2688e-02, -9.0852e-02],
        [-1.3044e-02,  1.5082e-02, -1.0895e-01],
        [-3.6241e-02,  4.8742e-02, -7.6698e-02],
        [ 1.4206e-02,  9.5238e-03, -6.9972e-02],
        [-1.8734e-02,  4.6447e-02, -8.8925e-02],
        [ 3.4263e-03, -2.0194e-02,  4.2755e-02],
        [-3.3903e-02,  5.1590e-02, -7.6720e-02],
        [-3.2453e-02, -2.0137e-02,  7.3827e-02],
        [-3.6778e-02,  4.5581e-02, -8.4517e-02],
        [ 3.7304e-02,  1.3513e-02, -7.2490e-02],
        [-1.3417e-02, -2.1469e-02, -7.4546e-02],
        [ 2.2668e-03,  1.8416e-02, -9.9831e-02],
        [ 1.5438e-02, -8.2595e-02,  3.5800e-02],
        [-2.8534e-02,  5.4791e-02, -8.7084e-02],
        [-1.2541e-02, -2.2571e-02,  8.5507e-02],
        [ 1.5546e-02, -3.1679e-02,  5.5603e-02],
        [-3.1387e-02,  6.2158e-02, -8.3874e-02],
        [-7.8732e-03, -3.0994e-02,  5.8667e-02],
        [-1.1531e-02, -5.8111e-02,  5.5584e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000023
penultimate_layer.0.bias: grad mean = 0.000176
output_layer.0.weight: grad mean = 0.002522
output_layer.0.bias: grad mean = 0.005977
[correct] no_actions is False
task_labels:  tensor([1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 0, 0, 1, 2, 1, 1, 2, 0, 2, 1, 2, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 2, 2, 2, 1, 2, 0, 1, 0, 1, 0, 1, 0, 2, 0, 2, 2, 2,
        0, 2, 0, 2, 1, 1, 0, 0, 1, 2, 0, 1, 0, 1, 1, 2, 1, 1, 0, 2, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 2, 2, 0, 2, 2, 0, 1, 2, 0, 2, 0, 1, 2, 0,
        1, 0, 1, 1, 2, 2, 1, 2, 1, 1, 2, 0, 2, 2, 2, 2, 1, 0, 2, 0, 1, 1, 0, 2,
        2, 2, 2, 0, 0, 2, 1, 2]) , task_preds: tensor([[ 0.0056, -0.0250,  0.0978],
        [ 0.0215, -0.0015,  0.0602],
        [ 0.0330, -0.0265, -0.0359],
        [-0.0346,  0.0485, -0.0852],
        [ 0.0274,  0.0169, -0.0953],
        [ 0.0023,  0.0210, -0.0861],
        [-0.0305, -0.0375,  0.0365],
        [-0.0128,  0.0362, -0.1150],
        [ 0.0144, -0.0374,  0.0908],
        [ 0.0234, -0.0448,  0.0984],
        [-0.0011, -0.0452,  0.0187],
        [ 0.0273, -0.0276, -0.0211],
        [-0.0243,  0.0217, -0.0809],
        [-0.0337,  0.0404, -0.1072],
        [ 0.0005, -0.0415,  0.0713],
        [-0.0164,  0.0304, -0.0887],
        [-0.0324,  0.0441, -0.0821],
        [ 0.0247, -0.0232,  0.0937],
        [-0.0187,  0.0365, -0.1023],
        [ 0.0115, -0.0162, -0.0556],
        [ 0.0164, -0.0360,  0.0783],
        [-0.0136, -0.0392,  0.0091],
        [-0.0038,  0.0280, -0.0478],
        [ 0.0214,  0.0241, -0.0877],
        [-0.0377,  0.0539, -0.0749],
        [-0.0120,  0.0115,  0.0526],
        [ 0.0299, -0.0063, -0.0724],
        [ 0.0280, -0.0611,  0.0789],
        [-0.0276,  0.0227, -0.1007],
        [-0.0106, -0.0079, -0.0086],
        [-0.0337,  0.0515, -0.0767],
        [-0.0308,  0.0541, -0.0831],
        [-0.0014,  0.0073, -0.0987],
        [ 0.0400, -0.0397,  0.0298],
        [-0.0322,  0.0488, -0.0771],
        [ 0.0161,  0.0168, -0.0988],
        [ 0.0609, -0.0491,  0.0297],
        [-0.0250,  0.0611, -0.0900],
        [-0.0079,  0.0347, -0.1059],
        [-0.0077, -0.0244, -0.0615],
        [-0.0153,  0.0457, -0.0814],
        [-0.0310,  0.0303, -0.0820],
        [-0.0087,  0.0196, -0.0971],
        [ 0.0035, -0.0211,  0.1081],
        [ 0.0364, -0.0500,  0.0613],
        [ 0.0166, -0.0162,  0.0025],
        [ 0.0093, -0.0171, -0.0248],
        [-0.0255,  0.0426, -0.0898],
        [ 0.0432, -0.0308, -0.0562],
        [-0.0356,  0.0605, -0.0730],
        [ 0.0373, -0.0171, -0.0459],
        [-0.0352, -0.0314,  0.0659],
        [-0.0180,  0.0451, -0.1009],
        [-0.0147,  0.0545, -0.0967],
        [ 0.0185,  0.0030, -0.0363],
        [ 0.0098, -0.0031, -0.0484],
        [-0.0337,  0.0515, -0.0767],
        [ 0.0129,  0.0119, -0.0580],
        [ 0.0369,  0.0006, -0.0233],
        [ 0.0279, -0.0540, -0.0032],
        [-0.0260,  0.0392, -0.0843],
        [-0.0139,  0.0543, -0.0888],
        [-0.0041,  0.0305, -0.0961],
        [ 0.0004, -0.0615,  0.0761],
        [-0.0363,  0.0357, -0.0934],
        [-0.0230, -0.0166,  0.1006],
        [-0.0055,  0.0330, -0.0962],
        [ 0.0073, -0.0452,  0.0301],
        [-0.0171,  0.0463, -0.0873],
        [-0.0014,  0.0144, -0.0838],
        [-0.0334,  0.0486, -0.0773],
        [-0.0345,  0.0323, -0.0858],
        [-0.0003, -0.0642,  0.0413],
        [-0.0301,  0.0433, -0.0776],
        [-0.0149,  0.0167, -0.0847],
        [-0.0232,  0.0493, -0.0789],
        [ 0.0287, -0.0575,  0.0883],
        [-0.0099,  0.0198, -0.0581],
        [ 0.0131, -0.0043, -0.0173],
        [ 0.0256,  0.0124, -0.0415],
        [ 0.0227, -0.0437,  0.0953],
        [-0.0282,  0.0491, -0.0929],
        [ 0.0163, -0.0556,  0.0247],
        [-0.0074, -0.0102,  0.0790],
        [-0.0257, -0.0521,  0.0500],
        [ 0.0098, -0.0039, -0.0809],
        [-0.0392,  0.0464, -0.0737],
        [ 0.0161, -0.0017, -0.0585],
        [-0.0331,  0.0559, -0.0834],
        [ 0.0125, -0.0605,  0.0071],
        [-0.0215,  0.0397, -0.0834],
        [ 0.0344, -0.0516,  0.0533],
        [-0.0421,  0.0376, -0.0778],
        [ 0.0224, -0.0598,  0.0644],
        [-0.0149, -0.0426,  0.0575],
        [-0.0014,  0.0092, -0.0932],
        [-0.0309,  0.0424, -0.0807],
        [ 0.0297, -0.0014, -0.0820],
        [-0.0033,  0.0075, -0.0496],
        [-0.0312,  0.0557, -0.0774],
        [-0.0221,  0.0426, -0.0926],
        [-0.0047, -0.0007, -0.0625],
        [-0.0413,  0.0505, -0.0948],
        [ 0.0085,  0.0255, -0.0804],
        [-0.0072, -0.0425,  0.0269],
        [-0.0192,  0.0562, -0.0836],
        [-0.0010,  0.0162, -0.0639],
        [-0.0307,  0.0421, -0.0751],
        [-0.0217,  0.0629, -0.0815],
        [-0.0214,  0.0096, -0.0225],
        [-0.0065, -0.0358, -0.0678],
        [-0.0323,  0.0417, -0.0827],
        [-0.0085, -0.0270,  0.0849],
        [-0.0226,  0.0399, -0.0732],
        [ 0.0253, -0.0377,  0.0658],
        [-0.0254,  0.0575, -0.0788],
        [-0.0330,  0.0605, -0.0852],
        [-0.0347,  0.0534, -0.0826],
        [-0.0252,  0.0530, -0.0819],
        [-0.0039, -0.0563,  0.0667],
        [ 0.0141, -0.0242,  0.0508],
        [ 0.0047,  0.0218, -0.1172],
        [ 0.0454, -0.0392,  0.0480],
        [-0.0385,  0.0541, -0.0753],
        [ 0.0093,  0.0170, -0.0865],
        [-0.0078,  0.0284, -0.0681],
        [-0.0024,  0.0211, -0.0872],
        [ 0.0198,  0.0246, -0.0643]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000018
penultimate_layer.0.bias: grad mean = 0.000335
output_layer.0.weight: grad mean = 0.002008
output_layer.0.bias: grad mean = 0.009475
[correct] no_actions is False
task_labels:  tensor([2, 1, 1, 0, 2, 2, 2, 1, 1, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 1, 1, 1, 2,
        0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 1, 1, 1, 1, 2, 2, 1, 0, 2, 2, 2,
        0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 1, 1, 0, 1, 2, 1, 0, 0, 2, 1, 2, 0, 2, 2,
        1, 1, 2, 0, 1, 1, 2, 0, 2, 2, 0, 0, 1, 1, 2, 0, 1, 0, 1, 1, 1, 2, 1, 2,
        2, 1, 0, 1, 1, 2, 1, 1, 2, 2, 1, 0, 0, 1, 0, 0, 0, 1, 1, 2, 2, 2, 1, 1,
        1, 1, 2, 2, 0, 2, 0, 1]) , task_preds: tensor([[-0.0314,  0.0515, -0.0882],
        [ 0.0367, -0.0188, -0.0248],
        [ 0.0049, -0.0248, -0.0406],
        [-0.0305,  0.0507, -0.0851],
        [-0.0340,  0.0408, -0.0806],
        [ 0.0128,  0.0043, -0.0014],
        [ 0.0200, -0.0208,  0.0189],
        [ 0.0013, -0.0372,  0.0635],
        [ 0.0485, -0.0020, -0.0717],
        [-0.0156, -0.0323,  0.0748],
        [ 0.0587, -0.0327,  0.0511],
        [-0.0218,  0.0617, -0.0794],
        [-0.0005,  0.0176, -0.0790],
        [ 0.0225, -0.0678,  0.0911],
        [ 0.0167, -0.0172, -0.0366],
        [-0.0169,  0.0255, -0.0813],
        [-0.0330,  0.0450, -0.0796],
        [ 0.0431, -0.0497,  0.0492],
        [-0.0335,  0.0504, -0.0750],
        [-0.0271, -0.0239,  0.1307],
        [-0.0070, -0.0441,  0.0036],
        [-0.0294,  0.0432, -0.0728],
        [ 0.0141, -0.0330, -0.0881],
        [-0.0189,  0.0531, -0.0765],
        [-0.0177,  0.0333, -0.0816],
        [-0.0075, -0.0402, -0.0601],
        [-0.0212,  0.0425, -0.0998],
        [ 0.0449, -0.0370, -0.0491],
        [ 0.0177, -0.0588,  0.0762],
        [ 0.0380, -0.0142, -0.0412],
        [ 0.0206, -0.0023, -0.0309],
        [-0.0186, -0.0670,  0.0774],
        [ 0.0277, -0.0347, -0.0349],
        [-0.0231,  0.0444, -0.0842],
        [ 0.0102, -0.0207, -0.0473],
        [ 0.0042, -0.0256, -0.0244],
        [-0.0052,  0.0322, -0.0718],
        [-0.0302,  0.0488, -0.0993],
        [-0.0147, -0.0304,  0.1271],
        [-0.0152,  0.0075,  0.0626],
        [-0.0171,  0.0490, -0.0750],
        [-0.0335,  0.0504, -0.0750],
        [-0.0397,  0.0466, -0.0863],
        [-0.0371,  0.0514, -0.0871],
        [-0.0007,  0.0178, -0.0883],
        [-0.0573, -0.0415,  0.0372],
        [ 0.0335, -0.0315,  0.0954],
        [-0.0108, -0.0168,  0.0930],
        [ 0.0067,  0.0047, -0.1029],
        [-0.0124, -0.0408,  0.0787],
        [-0.0044,  0.0265, -0.0483],
        [-0.0296,  0.0462, -0.0845],
        [-0.0195, -0.0472,  0.0701],
        [-0.0086, -0.0556,  0.0844],
        [-0.0211,  0.0560, -0.0724],
        [-0.0141, -0.0328,  0.0084],
        [-0.0206, -0.0106,  0.0683],
        [ 0.0352, -0.0451,  0.0593],
        [-0.0338,  0.0516, -0.0993],
        [-0.0144,  0.0534, -0.0866],
        [-0.0190,  0.0394, -0.0766],
        [-0.0355,  0.0458, -0.0836],
        [-0.0174,  0.0512, -0.0814],
        [ 0.0090, -0.0317, -0.0040],
        [ 0.0199, -0.0341,  0.0238],
        [ 0.0155,  0.0264, -0.0485],
        [-0.0051, -0.0294,  0.1127],
        [-0.0184,  0.0414, -0.0841],
        [ 0.0153, -0.0392,  0.0537],
        [ 0.0196, -0.0745,  0.0866],
        [-0.0326,  0.0523, -0.0754],
        [ 0.0171, -0.0502,  0.0497],
        [ 0.0068, -0.0153,  0.0072],
        [-0.0144,  0.0594, -0.0858],
        [ 0.0102, -0.0303,  0.0646],
        [-0.0344,  0.0525, -0.0863],
        [-0.0308,  0.0444, -0.0846],
        [-0.0310,  0.0391, -0.0836],
        [ 0.0126,  0.0066, -0.0461],
        [ 0.0619, -0.0568,  0.0397],
        [-0.0358,  0.0373, -0.0837],
        [-0.0304,  0.0479, -0.0761],
        [-0.0131, -0.0673,  0.0922],
        [-0.0242,  0.0599, -0.0733],
        [-0.0166, -0.0185,  0.0637],
        [-0.0079,  0.0275, -0.0947],
        [-0.0177,  0.0280, -0.0716],
        [ 0.0104,  0.0129, -0.0818],
        [-0.0011,  0.0117, -0.0801],
        [ 0.0341, -0.0537,  0.0680],
        [ 0.0133, -0.0083, -0.0493],
        [ 0.0264, -0.0146, -0.0497],
        [ 0.0226, -0.0594,  0.0137],
        [ 0.0383, -0.0376, -0.0123],
        [-0.0276,  0.0358, -0.0851],
        [ 0.0161, -0.0089, -0.0760],
        [ 0.0327, -0.0581,  0.0649],
        [-0.0075, -0.0496,  0.0814],
        [-0.0298,  0.0494, -0.0723],
        [-0.0189,  0.0468, -0.0926],
        [ 0.0140, -0.0213,  0.0397],
        [-0.0229, -0.0641,  0.0950],
        [ 0.0265, -0.0582,  0.1029],
        [ 0.0483, -0.0316,  0.0600],
        [-0.0126, -0.0585,  0.0886],
        [ 0.0050,  0.0191, -0.1133],
        [-0.0232,  0.0445, -0.0867],
        [ 0.0047, -0.0326,  0.0594],
        [-0.0191,  0.0421, -0.0759],
        [ 0.0159,  0.0335, -0.0560],
        [ 0.0462, -0.0127,  0.0012],
        [ 0.0067, -0.0127, -0.0133],
        [ 0.0154,  0.0241, -0.0589],
        [-0.0311,  0.0289, -0.0795],
        [-0.0401,  0.0474, -0.0759],
        [-0.0036, -0.0391, -0.0637],
        [-0.0267,  0.0472, -0.0900],
        [-0.0342,  0.0395, -0.0783],
        [-0.0276,  0.0466, -0.0891],
        [ 0.0438, -0.0185, -0.0764],
        [-0.0181, -0.0573,  0.0722],
        [ 0.0279,  0.0128, -0.0898],
        [-0.0291,  0.0443, -0.0828],
        [-0.0393, -0.0371,  0.0793],
        [-0.0025,  0.0353, -0.0852],
        [ 0.0300, -0.0276, -0.0534],
        [ 0.0307,  0.0156, -0.0506],
        [ 0.0016,  0.0146, -0.0693]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000030
penultimate_layer.0.bias: grad mean = 0.001119
output_layer.0.weight: grad mean = 0.003508
output_layer.0.bias: grad mean = 0.038425
[correct] no_actions is False
task_labels:  tensor([2, 1, 2, 1, 0, 1, 2, 0, 0, 0, 0, 2, 1, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2,
        1, 0, 1, 2, 1, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 2, 0, 1, 1, 2, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 2, 1, 0, 1, 2, 1, 1, 1,
        2, 1, 2, 2, 2, 1, 2, 0, 1, 0, 1, 0, 2, 2, 2, 1, 2, 1, 0, 0, 1, 0, 1, 1,
        1, 2, 2, 2, 1, 0, 2, 1, 0, 0, 2, 0, 1, 1, 2, 2, 1, 1, 0, 0, 0, 1, 2, 1,
        0, 1, 1, 0, 1, 1, 1, 0]) , task_preds: tensor([[-0.0266,  0.0446, -0.0801],
        [-0.0084,  0.0446, -0.0623],
        [ 0.0126, -0.0174,  0.0027],
        [ 0.0268, -0.0511,  0.0647],
        [-0.0367,  0.0484, -0.0789],
        [-0.0221,  0.0521, -0.0843],
        [ 0.0335, -0.0402, -0.0164],
        [ 0.0313, -0.0634, -0.0041],
        [ 0.0363, -0.0343, -0.0636],
        [ 0.0428, -0.0017, -0.0568],
        [-0.0157,  0.0130, -0.0816],
        [-0.0005, -0.0473,  0.1025],
        [ 0.0045, -0.0459, -0.0690],
        [ 0.0099, -0.0533,  0.0819],
        [ 0.0302,  0.0087, -0.0592],
        [ 0.0154, -0.0385,  0.0157],
        [-0.0332,  0.0377, -0.0691],
        [-0.0228,  0.0461, -0.0909],
        [-0.0185, -0.0416,  0.0954],
        [-0.0012, -0.0351, -0.0271],
        [ 0.0393, -0.0296, -0.0299],
        [ 0.0285,  0.0037, -0.0654],
        [-0.0391,  0.0550, -0.0705],
        [ 0.0425, -0.0545,  0.0713],
        [-0.0377,  0.0436, -0.0699],
        [-0.0079, -0.0502,  0.0846],
        [-0.0003, -0.0320,  0.1069],
        [ 0.0157,  0.0044, -0.0246],
        [ 0.0393, -0.0482, -0.0301],
        [ 0.0178, -0.0289,  0.0041],
        [ 0.0095, -0.0400,  0.0120],
        [-0.0296,  0.0447, -0.0724],
        [ 0.0099, -0.0216, -0.0668],
        [-0.0118,  0.0180, -0.0685],
        [-0.0008, -0.0007, -0.0409],
        [-0.0298,  0.0565, -0.0770],
        [-0.0100,  0.0037, -0.0749],
        [-0.0239,  0.0332, -0.0779],
        [ 0.0393, -0.0481, -0.0177],
        [ 0.0284, -0.0470, -0.0189],
        [-0.0356,  0.0441, -0.0687],
        [ 0.0021,  0.0270, -0.0890],
        [-0.0121, -0.0547,  0.0860],
        [ 0.0415, -0.0334, -0.0508],
        [ 0.0132, -0.0120, -0.0394],
        [ 0.0343, -0.0149, -0.0110],
        [ 0.0240,  0.0122, -0.0418],
        [-0.0356, -0.0490,  0.0597],
        [-0.0202,  0.0350, -0.0655],
        [ 0.0202, -0.0478,  0.1119],
        [ 0.0468, -0.0699, -0.0052],
        [-0.0338,  0.0412, -0.0756],
        [-0.0376,  0.0394, -0.1002],
        [ 0.0215, -0.0377,  0.1190],
        [ 0.0012, -0.0367,  0.1203],
        [-0.0281,  0.0422, -0.0685],
        [ 0.0406, -0.0456,  0.1111],
        [ 0.0045, -0.0537,  0.0976],
        [ 0.0316, -0.0036, -0.0675],
        [-0.0297,  0.0458, -0.0715],
        [ 0.0198, -0.0646,  0.0910],
        [ 0.0115,  0.0062, -0.0604],
        [-0.0269,  0.0430, -0.0841],
        [-0.0378,  0.0380, -0.0876],
        [ 0.0113,  0.0167, -0.0777],
        [ 0.0129,  0.0099, -0.0907],
        [ 0.0107, -0.0253, -0.0211],
        [-0.0024,  0.0102, -0.1026],
        [ 0.0239, -0.0630,  0.1173],
        [-0.0084, -0.0346, -0.0454],

MLP Fine-Tuning: task_loss = 1.0919:  47%|████▋     | 7/15 [00:57<00:24,  3.05s/it]
        [ 0.0196, -0.0572,  0.1188],
        [ 0.0113, -0.0475,  0.1015],
        [ 0.0233,  0.0049, -0.0601],
        [-0.0195, -0.0667,  0.0666],
        [-0.0231,  0.0091, -0.0260],
        [ 0.0222, -0.0662,  0.0775],
        [-0.0016, -0.0574,  0.0927],
        [-0.0130,  0.0077, -0.0706],
        [ 0.0192,  0.0123, -0.1162],
        [ 0.0056, -0.0430,  0.0990],
        [ 0.0403, -0.0283, -0.0901],
        [-0.0310,  0.0520, -0.0869],
        [ 0.0311, -0.0139,  0.0155],
        [-0.0114, -0.0221,  0.0991],
        [ 0.0071, -0.0549,  0.0449],
        [-0.0070,  0.0081, -0.0444],
        [-0.0590, -0.0183,  0.0700],
        [-0.0305,  0.0502, -0.0639],
        [-0.0087, -0.0100, -0.0522],
        [-0.0275,  0.0360, -0.0758],
        [ 0.0196,  0.0052, -0.0449],
        [ 0.0006,  0.0258, -0.0752],
        [-0.0202,  0.0324, -0.0793],
        [-0.0008,  0.0257, -0.0748],
        [ 0.0104, -0.0074, -0.0311],
        [-0.0195, -0.0083,  0.0870],
        [-0.0208,  0.0441, -0.0685],
        [ 0.0176,  0.0017, -0.0576],
        [-0.0039,  0.0321, -0.0808],
        [-0.0046, -0.0576,  0.0954],
        [-0.0172, -0.0308,  0.0041],
        [-0.0175,  0.0502, -0.0766],
        [-0.0245,  0.0336, -0.0803],
        [-0.0279,  0.0480, -0.0775],
        [ 0.0492, -0.0175,  0.0810],
        [ 0.0277, -0.0047, -0.0429],
        [-0.0295,  0.0463, -0.0826],
        [ 0.0080, -0.0501,  0.1173],
        [ 0.0181, -0.0722,  0.0877],
        [ 0.0201, -0.0416, -0.0121],
        [-0.0355, -0.0507,  0.0543],
        [ 0.0172,  0.0287, -0.0438],
        [-0.0034, -0.0773,  0.0641],
        [-0.0003,  0.0128, -0.0774],
        [-0.0136,  0.0267, -0.0939],
        [ 0.0099, -0.0003, -0.0316],
        [-0.0163, -0.0417,  0.1182]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000024
penultimate_layer.0.bias: grad mean = 0.000794
output_layer.0.weight: grad mean = 0.002466
output_layer.0.bias: grad mean = 0.024576
[correct] no_actions is False
task_labels:  tensor([2, 2, 2, 0, 2, 1, 1, 0, 1, 2, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0,
        0, 2, 1, 0, 1, 0, 1, 2, 2, 1, 1, 1, 2, 1, 2, 0, 1, 2, 1, 2, 2, 1, 1, 2,
        0, 0, 1, 2, 1, 1, 0, 2, 1, 0, 0, 2, 1, 0, 2, 1, 2, 2, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 2, 0, 0, 2, 0, 1, 1, 2, 2, 1, 0, 0, 1, 2, 1, 1, 2, 1, 2, 1, 0,
        0, 0, 2, 2, 0, 0, 0, 0, 2, 1, 1, 0, 2, 2, 1, 1, 0, 1, 2, 1, 0, 1, 0, 1,
        1, 0, 2, 1, 2, 1, 2, 0]) , task_preds: tensor([[-2.0746e-02, -1.4600e-02,  1.0724e-01],
        [-2.4746e-02,  4.6598e-02, -7.9100e-02],
        [-5.1177e-03, -6.3209e-02,  1.0422e-01],
        [-1.4500e-02,  4.5713e-02, -7.6703e-02],
        [-2.1161e-02,  3.1770e-02, -8.8348e-02],
        [ 5.4054e-03,  6.7431e-03, -4.6350e-02],
        [-2.8029e-02,  2.8107e-02, -7.7329e-02],
        [ 2.0119e-02, -8.9083e-02,  1.0714e-01],
        [-1.6729e-02,  3.0905e-02, -8.1316e-02],
        [ 2.1522e-02, -4.6956e-02,  5.9636e-02],
        [-3.3506e-02,  4.5763e-02, -8.2213e-02],
        [-2.3810e-02,  4.1066e-02, -9.2858e-02],
        [-2.2308e-02,  5.6787e-02, -6.9021e-02],
        [ 3.5271e-02, -5.5947e-03, -1.0471e-01],
        [ 3.5978e-02, -2.1901e-02, -2.2095e-02],
        [ 6.7150e-02, -6.6610e-02, -1.6654e-02],
        [-1.1576e-02, -1.7290e-02, -2.0402e-02],
        [ 2.8966e-02,  1.0845e-02, -7.7447e-02],
        [ 7.5634e-03,  1.0024e-02, -6.8164e-02],
        [-2.8622e-02,  4.6158e-02, -8.3944e-02],
        [-1.4360e-02,  4.0474e-02, -8.5795e-02],
        [ 1.6245e-02, -3.5877e-02, -3.0816e-02],
        [ 1.9605e-02, -2.1202e-02, -3.3895e-02],
        [-1.3376e-02,  3.8993e-02, -8.3971e-02],
        [-2.7827e-02,  4.4247e-02, -7.2833e-02],
        [-5.1695e-03, -3.7141e-02,  1.2759e-01],
        [-2.7827e-02,  4.4247e-02, -7.2833e-02],
        [-8.5753e-05,  8.3296e-03, -7.4900e-02],
        [-2.9429e-02,  5.3302e-02, -8.3933e-02],
        [-3.0908e-02,  4.2802e-02, -8.1375e-02],
        [ 3.2946e-02,  1.0754e-03, -8.5811e-02],
        [ 4.2547e-02, -2.1386e-02, -8.0360e-03],
        [ 2.0181e-02, -7.3387e-02,  8.8094e-02],
        [-1.2088e-02, -5.9030e-02,  9.2202e-02],
        [-4.7099e-02, -4.4198e-02,  1.0338e-01],
        [ 3.2451e-02, -7.0388e-02,  1.0231e-01],
        [-1.8206e-02,  3.3001e-02, -7.7334e-02],
        [-3.3091e-03, -2.2900e-02,  8.9051e-02],
        [ 2.0698e-02, -7.3337e-02,  9.8507e-02],
        [-2.7827e-02,  4.4247e-02, -7.2833e-02],
        [ 4.9324e-03, -7.6345e-02,  1.0992e-01],
        [ 2.9571e-02, -4.7173e-02,  6.8986e-02],
        [-1.3859e-02,  4.1289e-02, -8.9538e-02],
        [ 8.7237e-03, -3.3459e-02, -6.4378e-02],
        [-2.8201e-02,  4.1811e-02, -8.2082e-02],
        [-2.8722e-02,  3.8401e-02, -7.5628e-02],
        [ 3.6105e-02, -1.9066e-02, -6.2265e-02],
        [ 4.2941e-02, -6.0886e-02,  7.9968e-02],
        [-1.4110e-02,  3.7099e-02, -9.1697e-02],
        [-2.0458e-02, -2.2662e-02,  8.5722e-02],
        [ 2.1558e-02, -7.0318e-02,  3.1037e-02],
        [ 6.5583e-03, -4.3773e-02,  8.8862e-02],
        [ 2.2443e-02,  3.4563e-02, -4.8718e-02],
        [ 1.1776e-02,  5.0837e-04, -1.6299e-02],
        [ 4.4781e-02, -8.1806e-02,  8.4965e-02],
        [ 3.3582e-02, -5.8051e-02,  8.0277e-02],
        [-4.7147e-04,  3.0103e-02, -8.1982e-02],
        [ 1.2103e-02, -5.9624e-02,  8.8422e-02],
        [-1.9737e-02,  4.6634e-02, -8.7479e-02],
        [ 1.8381e-02, -3.3977e-02, -4.5356e-02],
        [-2.0652e-02, -4.9678e-02,  1.4946e-02],
        [ 3.4253e-02, -6.9252e-02,  9.1905e-02],
        [-1.7829e-02,  4.0572e-02, -8.0706e-02],
        [-2.8295e-02,  5.2981e-02, -8.0520e-02],
        [ 1.5181e-02, -4.5652e-02,  1.0091e-01],
        [ 2.6440e-02, -1.4733e-02, -2.0481e-02],
        [-9.1370e-03,  4.4803e-02, -8.0001e-02],
        [-1.8796e-02,  4.4684e-02, -9.3637e-02],
        [-2.2668e-02, -5.7392e-02,  1.3194e-01],
        [ 8.0708e-03, -8.1259e-02,  6.3112e-02],
        [-1.9418e-02,  3.8524e-02, -8.2546e-02],
        [-2.4031e-03, -3.8311e-02,  1.0685e-01],
        [ 5.0198e-02, -7.3869e-02,  5.3739e-02],
        [ 2.5034e-03,  2.7877e-02, -4.7659e-02],
        [-1.3378e-02,  4.6640e-02, -8.3594e-02],
        [ 2.5034e-03, -7.5901e-02,  9.2163e-02],
        [ 3.3525e-02, -4.7178e-02, -2.4986e-02],
        [-2.1008e-02,  4.1918e-02, -7.0942e-02],
        [-1.4540e-02, -7.1898e-02,  1.1046e-01],
        [ 3.8072e-02, -5.9725e-02, -1.0487e-02],
        [-3.1580e-02,  4.6307e-02, -7.3644e-02],
        [-1.0355e-02,  3.9936e-02, -8.2483e-02],
        [ 2.1283e-02, -4.3818e-02,  1.2818e-01],
        [ 1.9523e-02, -5.3950e-02,  1.2173e-01],
        [ 9.2302e-03, -3.6101e-02, -2.9405e-02],
        [ 1.3457e-02, -4.0351e-03, -9.8263e-02],
        [-2.5174e-02,  5.5169e-02, -7.8257e-02],
        [-1.0546e-02,  3.1934e-02, -9.5133e-02],
        [ 2.3630e-02, -1.9765e-02, -1.4740e-02],
        [-1.0082e-02, -5.6297e-02,  1.2462e-01],
        [-2.2090e-02,  3.0764e-02, -7.8363e-02],
        [ 1.3996e-02, -4.0325e-02,  1.2779e-01],
        [ 6.7593e-03, -3.7200e-02,  1.0309e-01],
        [-1.4517e-02, -3.2860e-02,  8.3031e-03],
        [-1.6299e-02,  4.3397e-02, -8.5986e-02],
        [ 2.3338e-02, -4.8781e-02,  4.0976e-02],
        [-1.2968e-02,  3.2975e-02, -7.4401e-02],
        [ 5.2371e-02, -5.0208e-02, -3.8949e-02],
        [-2.4297e-02, -5.8346e-02,  1.3067e-01],
        [-8.2689e-04, -4.5164e-02, -2.5201e-02],
        [-2.0107e-02,  4.0935e-02, -7.2280e-02],
        [ 4.1137e-02, -1.7047e-02, -2.6735e-03],
        [-2.7919e-02,  4.2629e-02, -7.9044e-02],
        [ 2.8000e-02,  8.1089e-03, -4.0728e-02],
        [-1.6714e-02,  5.5613e-02, -7.6540e-02],
        [-8.8905e-03,  4.5550e-02, -9.5475e-02],
        [ 8.8558e-03, -5.9256e-03, -6.5445e-02],
        [-2.1911e-02,  3.9572e-02, -8.4332e-02],
        [-3.8805e-03,  1.2174e-02, -4.6106e-02],
        [-1.8185e-02,  3.3450e-02, -7.5514e-02],
        [ 2.1097e-02, -5.0560e-02,  1.0335e-01],
        [ 7.1044e-03,  1.1315e-02, -4.6245e-02],
        [ 3.4537e-02, -8.4291e-02,  1.0467e-01],
        [-5.9874e-03, -3.8024e-02, -4.3410e-02],
        [-5.6521e-02, -2.6132e-02,  7.2609e-02],
        [-2.5794e-02,  4.2620e-02, -7.0985e-02],
        [ 7.2118e-03, -5.4394e-02,  9.9091e-02],
        [-4.9491e-03,  4.3180e-02, -6.4372e-02],
        [ 4.9884e-02, -2.2669e-02,  8.7758e-02],
        [-7.1569e-03,  3.1492e-02, -1.0726e-01],
        [ 5.8745e-03,  2.4628e-02, -9.0216e-02],
        [ 1.6423e-02, -7.1880e-02,  9.6956e-02],
        [ 3.0969e-02, -4.2170e-02,  5.0722e-02],
        [ 1.8363e-02, -1.4499e-02, -1.0239e-04],
        [-3.4273e-02,  4.0638e-02, -8.4167e-02],
        [-5.5701e-03, -4.5114e-02,  1.2984e-01],
        [-2.4333e-02, -7.7914e-02,  1.1691e-01],
        [-1.2848e-02, -5.3689e-02,  9.8039e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000030
penultimate_layer.0.bias: grad mean = 0.000772
output_layer.0.weight: grad mean = 0.003430
output_layer.0.bias: grad mean = 0.025915
[correct] no_actions is False
task_labels:  tensor([1, 0, 1, 2, 0, 1, 0, 1, 2, 1, 1, 0, 0, 2, 1, 0, 2, 0, 1, 1, 0, 0, 0, 1,
        2, 2, 1, 2, 2, 2, 0, 1, 1, 2, 2, 1, 0, 1, 2, 2, 0, 0, 0, 0, 0, 2, 1, 2,
        0, 1, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2, 2, 0, 0, 1, 1, 1, 1, 2, 1, 2, 1, 0,
        1, 1, 0, 0, 2, 1, 1, 2, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 1, 2, 0, 0, 1, 1,
        1, 0, 2, 0, 2, 0, 1, 0, 2, 1, 2, 2, 0, 1, 1, 0, 0, 2, 1, 2, 1, 2, 0, 0,
        1, 0, 2, 0, 2, 1, 1, 2]) , task_preds: tensor([[-0.0191,  0.0365, -0.0813],
        [-0.0033, -0.0742,  0.0911],
        [-0.0194,  0.0384, -0.0961],
        [ 0.0030, -0.0493,  0.1498],
        [ 0.0464, -0.0596, -0.0099],
        [ 0.0102, -0.0364,  0.0895],
        [-0.0142,  0.0405, -0.0803],
        [ 0.0024, -0.0303, -0.0762],
        [-0.0279,  0.0351, -0.0730],
        [-0.0252,  0.0439, -0.0787],
        [ 0.0041, -0.0379,  0.0827],
        [-0.0153,  0.0524, -0.0843],
        [-0.0200,  0.0541, -0.0855],
        [-0.0384, -0.0499,  0.1244],
        [-0.0026, -0.0446,  0.0038],
        [-0.0072,  0.0289, -0.0814],
        [ 0.0467, -0.0653,  0.0219],
        [ 0.0003, -0.0243,  0.1438],
        [ 0.0362, -0.0506, -0.0338],
        [-0.0167,  0.0448, -0.0924],
        [-0.0003,  0.0137, -0.0970],
        [ 0.0021, -0.0605,  0.0839],
        [ 0.0331, -0.0243, -0.0465],
        [-0.0213,  0.0363, -0.0885],
        [-0.0101, -0.0271,  0.0022],
        [ 0.0330, -0.0567, -0.0105],
        [-0.0250,  0.0377, -0.0868],
        [-0.0003, -0.0541, -0.0487],
        [ 0.0421,  0.0077, -0.0760],
        [ 0.0074, -0.0615,  0.1346],
        [ 0.0113, -0.0527,  0.0806],
        [-0.0210,  0.0234, -0.0795],
        [-0.0197,  0.0368, -0.0805],
        [-0.0042, -0.0203,  0.0269],
        [-0.0057,  0.0181, -0.0844],
        [-0.0095,  0.0504, -0.0988],
        [ 0.0513, -0.0433, -0.0486],
        [ 0.0190, -0.0214, -0.0372],
        [ 0.0405, -0.0503, -0.0104],
        [-0.0209,  0.0389, -0.0827],
        [ 0.0014, -0.0604,  0.0782],
        [-0.0056,  0.0258, -0.0853],
        [ 0.0062, -0.0492,  0.1368],
        [-0.0319, -0.0452,  0.0937],
        [ 0.0136, -0.0365, -0.0546],
        [-0.0097,  0.0259, -0.1080],
        [-0.0189,  0.0356, -0.0806],
        [ 0.0190, -0.0188, -0.0070],
        [ 0.0233, -0.0426,  0.0556],
        [-0.0171,  0.0393, -0.0798],
        [ 0.0385, -0.0267, -0.0096],
        [ 0.0154, -0.0099, -0.0256],
        [-0.0038, -0.0380,  0.0995],
        [-0.0178,  0.0367, -0.0975],
        [ 0.0078, -0.0227, -0.0490],
        [-0.0220,  0.0447, -0.0810],
        [ 0.0181, -0.0250, -0.0384],
        [-0.0192,  0.0426, -0.0731],
        [-0.0106,  0.0331, -0.1003],
        [ 0.0195, -0.0071, -0.0382],
        [ 0.0019, -0.0027, -0.0263],
        [ 0.0299,  0.0053, -0.0710],
        [ 0.0183,  0.0182, -0.0699],
        [ 0.0236, -0.0752,  0.1312],
        [ 0.0171, -0.0414,  0.0741],
        [ 0.0078, -0.0074, -0.0418],
        [ 0.0025, -0.0058, -0.0686],
        [-0.0161, -0.0293,  0.0383],
        [ 0.0353, -0.0702,  0.0847],
        [ 0.0408, -0.0504, -0.0108],
        [ 0.0465, -0.0574, -0.0255],
        [ 0.0706, -0.0792,  0.0614],
        [-0.0136,  0.0395, -0.0942],
        [-0.0022,  0.0147, -0.0754],
        [ 0.0137, -0.0571,  0.1108],
        [ 0.0336, -0.1057,  0.0859],
        [ 0.0126, -0.0843,  0.0419],
        [-0.0134,  0.0285, -0.0840],
        [ 0.0496, -0.0475,  0.0763],
        [-0.0194, -0.0520,  0.1117],
        [-0.0150,  0.0384, -0.0832],
        [-0.0215,  0.0433, -0.0792],
        [-0.0272,  0.0258, -0.0797],
        [ 0.0244, -0.0622,  0.1029],
        [-0.0219, -0.0838,  0.1058],
        [ 0.0107, -0.0056, -0.0946],
        [-0.0015,  0.0444, -0.0908],
        [ 0.0831, -0.0565,  0.0462],
        [-0.0169, -0.0431,  0.0249],
        [ 0.0279,  0.0195, -0.0506],
        [-0.0100, -0.0227,  0.0855],
        [ 0.0173, -0.0011, -0.0730],
        [ 0.0587, -0.0556, -0.0762],
        [-0.0184,  0.0336, -0.0775],
        [-0.0176,  0.0396, -0.0774],
        [-0.0158,  0.0444, -0.0881],
        [-0.0066,  0.0316, -0.0953],
        [ 0.0401,  0.0017, -0.0613],
        [ 0.0133, -0.0803,  0.1225],
        [ 0.0299, -0.0379,  0.0031],
        [ 0.0269, -0.0306, -0.0334],
        [-0.0091,  0.0140, -0.0823],
        [-0.0067,  0.0177, -0.0719],
        [ 0.0386, -0.0560,  0.0064],
        [-0.0213,  0.0386, -0.0911],
        [ 0.0187, -0.0493,  0.0616],
        [ 0.0083, -0.0085, -0.0769],
        [ 0.0013, -0.0781,  0.1233],
        [-0.0141,  0.0379, -0.0860],
        [ 0.0201, -0.0458, -0.0769],
        [ 0.0179, -0.0072, -0.0565],
        [ 0.0387, -0.0231,  0.0195],
        [-0.0145,  0.0455, -0.0813],
        [ 0.0156, -0.0517,  0.0120],
        [ 0.0065,  0.0180, -0.0906],
        [-0.0057, -0.0495,  0.0251],
        [-0.0138,  0.0539, -0.0927],
        [-0.0075,  0.0454, -0.0783],
        [ 0.0229, -0.0882,  0.0993],
        [-0.0060,  0.0320, -0.0771],
        [ 0.0339, -0.0210, -0.0208],
        [-0.0110,  0.0277, -0.0838],
        [-0.0184,  0.0407, -0.0802],
        [ 0.0241, -0.0875,  0.1177],
        [-0.0240,  0.0236, -0.0901],
        [ 0.0423, -0.0228, -0.0092],
        [ 0.0279,  0.0086, -0.1215],
        [-0.0004, -0.0442,  0.1152]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000028
penultimate_layer.0.bias: grad mean = 0.000460
output_layer.0.weight: grad mean = 0.003585
output_layer.0.bias: grad mean = 0.013792
[correct] no_actions is False
task_labels:  tensor([0, 2, 1, 2, 1, 0, 0, 2, 0, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2,
        0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 0, 0, 1, 1, 0, 1, 2, 0, 1, 1,
        1, 1, 2, 1, 2, 1, 0, 2, 0, 1, 1, 0, 1, 2, 0, 2, 1, 2, 2, 0, 1, 0, 2, 2,
        1, 0, 2, 1, 2, 0, 0, 0, 1, 2, 0, 1, 1, 2, 1, 0, 1, 1, 0, 2, 2, 0, 1, 1,
        1, 2, 0, 2, 2, 1, 2, 0, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 0, 2, 1, 0, 0, 0,
        1, 2, 0, 1, 0, 0, 1, 1]) , task_preds: tensor([[-0.0082,  0.0483, -0.0871],
        [ 0.0388, -0.0495, -0.0269],
        [-0.0085, -0.0737,  0.0880],
        [ 0.0170, -0.0517,  0.1405],
        [ 0.0145,  0.0013, -0.0842],
        [ 0.0318, -0.0513,  0.0062],
        [ 0.0061,  0.0211, -0.0976],
        [ 0.0465, -0.0731,  0.0933],
        [ 0.0446, -0.0811,  0.0050],
        [-0.0199,  0.0330, -0.0861],
        [-0.0133,  0.0344, -0.0884],
        [-0.0263, -0.0459,  0.1471],
        [ 0.0269, -0.0513, -0.0072],
        [ 0.0330, -0.0376,  0.0928],
        [ 0.0096, -0.0694,  0.0628],
        [ 0.0187,  0.0046, -0.0337],
        [ 0.0074, -0.0550,  0.1042],
        [-0.0230,  0.0401, -0.0971],
        [ 0.0188, -0.0423, -0.0214],
        [ 0.0007, -0.0965,  0.1476],
        [ 0.0370, -0.0411, -0.0481],
        [-0.0156,  0.0465, -0.0900],
        [ 0.0140, -0.0407,  0.0172],
        [-0.0064, -0.0604,  0.0521],
        [ 0.0207, -0.0293, -0.0113],
        [ 0.0168, -0.0465, -0.0211],
        [ 0.0275, -0.0096, -0.0531],
        [-0.0055, -0.0861,  0.1121],
        [-0.0082,  0.0335, -0.1063],
        [-0.0164,  0.0458, -0.0867],
        [-0.0158,  0.0440, -0.0891],
        [ 0.0084, -0.0478,  0.1164],
        [ 0.0049, -0.0771,  0.0577],
        [ 0.0450, -0.0212, -0.0176],
        [-0.0100,  0.0372, -0.0932],
        [ 0.0014, -0.0497,  0.1208],
        [ 0.0040,  0.0018, -0.0031],
        [-0.0151,  0.0377, -0.1083],
        [-0.0053,  0.0333, -0.1072],
        [-0.0141,  0.0335, -0.1055],
        [ 0.0074, -0.0655,  0.1142],
        [ 0.0044, -0.0525,  0.1392],
        [ 0.0416, -0.0177, -0.0413],
        [-0.0176,  0.0411, -0.0921],
        [ 0.0142, -0.0429,  0.0083],
        [ 0.0451, -0.0006, -0.0642],
        [ 0.0288, -0.0822,  0.0839],
        [ 0.0208,  0.0012, -0.0856],
        [ 0.0234, -0.0481, -0.0776],
        [-0.0137,  0.0445, -0.0995],
        [ 0.0372, -0.0820,  0.0940],
        [-0.0177,  0.0396, -0.0942],
        [-0.0107,  0.0414, -0.0876],
        [-0.0107,  0.0496, -0.0740],
        [ 0.0107,  0.0058, -0.0799],
        [ 0.0147, -0.0667,  0.0925],
        [-0.0184,  0.0409, -0.0851],
        [ 0.0157,  0.0229, -0.0998],
        [ 0.0324,  0.0180, -0.0540],
        [ 0.0754, -0.0858,  0.0650],
        [ 0.0002, -0.0449, -0.0049],
        [ 0.0502, -0.0612,  0.0833],
        [ 0.0250, -0.0757,  0.1388],
        [ 0.0232, -0.0099, -0.0387],
        [ 0.0139, -0.0406,  0.0021],
        [ 0.0186, -0.0368, -0.0341],
        [-0.0048,  0.0447, -0.0872],
        [-0.0012,  0.0417, -0.0862],
        [ 0.0004,  0.0477, -0.0962],
        [-0.0106,  0.0349, -0.0895],
        [ 0.0210, -0.0476,  0.0979],
        [-0.0084,  0.0406, -0.0910],
        [-0.0326, -0.0792,  0.1335],
        [ 0.0397,  0.0071, -0.0853],
        [ 0.0224, -0.0503,  0.0177],
        [ 0.0238, -0.0023, -0.0459],
        [ 0.0124, -0.0464, -0.0320],
        [ 0.0196, -0.0722,  0.0974],
        [-0.0088, -0.0651,  0.1095],
        [ 0.0054,  0.0231, -0.1032],
        [ 0.0259, -0.0709,  0.0973],
        [-0.0025,  0.0462, -0.0863],
        [ 0.0586, -0.0120, -0.0614],
        [-0.0086, -0.0509,  0.1400],
        [ 0.0471, -0.0314, -0.0273],
        [ 0.0059,  0.0068, -0.0504],
        [ 0.0268, -0.0469,  0.0705],
        [ 0.0439, -0.0616,  0.0080],
        [ 0.0442, -0.0023, -0.0879],
        [-0.0163,  0.0436, -0.0860],
        [ 0.0275, -0.0940,  0.1229],
        [ 0.0115, -0.0045, -0.0874],
        [-0.0073, -0.0412,  0.0115],
        [ 0.0641, -0.0596, -0.0399],
        [-0.0198,  0.0380, -0.0936],
        [-0.0007, -0.0013, -0.0818],
        [ 0.0133, -0.0262,  0.0112],
        [-0.0080, -0.0518,  0.1487],
        [ 0.0577, -0.0676,  0.0987],
        [-0.0232,  0.0354, -0.0792],
        [ 0.0104, -0.0046, -0.0637],
        [ 0.0421, -0.0557, -0.0361],
        [-0.0541, -0.0360,  0.0947],
        [-0.0027,  0.0384, -0.0962],
        [ 0.0252, -0.0061, -0.0704],
        [ 0.0377, -0.0792,  0.1051],
        [ 0.0418, -0.0542, -0.0250],
        [-0.0112,  0.0479, -0.0958],
        [ 0.0531, -0.0353, -0.0660],
        [-0.0068,  0.0410, -0.0958],
        [-0.0244, -0.0402,  0.0888],
        [ 0.0543, -0.0024, -0.0742],
        [ 0.0259, -0.0944,  0.1046],
        [ 0.0004,  0.0108, -0.0813],
        [ 0.0312, -0.0281, -0.0383],
        [-0.0098,  0.0361, -0.0761],
        [ 0.0084, -0.0169, -0.0632],
        [ 0.0855, -0.0709,  0.0174],
        [ 0.0194, -0.0023, -0.0483],
        [-0.0205,  0.0399, -0.0960],
        [-0.0111, -0.0552,  0.1291],
        [-0.0180, -0.0572,  0.1176],
        [ 0.0289, -0.0023, -0.0825],
        [ 0.0011,  0.0472, -0.1050],
        [-0.0044, -0.0677,  0.1055],
        [-0.0101,  0.0470, -0.0954],
        [-0.0030, -0.0338,  0.0199],
        [ 0.0187, -0.0542,  0.0440]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000023
penultimate_layer.0.bias: grad mean = 0.000398
output_layer.0.weight: grad mean = 0.002855
output_layer.0.bias: grad mean = 0.011990
[correct] no_actions is False
task_labels:  tensor([0, 0, 2, 0, 1, 0, 1, 2, 1, 0, 1, 0, 2, 1, 0, 1, 1, 0, 1, 2, 0, 1, 1, 0,
        2, 1, 0, 0, 2, 0, 0, 2, 1, 1, 1, 1, 0, 1, 2, 1, 0, 2, 2, 0, 2, 0, 1, 2,
        1, 0, 2, 0, 1, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 2, 1, 1, 2, 2, 0, 2, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0, 0, 2, 1, 1, 1,
        0, 1, 0, 1, 2, 0, 0, 2, 0, 0, 1, 2, 1, 2, 2, 1, 0, 0, 2, 2, 1, 1, 1, 0,
        2, 2, 1, 2, 2, 2, 2, 1]) , task_preds: tensor([[ 0.0066, -0.0720,  0.0885],
        [ 0.0339, -0.0656,  0.0522],
        [ 0.0188, -0.0082, -0.1008],
        [ 0.0214,  0.0072, -0.0492],
        [-0.0140, -0.0490,  0.0280],
        [ 0.0244, -0.0112, -0.0603],
        [ 0.0293, -0.0899,  0.1345],
        [-0.0077,  0.0443, -0.1138],
        [-0.0196,  0.0400, -0.0901],
        [ 0.0548, -0.0216, -0.0343],
        [-0.0391, -0.0671,  0.0826],
        [ 0.0409, -0.0858,  0.1075],
        [ 0.0205, -0.0851,  0.0990],
        [-0.0167,  0.0231, -0.0939],
        [-0.0052,  0.0372, -0.0959],
        [ 0.0003,  0.0381, -0.1033],
        [-0.0095,  0.0389, -0.0885],
        [-0.0138,  0.0398, -0.0941],
        [ 0.0469, -0.0029, -0.0972],
        [-0.0042,  0.0446, -0.1026],
        [ 0.0396, -0.0810,  0.0524],
        [-0.0109,  0.0427, -0.0916],
        [ 0.0515, -0.0721,  0.0291],
        [ 0.0135,  0.0128, -0.1074],
        [-0.0259, -0.0802,  0.1014],
        [ 0.0216, -0.0130, -0.0709],
        [-0.0133,  0.0441, -0.0998],
        [-0.0005,  0.0396, -0.0862],
        [ 0.0372, -0.0559,  0.0628],
        [-0.0127,  0.0426, -0.0891],
        [-0.0093,  0.0377, -0.0939],
        [ 0.0162,  0.0037, -0.0891],
        [-0.0047, -0.0264, -0.0157],
        [-0.0032, -0.0231, -0.0925],
        [ 0.0023,  0.0339, -0.0995],
        [ 0.0405, -0.0242, -0.0422],
        [ 0.0178, -0.1242,  0.0932],
        [-0.0121,  0.0376, -0.0886],
        [-0.0123, -0.0598,  0.1106],
        [ 0.0305,  0.0182, -0.0765],
        [ 0.0242, -0.0184, -0.0848],
        [-0.0060,  0.0339, -0.0997],
        [-0.0389, -0.0671,  0.1223],
        [ 0.0611, -0.0523, -0.0230],
        [ 0.0298, -0.0463,  0.0438],
        [-0.0130,  0.0380, -0.1061],
        [ 0.0169,  0.0065, -0.0894],
        [-0.0127,  0.0426, -0.0891],
        [ 0.0049,  0.0522, -0.0987],
        [ 0.0008,  0.0219, -0.1111],
        [-0.0041, -0.0938,  0.1285],
        [ 0.0163,  0.0177, -0.1081],
        [-0.0085,  0.0441, -0.0876],
        [ 0.0031, -0.0853,  0.0991],
        [ 0.0938, -0.0649,  0.0619],
        [-0.0351, -0.0670,  0.0852],
        [ 0.0105,  0.0458, -0.0936],
        [-0.0156,  0.0408, -0.0874],
        [ 0.0283, -0.0208, -0.0339],
        [-0.0086,  0.0475, -0.0888],
        [ 0.0595, -0.0900,  0.0656],
        [-0.0080,  0.0434, -0.0952],
        [ 0.0547, -0.0449, -0.0268],
        [ 0.0070,  0.0253, -0.0901],
        [ 0.0187,  0.0163, -0.1013],
        [ 0.0208, -0.0877,  0.1144],
        [ 0.0361, -0.0644,  0.0591],
        [-0.0127,  0.0426, -0.0891],
        [-0.0069, -0.0214,  0.0949],
        [ 0.0170, -0.0321,  0.0719],
        [ 0.0231, -0.0652,  0.0368],
        [-0.0137,  0.0407, -0.0879],
        [ 0.0093,  0.0036, -0.0742],
        [-0.0019,  0.0302, -0.0833],
        [ 0.0500, -0.0733,  0.0295],
        [ 0.0531, -0.0645, -0.0234],
        [ 0.0229, -0.0025, -0.0211],
        [-0.0094,  0.0399, -0.0900],
        [ 0.0233, -0.0792,  0.1485],
        [ 0.0446, -0.0580, -0.0132],
        [-0.0104,  0.0120, -0.0845],
        [-0.0036,  0.0295, -0.0954],
        [ 0.0221,  0.0101, -0.0279],
        [ 0.0056, -0.0648,  0.0338],
        [-0.0052,  0.0269, -0.0918],
        [-0.0127,  0.0426, -0.0891],
        [-0.0116,  0.0476, -0.1046],
        [ 0.0412, -0.0004, -0.0448],
        [ 0.0239, -0.0262, -0.0362],
        [-0.0099,  0.0397, -0.0931],
        [-0.0056,  0.0396, -0.0951],
        [ 0.0599, -0.0371, -0.0987],
        [ 0.0103, -0.0720,  0.1462],
        [ 0.0008,  0.0458, -0.0947],
        [-0.0107,  0.0378, -0.1058],
        [ 0.0051,  0.0234, -0.1047],
        [ 0.0057,  0.0313, -0.1013],
        [ 0.0093, -0.0864,  0.0845],
        [-0.0022,  0.0267, -0.0932],
        [ 0.0169, -0.0035, -0.0492],
        [-0.0027,  0.0386, -0.0965],
        [-0.0095,  0.0328, -0.0873],
        [-0.0107,  0.0413, -0.0907],
        [-0.0266, -0.0694,  0.1114],
        [ 0.0430, -0.0310, -0.0491],
        [-0.0127,  0.0426, -0.0891],
        [-0.0101,  0.0290, -0.0963],
        [-0.0170,  0.0448, -0.0880],
        [-0.0207,  0.0351, -0.1147],
        [ 0.0031,  0.0156, -0.0916],
        [ 0.0475, -0.0627,  0.0128],
        [ 0.0114, -0.0045, -0.0487],
        [-0.0139, -0.0379,  0.0996],
        [ 0.0279, -0.0086, -0.1095],
        [ 0.0109, -0.0580, -0.0540],
        [ 0.0279, -0.0719,  0.1126],
        [-0.0076,  0.0285, -0.0936],
        [ 0.0488, -0.0285, -0.0090],
        [-0.0026,  0.0307, -0.0831],
        [ 0.0653, -0.0453, -0.0039],
        [ 0.0377, -0.0646, -0.0049],
        [-0.0020, -0.0921,  0.1484],
        [-0.0127,  0.0426, -0.0891],
        [-0.0116,  0.0419, -0.0874],
        [-0.0026,  0.0539, -0.0914],
        [-0.0002,  0.0448, -0.0928],
        [-0.0140, -0.0711,  0.0562],
        [ 0.0332, -0.0008, -0.1067]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000024
penultimate_layer.0.bias: grad mean = 0.000617
output_layer.0.weight: grad mean = 0.002785

MLP Fine-Tuning: task_loss = 1.0922:  60%|██████    | 9/15 [00:59<00:12,  2.05s/it]
[correct] no_actions is False
task_labels:  tensor([2, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 0, 2, 0,
        0, 0, 1, 2, 2, 1, 0, 2, 2, 1, 1, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 2, 1, 0, 2, 1, 0, 1, 1, 0, 2, 2, 0, 0, 0, 2, 1, 1, 2, 2, 1, 1, 2, 1,
        1, 0, 1, 2, 1, 2, 0, 2, 0, 2, 0, 1, 0, 2, 2, 1, 1, 2, 1, 1, 1, 0, 2, 1,
        1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 0, 0, 0, 2, 1, 2, 0, 1,
        2, 0, 1, 1, 2, 2, 0, 1]) , task_preds: tensor([[ 0.0519, -0.0273, -0.0173],
        [ 0.0077, -0.0122, -0.0520],
        [ 0.0385, -0.0037, -0.0930],
        [ 0.0590, -0.0416,  0.1038],
        [-0.0041, -0.0742,  0.1169],
        [ 0.0277, -0.0584,  0.0228],
        [ 0.0437, -0.0093, -0.0684],
        [ 0.0190, -0.0929,  0.1325],
        [ 0.0454, -0.0483, -0.0481],
        [ 0.0587, -0.0891,  0.0401],
        [ 0.0329, -0.0519, -0.0024],
        [-0.0107,  0.0305, -0.0937],
        [ 0.0279, -0.0287, -0.0376],
        [ 0.0005, -0.0551,  0.0482],
        [ 0.0549, -0.0063, -0.0680],
        [ 0.0047,  0.0348, -0.1039],
        [ 0.0163,  0.0089, -0.0659],
        [ 0.0254, -0.0497, -0.0298],
        [ 0.0037, -0.0584,  0.1307],
        [ 0.0104, -0.0619, -0.0506],
        [-0.0034,  0.0358, -0.1113],
        [ 0.0299, -0.1119,  0.1279],
        [-0.0025,  0.0276, -0.1191],
        [ 0.0023,  0.0350, -0.1041],
        [-0.0029,  0.0378, -0.0969],
        [-0.0016,  0.0291, -0.1023],
        [ 0.0429, -0.0970,  0.1029],
        [-0.0144, -0.0554,  0.1244],
        [-0.0061, -0.0404,  0.0418],
        [ 0.0125, -0.0594,  0.0620],
        [ 0.0234, -0.0828,  0.1449],
        [ 0.0556, -0.0369, -0.0356],
        [ 0.0093, -0.0657,  0.1658],
        [ 0.0125, -0.0771,  0.0704],
        [ 0.0638, -0.0533, -0.0497],
        [ 0.0366, -0.0520, -0.0064],
        [ 0.0191, -0.0334, -0.0482],
        [ 0.0108,  0.0451, -0.1078],
        [-0.0132,  0.0383, -0.0956],
        [ 0.0124, -0.0767,  0.1503],
        [ 0.0224,  0.0152, -0.1144],
        [ 0.0264, -0.0826,  0.1039],
        [-0.0074, -0.1027,  0.1363],
        [ 0.0538, -0.0910,  0.0080],
        [ 0.0320, -0.0452, -0.0372],
        [-0.0064,  0.0193, -0.0958],
        [ 0.0713, -0.0577,  0.0237],
        [-0.0036,  0.0384, -0.1137],
        [ 0.0056,  0.0386, -0.1093],
        [ 0.0641, -0.0594,  0.0129],
        [-0.0032,  0.0434, -0.0817],
        [ 0.0492, -0.0605, -0.0251],
        [ 0.0210, -0.0453,  0.1247],
        [ 0.0271, -0.0156, -0.0740],
        [ 0.0641, -0.0669, -0.0357],
        [-0.0063,  0.0350, -0.1177],
        [ 0.0318,  0.0058, -0.0436],
        [ 0.0240, -0.1264,  0.0877],
        [ 0.0457, -0.0439,  0.0050],
        [ 0.0391, -0.0625, -0.0363],
        [-0.0058, -0.0696,  0.1455],
        [ 0.0028,  0.0355, -0.0872],
        [-0.0097,  0.0386, -0.0978],
        [ 0.0460, -0.0559, -0.0267],
        [-0.0049, -0.0566, -0.0092],
        [-0.0044,  0.0423, -0.1078],
        [ 0.0248, -0.0103, -0.1052],
        [-0.0138,  0.0328, -0.0871],
        [ 0.0232, -0.0529,  0.0831],
        [ 0.0288, -0.0893,  0.1451],
        [ 0.0406, -0.0662,  0.0472],
        [-0.0033,  0.0211, -0.0956],
        [-0.0061,  0.0409, -0.0946],
        [ 0.0084,  0.0127, -0.0922],
        [-0.0070,  0.0327, -0.0989],
        [-0.0061,  0.0410, -0.0945],
        [ 0.0168,  0.0435, -0.0976],
        [-0.0258, -0.0729,  0.1154],
        [ 0.0258, -0.0505,  0.0188],
        [ 0.0347,  0.0103, -0.0903],
        [ 0.0458,  0.0143, -0.0795],
        [ 0.0241, -0.0422, -0.0331],
        [ 0.0187,  0.0166, -0.1040],
        [ 0.0290, -0.0317, -0.0421],
        [ 0.0125, -0.0813,  0.1275],
        [ 0.0320, -0.0770,  0.0801],
        [ 0.0399, -0.0772,  0.1002],
        [-0.0044,  0.0451, -0.0954],
        [ 0.0083,  0.0324, -0.1046],
        [ 0.0041,  0.0146, -0.0823],
        [ 0.0023,  0.0392, -0.1016],
        [ 0.0412,  0.0137, -0.0585],
        [ 0.0302, -0.0484, -0.0320],
        [-0.0020,  0.0119, -0.0877],
        [-0.0060,  0.0402, -0.0947],
        [ 0.0106, -0.0743,  0.1229],
        [-0.0013, -0.0292, -0.0160],
        [ 0.0065,  0.0365, -0.1086],
        [-0.0071,  0.0361, -0.1016],
        [-0.0089,  0.0256, -0.0972],
        [-0.0021,  0.0286, -0.0837],
        [ 0.0255,  0.0124, -0.0900],
        [ 0.0047,  0.0387, -0.1020],
        [ 0.0256, -0.0482, -0.0213],
        [ 0.0122, -0.0467, -0.0093],
        [ 0.0269, -0.0016, -0.0586],
        [ 0.0323, -0.0244,  0.0373],
        [ 0.0261, -0.0908,  0.1082],
        [ 0.0178,  0.0075, -0.0524],
        [ 0.0523, -0.0697,  0.0038],
        [ 0.0247, -0.0900,  0.1218],
        [-0.0028,  0.0429, -0.1006],
        [ 0.0741, -0.0677, -0.0405],
        [ 0.0310, -0.1038,  0.1119],
        [ 0.0341,  0.0053, -0.0957],
        [-0.0006,  0.0336, -0.0837],
        [ 0.0191, -0.0008, -0.0287],
        [-0.0160, -0.0335,  0.1287],
        [ 0.0738, -0.0473, -0.0805],
        [ 0.0435, -0.0142, -0.0247],
        [ 0.0249, -0.0736,  0.0995],
        [ 0.0535, -0.0718,  0.0110],
        [-0.0129,  0.0355, -0.1072],
        [-0.0098,  0.0354, -0.1021],
        [-0.0032, -0.0509,  0.0161],
        [ 0.0171, -0.0781,  0.0313],
        [ 0.0052,  0.0364, -0.1166],
        [ 0.0005,  0.0417, -0.1050]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000032
penultimate_layer.0.bias: grad mean = 0.001013
output_layer.0.weight: grad mean = 0.003470
output_layer.0.bias: grad mean = 0.029804
[correct] no_actions is False
task_labels:  tensor([2, 1, 1, 1, 0, 2, 2, 1, 0, 1, 2, 1, 2, 0, 1, 2, 2, 0, 0, 2, 1, 0, 2, 1,
        0, 1, 2, 1, 1, 0, 0, 1, 1, 0, 2, 1, 1, 0, 2, 2, 2, 0, 2, 0, 2, 0, 2, 0,
        1, 1, 0, 1, 2, 2, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 2, 0, 1, 2, 0, 2, 1,
        0, 2, 0, 1, 1, 0, 2, 0, 2, 2, 1, 0, 1, 1, 0, 1, 0, 2, 0, 1, 2, 0, 1, 0,
        2, 0, 1, 0, 1, 1, 1, 1, 2, 1, 0, 2, 1, 1, 2, 0, 1, 1, 1, 2, 1, 2, 0, 0,
        0, 0, 0, 2, 2, 2, 2, 1]) , task_preds: tensor([[ 8.3312e-03,  2.9331e-02, -1.1018e-01],
        [-1.2035e-02, -7.8542e-02,  8.5353e-02],
        [-9.3906e-03,  4.9887e-02, -1.0905e-01],
        [ 3.7723e-02, -7.4879e-02,  1.1792e-01],
        [ 1.0489e-02,  2.9285e-02, -1.2074e-01],
        [ 1.2639e-02,  4.2131e-02, -9.9052e-02],
        [-8.6230e-03, -4.8246e-02,  8.8477e-02],
        [ 4.3783e-02, -4.2018e-03, -1.1488e-01],
        [ 4.7232e-02, -3.8104e-02, -4.3711e-02],
        [ 9.0301e-03, -4.6583e-02,  1.3070e-01],
        [ 5.5907e-02, -8.0953e-02,  3.3265e-02],
        [-4.9431e-04,  7.0370e-03, -1.1002e-01],
        [ 1.0885e-02,  3.6492e-02, -1.2471e-01],
        [ 7.4257e-03,  1.5002e-02, -9.5764e-02],
        [ 7.6914e-02, -4.3019e-02, -5.8082e-02],
        [ 5.5632e-02, -6.5446e-02, -7.1541e-03],
        [-6.8722e-04,  3.9677e-02, -9.9522e-02],
        [ 3.2359e-02, -7.5096e-03, -1.1172e-01],
        [ 2.4680e-03,  2.9833e-02, -9.7656e-02],
        [-6.8722e-04,  3.9677e-02, -9.9522e-02],
        [ 3.5732e-02,  1.3344e-02, -4.3406e-02],
        [ 6.9121e-02, -8.0399e-02,  1.5611e-02],
        [ 4.9322e-03, -8.8594e-02,  1.2846e-01],
        [ 4.4920e-04,  3.3779e-02, -1.0406e-01],
        [ 1.6904e-02,  2.8210e-02, -1.1063e-01],
        [-1.7968e-03,  3.4730e-02, -1.1637e-01],
        [ 3.4182e-02, -1.8872e-02, -3.9416e-02],
        [ 1.4996e-02, -2.8893e-02, -2.4390e-02],
        [ 1.4518e-02, -7.1842e-02,  7.3825e-02],
        [ 5.4132e-02, -1.1269e-01,  1.2136e-01],
        [ 3.3777e-03,  2.6089e-02, -1.1519e-01],
        [ 5.1661e-02, -6.2973e-02, -3.4982e-02],
        [ 1.7321e-02, -3.4969e-02,  8.3587e-02],
        [ 1.4421e-02,  2.7827e-02, -1.0059e-01],
        [-5.1425e-03,  3.5364e-02, -1.0987e-01],
        [ 2.9004e-02, -5.9494e-02,  5.2991e-02],
        [ 3.0888e-03,  3.6045e-02, -1.1057e-01],
        [ 9.1924e-02, -9.2911e-02, -9.2517e-03],
        [ 3.0301e-02, -7.0696e-02,  1.5598e-01],
        [ 6.6647e-03,  3.5179e-02, -1.1463e-01],
        [ 1.2229e-02,  1.0508e-02, -9.1075e-02],
        [-1.5947e-02, -6.6320e-02,  1.0447e-01],
        [-4.5644e-03, -5.1739e-02,  1.4245e-01],
        [ 1.7055e-03,  3.4214e-02, -1.0939e-01],
        [ 3.7194e-02, -6.7102e-02,  2.2621e-02],
        [-2.8789e-02, -6.7360e-02,  1.2310e-01],
        [-8.9010e-03, -7.8947e-02,  6.1332e-02],
        [ 4.0712e-02,  8.4002e-03, -1.1740e-01],
        [ 3.3094e-02, -5.5447e-02, -7.8982e-02],
        [-1.8334e-02, -5.9672e-02,  1.5718e-01],
        [ 3.9922e-02, -2.0663e-02, -3.2391e-02],
        [ 8.3336e-03, -4.8995e-02,  1.1632e-01],
        [ 2.2883e-02, -6.5670e-02,  1.5341e-01],
        [-6.8722e-04,  3.9677e-02, -9.9522e-02],
        [-9.9604e-04,  4.0634e-02, -1.1365e-01],
        [-4.1052e-03, -7.6492e-02,  1.1613e-01],
        [-1.1216e-02, -6.0970e-02,  1.6713e-01],
        [ 5.6293e-02, -3.9801e-02, -2.6524e-02],
        [ 4.8827e-02, -9.0931e-02,  5.5257e-02],
        [ 1.9216e-02,  2.1618e-02, -1.1494e-01],
        [-7.1068e-03, -4.6826e-02,  1.0411e-01],
        [ 2.5710e-02, -6.9593e-02,  1.1523e-01],
        [ 8.9008e-03,  2.8879e-02, -1.0949e-01],
        [ 1.2077e-02, -1.1009e-01,  9.2417e-02],
        [ 6.8982e-03, -2.8631e-02, -9.7085e-02],
        [ 8.9964e-03,  7.1982e-03, -9.1419e-02],
        [ 2.8432e-03,  3.1556e-02, -1.0554e-01],
        [ 5.1107e-03,  4.1535e-02, -1.1354e-01],
        [ 2.2222e-02, -4.0390e-02,  4.4783e-02],
        [ 4.8713e-02, -9.5619e-02,  1.1199e-01],
        [ 3.2450e-02, -1.0377e-03, -7.4123e-02],
        [ 6.0035e-03,  2.3910e-02, -1.0127e-01],
        [-6.8722e-04,  3.9677e-02, -9.9522e-02],
        [-2.7334e-02, -7.0356e-02,  1.4028e-01],
        [ 2.4075e-02,  2.1024e-02, -1.1809e-01],
        [ 9.1216e-03,  3.4099e-02, -1.1779e-01],
        [ 1.7760e-02,  9.6121e-03, -9.2654e-02],
        [ 5.8104e-02, -3.1313e-02, -3.5306e-02],
        [ 3.2092e-02, -9.2169e-02,  1.4937e-01],
        [ 6.8160e-03,  3.4206e-02, -1.0643e-01],
        [-6.8722e-04,  3.9677e-02, -9.9522e-02],
        [ 3.2625e-03,  3.3703e-02, -1.0690e-01],
        [-1.3296e-02,  1.8757e-02, -9.4011e-02],
        [ 3.6429e-02, -5.7899e-03, -9.5911e-02],
        [-3.8237e-03,  3.6072e-02, -1.0813e-01],
        [-3.0883e-03,  3.5054e-02, -1.0436e-01],
        [ 7.2352e-02, -7.1345e-02, -7.2225e-02],
        [ 3.6124e-02,  6.9118e-03, -7.2341e-02],
        [ 5.1373e-02, -4.5893e-02, -3.3731e-02],
        [ 3.1928e-02, -4.0380e-02,  1.2089e-02],
        [ 4.6539e-02, -7.7568e-03, -6.1257e-02],
        [-2.6873e-03,  4.9041e-02, -1.0623e-01],
        [ 2.6576e-02, -8.7004e-03, -9.7931e-02],
        [ 9.6327e-03,  2.3758e-02, -1.0344e-01],
        [ 5.8765e-02, -1.0100e-02, -9.4526e-02],
        [ 1.1034e-02,  4.2480e-02, -1.0734e-01],
        [ 1.5213e-02, -8.8972e-02,  1.2200e-01],
        [ 5.1582e-02, -5.7949e-03, -4.9769e-02],
        [ 1.2533e-02, -6.5320e-02,  1.4755e-01],
        [-1.2210e-03,  4.1014e-02, -1.1014e-01],
        [ 2.6904e-02, -5.5766e-02, -2.0205e-02],
        [-5.5821e-04,  3.9193e-02, -9.9524e-02],
        [ 1.1606e-02,  3.9227e-02, -9.5756e-02],
        [ 3.2206e-02, -8.8765e-02,  4.4536e-02],
        [-4.1069e-03, -9.8095e-02,  1.3536e-01],
        [ 9.1259e-03,  3.9892e-02, -1.1035e-01],
        [ 5.0195e-02,  6.6081e-03, -6.8608e-02],
        [ 1.5036e-02,  3.7490e-02, -1.0046e-01],
        [ 6.0834e-03, -4.4070e-02,  2.3495e-02],
        [ 1.9656e-02,  4.9187e-02, -1.0423e-01],
        [ 1.3056e-02,  3.1820e-02, -1.0460e-01],
        [ 8.7746e-03,  3.1816e-02, -1.0641e-01],
        [ 1.1109e-03,  3.4949e-02, -1.1619e-01],
        [ 4.2448e-02, -5.6972e-02,  3.0899e-02],
        [-1.3518e-02, -1.0213e-01,  1.2189e-01],
        [ 4.4742e-02, -7.1957e-02, -2.9839e-03],
        [ 1.1441e-02, -2.0357e-02, -5.4231e-02],
        [ 1.8708e-02, -1.2683e-02, -3.2688e-02],
        [ 3.3541e-02,  2.9104e-02, -1.0454e-01],
        [ 1.3762e-02,  3.7443e-02, -9.8369e-02],
        [ 5.4980e-02,  1.6110e-04, -9.4087e-02],
        [ 7.8022e-03,  2.1253e-02, -9.8820e-02],
        [ 3.4918e-02, -9.0634e-02,  1.4809e-01],
        [ 1.1070e-02, -6.8969e-02,  1.1974e-01],
        [ 9.3427e-03, -9.6650e-02,  1.4014e-01],
        [-5.8081e-03, -7.0819e-02,  1.0818e-01],
        [ 5.4593e-03,  3.1053e-02, -1.0966e-01],
        [ 1.1504e-03,  3.6495e-02, -9.8174e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000024
penultimate_layer.0.bias: grad mean = 0.000496
output_layer.0.weight: grad mean = 0.003104
output_layer.0.bias: grad mean = 0.014727
[correct] no_actions is False
task_labels:  tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 2, 0, 1, 2, 0, 1, 1, 1, 1, 2, 1, 1, 2,
        1, 2, 0, 1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 0, 2, 1, 2, 1, 2, 0, 2, 1, 0, 0, 2, 0,
        2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 0, 2, 0, 0, 2, 2, 1, 2, 0, 0, 1, 1,
        0, 0, 1, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 0, 1, 2, 2, 0, 1, 2, 0, 0, 1, 0,
        1, 2, 0, 1, 0, 2, 2, 1]) , task_preds: tensor([[-0.0105, -0.0817,  0.0880],
        [ 0.0012,  0.0361, -0.1021],
        [ 0.0216,  0.0055, -0.0995],
        [ 0.0076,  0.0441, -0.1147],
        [ 0.0167,  0.0348, -0.1004],
        [ 0.0194, -0.0380,  0.0853],
        [ 0.0633, -0.0488,  0.1071],
        [ 0.0151,  0.0216, -0.1080],
        [ 0.0676, -0.0489, -0.1064],
        [ 0.0556, -0.0656, -0.0361],
        [-0.0011, -0.0825,  0.1513],
        [ 0.0329, -0.1024,  0.1254],
        [ 0.0455, -0.0584, -0.0080],
        [-0.0359, -0.0732,  0.1313],
        [ 0.0086,  0.0395, -0.1098],
        [ 0.0162, -0.0861,  0.0971],
        [ 0.0315, -0.0034, -0.0926],
        [ 0.0047,  0.0320, -0.1268],
        [ 0.0079, -0.0748,  0.0275],
        [ 0.0049,  0.0240, -0.0980],
        [-0.0038,  0.0353, -0.0991],
        [-0.0040,  0.0190, -0.1045],
        [ 0.0052,  0.0376, -0.1037],
        [ 0.0147, -0.0751,  0.1725],
        [ 0.0122,  0.0104, -0.1188],
        [ 0.0231,  0.0098, -0.1230],
        [ 0.0061,  0.0484, -0.1093],
        [ 0.0042,  0.0349, -0.1109],
        [ 0.0786, -0.0638, -0.1000],
        [ 0.0181, -0.0930,  0.1243],
        [ 0.0041,  0.0317, -0.1062],
        [ 0.0069,  0.0330, -0.1205],
        [ 0.0313, -0.0462, -0.0035],
        [ 0.0061,  0.0395, -0.1167],
        [ 0.0142, -0.0065, -0.1044],
        [ 0.0014,  0.0322, -0.1112],
        [ 0.0275, -0.0046, -0.0563],
        [ 0.0393, -0.0534, -0.0340],
        [ 0.0214,  0.0194, -0.1092],
        [ 0.0180,  0.0137, -0.1190],
        [ 0.0513,  0.0009, -0.1392],
        [ 0.0302, -0.0199, -0.0515],
        [ 0.0337, -0.0537, -0.0233],
        [ 0.0225,  0.0261, -0.1145],
        [ 0.0144,  0.0323, -0.1217],
        [ 0.0170, -0.0831,  0.1277],
        [ 0.0735, -0.0599, -0.0523],
        [ 0.0089,  0.0401, -0.1122],
        [ 0.0124,  0.0384, -0.1121],
        [ 0.0304, -0.0584, -0.0207],
        [ 0.0071, -0.0574,  0.1401],
        [ 0.0352, -0.0543, -0.0185],
        [ 0.0160,  0.0355, -0.1075],
        [ 0.0183,  0.0466, -0.1205],
        [ 0.0469,  0.0058, -0.1027],
        [ 0.0225,  0.0046, -0.1040],
        [ 0.0066, -0.0612,  0.1201],
        [ 0.0091,  0.0230, -0.1062],
        [ 0.0410,  0.0111, -0.0459],
        [ 0.0082,  0.0294, -0.1164],
        [-0.0104, -0.0876,  0.1558],
        [ 0.0116, -0.0312, -0.0991],
        [ 0.0554, -0.0919,  0.1097],
        [ 0.0080,  0.0432, -0.0929],
        [ 0.0414, -0.0115, -0.0876],
        [ 0.0213,  0.0241, -0.1070],
        [ 0.0628, -0.0980,  0.0259],
        [-0.0020,  0.0345, -0.1003],
        [ 0.0660, -0.0028, -0.0862],
        [ 0.0164,  0.0272, -0.1251],
        [ 0.0045,  0.0351, -0.1124],
        [ 0.0058,  0.0338, -0.1103],
        [ 0.0457,  0.0065, -0.0982],
        [-0.0242, -0.0745,  0.1424],
        [-0.0022, -0.0794,  0.1180],
        [ 0.0393, -0.1029,  0.1410],
        [ 0.0213,  0.0416, -0.1167],
        [-0.0107, -0.0418,  0.1342],
        [-0.0065,  0.0260, -0.0962],
        [ 0.0349,  0.0142, -0.1128],
        [ 0.0491, -0.0063, -0.1183],
        [ 0.0636, -0.0090, -0.1092],
        [ 0.0184,  0.0401, -0.1032],
        [ 0.0030,  0.0471, -0.1104],
        [ 0.0291, -0.0762,  0.0926],
        [ 0.0404, -0.0568,  0.0460],
        [ 0.0193,  0.0291, -0.1240],
        [-0.0002, -0.1120,  0.1409],
        [ 0.0095,  0.0275, -0.1035],
        [ 0.0084, -0.0670,  0.1372],
        [ 0.0449, -0.0911,  0.0866],
        [-0.0243, -0.0551,  0.1395],
        [ 0.0154, -0.1142,  0.0942],
        [ 0.0194,  0.0354, -0.1024],
        [-0.0140, -0.0499,  0.1079],
        [ 0.0370, -0.0363, -0.0776],
        [ 0.0161,  0.0315, -0.1132],
        [ 0.0261, -0.0058, -0.0852],
        [ 0.0136,  0.0341, -0.1115],
        [ 0.0565, -0.0013, -0.1009],
        [ 0.0355, -0.0146, -0.1121],
        [-0.0004,  0.0324, -0.1042],
        [ 0.0223, -0.0277, -0.0557],
        [ 0.0352, -0.0978,  0.1494],
        [ 0.0265, -0.0535,  0.0063],
        [ 0.0156, -0.0514, -0.0508],
        [ 0.0282, -0.0532,  0.1153],
        [ 0.0178,  0.0523, -0.1065],
        [ 0.0135, -0.0822,  0.1354],
        [ 0.0680, -0.0539, -0.0302],
        [ 0.0047,  0.0125, -0.1058],
        [ 0.0244, -0.0043, -0.0572],
        [ 0.0096,  0.0328, -0.0979],
        [ 0.0197,  0.0309, -0.1222],
        [ 0.0348, -0.0917,  0.0456],
        [ 0.0593, -0.0080, -0.0977],
        [ 0.0339,  0.0106, -0.1220],
        [ 0.0402, -0.0101, -0.0918],
        [-0.0110, -0.1058,  0.1240],
        [ 0.0245, -0.0767,  0.1220],
        [ 0.0163,  0.0300, -0.1100],
        [-0.0490, -0.0524,  0.1121],
        [ 0.0047,  0.0389, -0.1144],
        [ 0.0128,  0.0251, -0.1092],
        [ 0.0114,  0.0298, -0.1173],
        [ 0.0053,  0.0369, -0.1037],
        [ 0.0640, -0.0814,  0.0273],
        [ 0.0168, -0.0640,  0.1295]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000021
penultimate_layer.0.bias: grad mean = 0.000157
output_layer.0.weight: grad mean = 0.002666
output_layer.0.bias: grad mean = 0.004512
[correct] no_actions is False
task_labels:  tensor([0, 1, 1, 1, 1, 0, 2, 2, 0, 2, 2, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 1, 1, 2,
        1, 2, 2, 0, 0, 0, 0, 0, 2, 1, 2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 1, 2, 2,
        2, 1, 1, 0, 1, 0, 2, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 2, 0, 0, 2,
        1, 0, 1, 1, 1, 1, 2, 0, 1, 2, 2, 1, 2, 0, 2, 0, 0, 2, 2, 2, 1, 2, 2, 0,
        2, 1, 0, 1, 1, 2, 0, 2, 1, 2, 0, 2, 0, 0, 1, 2, 2, 2, 1, 1, 0, 1, 1, 2,
        0, 1, 0, 2, 2, 2, 2, 1]) , task_preds: tensor([[ 0.0765, -0.0556, -0.0029],
        [ 0.0244,  0.0494, -0.1187],
        [-0.0007, -0.0728,  0.1416],
        [ 0.0007,  0.0186, -0.1136],
        [ 0.0460,  0.0050, -0.1148],
        [ 0.0865, -0.0748, -0.0456],
        [ 0.0481, -0.0997,  0.1061],
        [ 0.0138,  0.0305, -0.1192],
        [ 0.0932, -0.0847,  0.0837],
        [ 0.0039, -0.1074,  0.1382],
        [ 0.0271, -0.0667,  0.0245],
        [ 0.0166, -0.0671,  0.0722],
        [ 0.0613, -0.0310, -0.0465],
        [ 0.0112,  0.0292, -0.1075],
        [ 0.0608, -0.0687, -0.0088],
        [ 0.0531, -0.1339,  0.1024],
        [ 0.0305, -0.0463, -0.0011],
        [ 0.0201, -0.0846,  0.1269],
        [ 0.0484, -0.0707,  0.0182],
        [ 0.0728, -0.1046,  0.0702],
        [ 0.0005, -0.0758,  0.1346],
        [ 0.0225,  0.0288, -0.1161],
        [ 0.0215,  0.0433, -0.1143],
        [ 0.0015,  0.0053, -0.1164],
        [ 0.0147,  0.0339, -0.1182],
        [ 0.0094,  0.0419, -0.1122],
        [ 0.0332, -0.0934,  0.1567],

MLP Fine-Tuning: task_loss = 1.0882:  73%|███████▎  | 11/15 [01:01<00:06,  1.59s/it]
        [ 0.0506, -0.0625, -0.0281],
        [ 0.0527, -0.1007,  0.1144],
        [ 0.0391, -0.0960,  0.1507],
        [ 0.0424, -0.0653,  0.0854],
        [ 0.0401, -0.0117, -0.0523],
        [ 0.0443, -0.0520, -0.0154],
        [ 0.0173,  0.0281, -0.1187],
        [ 0.0080,  0.0391, -0.1093],
        [ 0.0191,  0.0215, -0.0934],
        [ 0.0099,  0.0420, -0.0959],
        [ 0.0689, -0.0828, -0.0040],
        [ 0.0023,  0.0201, -0.1129],
        [ 0.0536, -0.1094,  0.1074],
        [ 0.0094,  0.0384, -0.1205],
        [ 0.0544, -0.0963,  0.0560],
        [ 0.0347,  0.0026, -0.0921],
        [ 0.0334, -0.0583,  0.0729],
        [ 0.0080,  0.0391, -0.1093],
        [ 0.0215, -0.0581, -0.0385],
        [ 0.0552, -0.0755,  0.0848],
        [ 0.0455,  0.0118, -0.0545],
        [ 0.0288,  0.0034, -0.1082],
        [ 0.0085,  0.0350, -0.1107],
        [ 0.0107, -0.0511,  0.1352],
        [ 0.0985, -0.0979, -0.0101],
        [ 0.0037,  0.0273, -0.1072],
        [ 0.0032, -0.0660,  0.1590],
        [ 0.0230,  0.0245, -0.1020],
        [ 0.0262, -0.0748,  0.1294],
        [ 0.0534, -0.0728,  0.0470],
        [ 0.0549, -0.0887,  0.0219],
        [ 0.0867, -0.0553, -0.0843],
        [ 0.0226,  0.0384, -0.1071],
        [ 0.0238,  0.0387, -0.1194],
        [ 0.0213, -0.0013, -0.1023],
        [ 0.0187,  0.0355, -0.1121],
        [ 0.0080,  0.0391, -0.1093],
        [ 0.0485,  0.0073, -0.1254],
        [ 0.0325, -0.0762,  0.1612],
        [ 0.0046, -0.0570,  0.0152],
        [ 0.0474,  0.0106, -0.0405],
        [ 0.0657, -0.0818,  0.0102],
        [ 0.0229,  0.0158, -0.1095],
        [ 0.0057, -0.0338, -0.0179],
        [ 0.0596, -0.0586, -0.0425],
        [ 0.0063,  0.0271, -0.1140],
        [ 0.0425, -0.0798,  0.1192],
        [ 0.0462, -0.0908,  0.1010],
        [ 0.0324, -0.0552, -0.0426],
        [ 0.0154,  0.0446, -0.1051],
        [ 0.0150, -0.1014,  0.0887],
        [ 0.0080,  0.0391, -0.1093],
        [ 0.0515, -0.0075, -0.0533],
        [ 0.0030, -0.0355,  0.0403],
        [-0.0153, -0.0643,  0.1600],
        [ 0.0556, -0.0610, -0.0312],
        [ 0.0666, -0.0341, -0.0418],
        [ 0.0051,  0.0402, -0.1103],
        [ 0.0170,  0.0189, -0.1281],
        [ 0.0292, -0.1393,  0.1010],
        [ 0.0226,  0.0108, -0.1087],
        [ 0.0210, -0.0161, -0.1102],
        [ 0.0272, -0.1059,  0.1423],
        [ 0.0289, -0.0598,  0.0861],
        [ 0.0065,  0.0370, -0.1103],
        [ 0.0053,  0.0276, -0.1197],
        [ 0.0538, -0.0836,  0.1498],
        [-0.0106, -0.1097,  0.1457],
        [ 0.0038, -0.0733,  0.1589],
        [ 0.0028, -0.0802,  0.1501],
        [ 0.0178,  0.0261, -0.1167],
        [ 0.0373,  0.0176, -0.1187],
        [-0.0011, -0.1031,  0.1389],
        [ 0.0630, -0.0659, -0.0295],
        [ 0.0239, -0.0785,  0.1002],
        [ 0.0660, -0.0118, -0.1011],
        [ 0.0337, -0.0834,  0.0506],
        [ 0.0544, -0.0164, -0.0432],
        [ 0.0320, -0.1001,  0.1144],
        [ 0.0571, -0.0186, -0.0669],
        [ 0.0074,  0.0376, -0.1120],
        [ 0.0056,  0.0363, -0.1254],
        [ 0.0501, -0.0175, -0.0237],
        [ 0.0122,  0.0321, -0.0973],
        [ 0.0070,  0.0428, -0.1151],
        [ 0.0063,  0.0427, -0.1174],
        [-0.0089, -0.0873,  0.1580],
        [ 0.0139,  0.0367, -0.1195],
        [ 0.0248, -0.0813,  0.1337],
        [ 0.0057,  0.0349, -0.1156],
        [ 0.0047,  0.0332, -0.1120],
        [ 0.0481,  0.0019, -0.1081],
        [ 0.0395, -0.0295,  0.0358],
        [ 0.0576, -0.0048, -0.0591],
        [ 0.0171,  0.0507, -0.1109],
        [ 0.0049, -0.0333, -0.0539],
        [-0.0304, -0.0819,  0.1332],
        [ 0.0540,  0.0053, -0.0918],
        [ 0.0511, -0.0984,  0.1161]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000037
penultimate_layer.0.bias: grad mean = 0.001350
output_layer.0.weight: grad mean = 0.004381
output_layer.0.bias: grad mean = 0.045462
[correct] no_actions is False
task_labels:  tensor([1, 0, 1, 2, 1, 2, 2, 2, 1, 0, 1, 1, 1, 2, 2, 0, 1, 1, 1, 1, 2, 0, 1, 0,
        1, 1, 1, 2, 2, 0, 0, 1, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2,
        1, 0, 2, 0, 2, 2, 0, 2, 1, 0, 2, 1, 1, 2, 0, 0, 1, 1, 1, 1, 2, 0, 0, 2,
        2, 0, 0, 0, 2, 0, 2, 1, 2, 0, 1, 1, 1, 1, 0, 2, 1, 0, 0, 0, 0, 2, 0, 2,
        2, 2, 2, 0, 1, 1, 0, 2, 2, 2, 0, 2, 1, 1, 1, 0, 2, 0, 2, 1, 1, 1, 2, 2,
        1, 2, 2, 2, 2, 2, 1, 1]) , task_preds: tensor([[ 2.0482e-02,  3.9787e-02, -1.2044e-01],
        [ 5.6664e-02, -2.2664e-02, -6.4203e-02],
        [ 1.6670e-02, -1.0669e-01,  1.3911e-01],
        [ 2.2653e-02,  3.1148e-02, -1.2138e-01],
        [ 5.5667e-02,  8.8010e-03, -6.9386e-02],
        [ 6.2987e-02, -7.0974e-02, -8.3050e-03],
        [ 3.3893e-02, -4.8226e-02, -3.6593e-02],
        [ 5.7354e-02, -2.1684e-03, -8.0486e-02],
        [ 3.5058e-02, -3.2419e-03, -1.0736e-01],
        [ 3.7044e-02, -8.6221e-02,  1.2394e-01],
        [ 1.8138e-02,  3.7112e-02, -1.2592e-01],
        [ 3.6375e-02, -8.5810e-02,  7.7308e-02],
        [ 1.9143e-02, -5.1986e-02, -5.4336e-02],
        [ 2.4149e-02,  4.0188e-02, -1.1042e-01],
        [ 8.6067e-03,  2.6838e-02, -1.0455e-01],
        [ 1.5811e-02, -8.8468e-02,  1.0134e-01],
        [ 2.4616e-02,  4.1893e-02, -1.1612e-01],
        [ 1.1624e-02, -5.3311e-02,  1.3716e-01],
        [ 9.9065e-03, -9.2383e-02,  1.7170e-01],
        [-2.3225e-03, -4.2879e-02,  1.0516e-01],
        [ 3.0419e-02, -2.4820e-02, -7.7435e-02],
        [ 6.8397e-02, -1.0336e-01,  7.7554e-03],
        [ 5.2601e-02, -4.2276e-02, -4.6727e-02],
        [ 3.5364e-02, -8.0191e-02,  1.1440e-01],
        [ 1.1846e-02,  2.6081e-02, -1.4931e-01],
        [ 1.2717e-02,  3.5829e-02, -1.3012e-01],
        [ 1.1501e-02,  3.5212e-02, -1.2224e-01],
        [ 2.6223e-02,  1.2714e-02, -1.0547e-01],
        [ 2.2218e-03,  3.0077e-02, -1.0282e-01],
        [ 1.0784e-01, -9.6558e-02,  2.6872e-02],
        [ 8.4787e-02, -2.5110e-02, -7.4766e-02],
        [ 2.9981e-02,  3.7864e-02, -9.9650e-02],
        [ 3.2376e-02, -7.9920e-02,  9.3848e-02],
        [ 3.2857e-03,  3.5846e-02, -1.1963e-01],
        [ 1.0095e-02,  3.1753e-02, -1.1372e-01],
        [ 3.0552e-02, -3.8220e-02,  7.2628e-03],
        [ 1.1858e-02,  3.6323e-02, -1.1913e-01],
        [ 1.8441e-02,  4.0087e-02, -1.1616e-01],
        [ 3.2983e-02, -1.4521e-02, -6.1103e-02],
        [ 2.6867e-02, -4.4451e-02,  5.9640e-03],
        [ 2.8807e-02, -8.7997e-02,  1.5940e-01],
        [ 2.9017e-02,  7.4275e-03, -1.0340e-01],
        [ 6.6135e-02, -3.8882e-02, -1.4202e-02],
        [ 3.8901e-02, -3.5557e-02, -2.7899e-02],
        [ 2.7684e-02, -1.0672e-01,  5.5686e-02],
        [ 6.7705e-03,  2.3199e-02, -1.1304e-01],
        [ 2.6913e-02, -5.8036e-02,  1.3573e-01],
        [ 9.0056e-03, -7.3794e-02,  5.1104e-02],
        [ 2.5585e-02,  4.3117e-02, -1.2057e-01],
        [ 4.0727e-02, -1.1646e-01,  1.1800e-01],
        [ 2.8429e-02, -2.7442e-02, -6.4237e-02],
        [ 5.1410e-02, -9.5804e-02,  1.3859e-01],
        [-3.2439e-03, -7.4472e-02,  1.2057e-01],
        [ 4.4673e-02,  6.7868e-04, -1.3537e-01],
        [ 7.2101e-02, -5.5669e-02, -3.2718e-02],
        [ 1.2487e-02, -1.0332e-01,  1.4582e-01],
        [ 1.5441e-02,  3.3722e-02, -1.0957e-01],
        [ 1.1603e-02,  3.3637e-02, -1.1255e-01],
        [-7.0516e-03, -6.1918e-02,  1.5381e-01],
        [-3.5305e-02, -7.6056e-02,  1.3473e-01],
        [ 4.3420e-03,  3.9309e-02, -1.0619e-01],
        [ 1.6190e-02, -7.1651e-02,  1.2465e-01],
        [ 3.6741e-02, -7.4526e-02,  6.4754e-03],
        [ 6.4009e-02, -3.3157e-02, -4.6694e-02],
        [ 2.8553e-02, -3.9211e-02,  8.2586e-02],
        [ 7.5174e-03,  3.0799e-02, -1.1582e-01],
        [ 1.4649e-02,  7.5032e-04, -1.0781e-01],
        [ 7.5640e-03,  3.4784e-02, -1.2552e-01],
        [ 5.9910e-02, -8.6488e-02,  3.6270e-02],
        [ 9.7162e-03,  3.5949e-02, -1.0992e-01],
        [ 5.8158e-02, -4.8980e-02, -4.2209e-02],
        [ 1.3478e-02,  3.4107e-02, -1.1091e-01],
        [ 3.5328e-02, -9.8575e-02,  1.5457e-01],
        [ 6.6084e-02, -1.0975e-04, -9.1876e-02],
        [ 5.0591e-02,  1.3031e-02, -4.6754e-02],
        [ 7.0033e-02, -3.6262e-02, -4.2939e-02],
        [ 2.0140e-02, -9.7143e-02,  1.2768e-01],
        [ 6.6237e-02, -3.2995e-03, -1.0300e-01],
        [ 6.3540e-02, -3.4420e-02, -2.1403e-02],
        [ 3.1675e-02, -7.0609e-02,  5.1663e-02],
        [ 4.0431e-02, -4.3493e-02, -6.7527e-03],
        [ 2.1995e-02,  3.8996e-02, -1.2520e-01],
        [ 3.1405e-02, -7.4039e-02, -1.2832e-02],
        [ 5.9019e-02, -6.6753e-02, -3.8663e-02],
        [ 8.6023e-03,  4.7419e-02, -1.1778e-01],
        [ 9.3698e-03, -5.8679e-02, -2.3712e-02],
        [ 5.4135e-02, -1.0308e-01,  1.1601e-01],
        [ 5.7978e-02, -6.3011e-02, -3.1213e-02],
        [ 2.0476e-02, -2.4024e-02, -6.0134e-02],
        [ 6.8677e-02, -8.1735e-02,  1.4871e-02],
        [ 1.3394e-02,  3.6679e-02, -1.1344e-01],
        [ 5.6711e-02, -1.2177e-01,  9.3157e-02],
        [ 1.7963e-02,  4.6594e-02, -1.1755e-01],
        [ 4.9982e-03,  2.7648e-02, -1.0896e-01],
        [ 1.9510e-02,  3.2136e-02, -1.0292e-01],
        [-9.6015e-03, -1.1198e-01,  1.4780e-01],
        [ 3.6761e-02, -6.8233e-02,  2.5824e-02],
        [ 2.6239e-02, -7.3294e-02,  1.6024e-01],
        [ 6.0230e-02, -7.5475e-02,  1.5466e-02],
        [ 9.2014e-03, -8.9495e-02,  1.2024e-01],
        [ 5.6345e-02,  7.4266e-04, -1.4521e-01],
        [ 2.8003e-04, -7.4919e-02,  1.4344e-01],
        [ 4.9273e-02, -1.9031e-02, -6.7461e-02],
        [ 2.2446e-02,  2.8768e-02, -1.1728e-01],
        [ 2.0180e-02,  3.9331e-02, -1.2652e-01],
        [ 1.3446e-02,  3.6592e-02, -1.1201e-01],
        [ 2.0737e-02,  4.5496e-02, -1.1927e-01],
        [ 5.2734e-02, -6.7951e-02,  1.1922e-01],
        [ 1.4879e-02, -5.8531e-02, -3.2795e-03],
        [ 1.7081e-02,  2.1920e-02, -1.1207e-01],
        [ 3.3072e-02, -6.0694e-03, -9.8268e-02],
        [ 1.9426e-02,  3.4472e-02, -1.2081e-01],
        [ 3.2703e-02, -1.0224e-01,  1.1657e-01],
        [ 1.1100e-02,  3.7729e-02, -1.1121e-01],
        [ 1.9614e-02,  2.7385e-02, -1.2128e-01],
        [ 6.5863e-02, -3.8216e-02, -1.6165e-02],
        [ 6.3680e-03, -6.7599e-02,  1.7414e-02],
        [ 3.9220e-03,  3.5311e-02, -1.1231e-01],
        [ 4.6612e-03, -1.0954e-01,  1.4035e-01],
        [ 1.5553e-02,  3.3608e-02, -1.2294e-01],
        [ 9.1673e-03,  3.1414e-02, -1.1370e-01],
        [ 1.2192e-02,  3.5709e-02, -1.1433e-01],
        [ 2.1613e-02,  3.1997e-02, -1.3390e-01],
        [ 2.0224e-02, -6.8777e-02,  1.2572e-02],
        [ 8.7648e-03, -4.3122e-02, -1.4530e-02],
        [ 3.4224e-02, -9.5577e-02,  1.5854e-01],
        [ 3.2316e-02, -5.1621e-02, -4.0256e-02],
        [ 4.0747e-02, -1.0578e-01,  1.4325e-01]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000042
penultimate_layer.0.bias: grad mean = 0.001611
output_layer.0.weight: grad mean = 0.004855
output_layer.0.bias: grad mean = 0.054426
[correct] no_actions is False
task_labels:  tensor([1, 1, 0, 1, 2, 1, 1, 2, 1, 1, 0, 0, 2, 1, 1, 1, 1, 1, 1, 2, 2, 0, 0, 2,
        1, 0, 1, 1, 0, 0, 0, 2, 0, 0, 2, 2, 1, 2, 1, 2, 2, 0, 1, 0, 0, 1, 2, 1,
        1, 1, 1, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 2, 1, 1, 0, 0, 0, 2, 2, 1, 0, 0,
        0, 0, 0, 1, 1, 2, 2, 1, 2, 0, 0, 2, 2, 0, 2, 0, 1, 1, 0, 0, 0, 0, 2, 0,
        1, 0, 1, 1, 0, 1, 1, 0]) , task_preds: tensor([[ 2.7361e-02,  4.8138e-02, -1.2047e-01],
        [ 5.7883e-02, -6.8340e-02,  1.0138e-01],
        [ 3.0498e-02, -1.4347e-01,  1.0528e-01],
        [ 5.1930e-03,  1.8707e-02, -1.1449e-01],
        [ 4.0039e-03,  3.4420e-02, -1.2218e-01],
        [ 3.7808e-02, -9.3731e-03, -3.4342e-02],
        [ 4.3052e-02,  3.5589e-04, -9.1973e-02],
        [ 1.6153e-02,  3.8876e-02, -1.2652e-01],
        [-8.2162e-03, -9.1046e-02,  1.6251e-01],
        [ 7.8329e-02, -1.1472e-02, -1.1786e-01],
        [ 2.7493e-02, -2.9847e-02, -5.9112e-02],
        [ 3.5763e-02,  2.0804e-02, -1.1915e-01],
        [-7.6810e-03, -1.1125e-01,  1.0433e-01],
        [ 3.7582e-02, -3.4698e-02, -4.0940e-02],
        [ 4.5202e-02, -8.3761e-02,  1.2145e-01],
        [ 1.6238e-02, -7.2969e-02,  1.5393e-01],
        [ 2.2391e-02,  4.0624e-02, -1.1540e-01],
        [ 4.0609e-02,  9.0242e-03, -8.4525e-02],
        [ 9.7882e-03,  3.7105e-02, -1.1293e-01],
        [ 4.0986e-03, -7.0037e-02,  1.6367e-01],
        [ 1.4989e-02,  3.0750e-02, -9.8797e-02],
        [ 2.2518e-02,  3.4878e-02, -1.0767e-01],
        [ 2.3454e-02,  3.2963e-02, -1.2046e-01],
        [ 3.9203e-02, -8.2909e-03, -1.0597e-01],
        [ 1.2949e-02,  3.8848e-02, -1.0602e-01],
        [ 1.7340e-02, -3.4300e-02, -5.1728e-02],
        [ 1.1144e-02,  3.7609e-02, -1.1107e-01],
        [ 3.3375e-02,  2.2096e-02, -1.1209e-01],
        [ 1.5969e-02,  4.3311e-02, -1.2498e-01],
        [ 4.1755e-03, -8.4471e-02,  1.5441e-01],
        [ 9.6637e-02, -8.9541e-02,  8.6417e-02],
        [ 3.2573e-02,  9.0516e-04, -8.0744e-02],
        [ 1.8596e-02,  3.2401e-02, -1.2203e-01],
        [ 8.4246e-02, -7.1143e-02,  2.7120e-02],
        [ 2.1686e-02,  3.1667e-02, -1.1664e-01],
        [ 1.9958e-02, -1.3891e-04, -6.4119e-02],
        [ 2.9318e-02, -2.2295e-02, -1.0442e-01],
        [ 1.2823e-02,  4.6877e-02, -1.2423e-01],
        [ 7.2545e-03,  3.2270e-02, -1.1859e-01],
        [ 6.0141e-02, -8.0806e-02,  4.6826e-02],
        [-5.7928e-04, -7.2409e-02,  1.6928e-01],
        [ 2.8163e-02,  2.5888e-02, -1.2114e-01],
        [ 2.5084e-02,  2.8257e-02, -1.2257e-01],
        [ 2.1644e-02,  3.3560e-02, -1.2513e-01],
        [ 2.3344e-02, -8.9447e-02,  1.2855e-01],
        [ 3.4247e-02, -7.0604e-02,  7.6481e-02],
        [ 3.7468e-02, -1.8940e-03, -7.1970e-02],
        [-8.8981e-03, -8.7583e-02,  1.2294e-01],
        [ 8.5729e-03,  3.3698e-02, -1.1729e-01],
        [ 1.8606e-02,  3.8405e-02, -1.1520e-01],
        [ 3.5149e-02,  9.0601e-03, -1.1296e-01],
        [ 1.1614e-03, -1.1719e-01,  1.4667e-01],
        [-8.5502e-03, -6.8077e-02,  1.7404e-01],
        [ 5.7606e-02, -1.1316e-01,  1.0915e-01],
        [ 1.5577e-02, -9.7528e-02,  5.4678e-02],
        [ 4.0333e-02,  9.3587e-03, -1.2813e-01],
        [ 4.3334e-02, -1.1880e-01,  1.3904e-01],
        [ 1.1144e-02,  3.7609e-02, -1.1107e-01],
        [ 1.5959e-02, -8.1174e-02,  1.7974e-01],
        [ 1.5140e-02,  3.2903e-02, -1.1774e-01],
        [ 1.4706e-02,  2.2572e-02, -1.1248e-01],
        [ 6.1828e-02, -4.4017e-02, -2.7327e-02],
        [ 6.9183e-02, -9.5879e-03, -1.1513e-01],
        [ 4.1148e-02, -3.7601e-02, -4.8709e-02],
        [ 3.7452e-02, -5.7693e-02, -3.3173e-02],
        [ 4.0200e-02, -3.8041e-02, -5.6173e-02],
        [ 3.8393e-02,  2.1662e-04, -1.0680e-01],
        [ 3.0518e-02, -5.8475e-02, -6.7500e-02],
        [ 2.0050e-02, -8.1705e-02,  5.4656e-02],
        [ 3.6098e-02, -4.2709e-02, -2.4293e-02],
        [ 1.1144e-02,  3.7609e-02, -1.1107e-01],
        [ 6.0873e-03,  3.8488e-02, -1.1596e-01],
        [ 2.0429e-02, -5.1471e-02,  6.7368e-02],
        [ 3.7055e-02, -6.6549e-02, -5.6394e-03],
        [ 6.8876e-02, -7.6530e-02, -2.5676e-02],
        [ 6.2762e-02, -1.7147e-02, -8.0485e-02],
        [ 2.6760e-03,  3.6588e-02, -1.0419e-01],
        [ 6.9246e-02, -1.4187e-02, -1.0157e-01],
        [ 6.1020e-02, -2.6674e-03, -9.5455e-02],
        [ 4.0185e-02,  1.6064e-02, -1.2004e-01],
        [ 9.0193e-02, -7.8318e-02, -4.5114e-02],
        [ 1.9008e-02, -9.2047e-02,  1.0220e-01],
        [ 2.2597e-02,  1.4334e-04, -1.0702e-01],
        [ 7.7951e-02, -8.9486e-02,  1.7786e-02],
        [ 2.2794e-02,  3.3400e-02, -1.2415e-01],
        [ 1.8078e-02,  3.3366e-02, -1.2604e-01],
        [ 1.2417e-02,  6.2700e-04, -1.1766e-01],
        [ 7.0203e-03,  4.1487e-02, -1.1129e-01],
        [ 7.3495e-02, -3.4610e-02, -5.8101e-02],
        [ 6.3343e-02, -2.1793e-02, -4.5882e-02],
        [ 3.9613e-02, -1.2711e-01,  1.3785e-01],
        [ 2.8686e-02, -6.7595e-02, -6.2157e-02],
        [ 1.4726e-02,  3.9382e-02, -1.1714e-01],
        [ 1.3168e-02,  3.6977e-02, -1.1332e-01],
        [ 1.6442e-02,  3.8655e-02, -1.1339e-01],
        [ 1.2950e-02,  1.2742e-02, -1.1237e-01],
        [ 3.0506e-02,  3.6946e-02, -1.2208e-01],
        [ 1.2343e-02,  3.3432e-02, -1.1441e-01],
        [ 2.7183e-02, -1.0692e-01,  1.4252e-01],
        [ 3.3302e-02,  4.1519e-02, -1.1539e-01],
        [ 1.4182e-02, -1.0115e-01,  1.0307e-01],
        [ 4.0905e-02, -1.5513e-02, -1.1752e-01],
        [ 2.0264e-02, -9.9668e-02,  7.1370e-02],
        [ 4.5418e-02, -9.6946e-02,  1.2186e-01],
        [ 2.6366e-02,  8.4770e-03, -7.2859e-02],
        [ 7.8382e-02, -5.8896e-02, -4.4335e-04],
        [ 3.1096e-02, -1.0543e-01,  1.2719e-01],
        [ 2.3724e-02,  2.0361e-02, -1.2248e-01],
        [ 3.5358e-02, -7.5390e-02,  7.6276e-02],
        [ 9.2591e-02, -1.0839e-01,  7.5523e-02],
        [ 4.6402e-02, -8.9932e-02,  1.1050e-01],
        [ 1.4403e-02,  2.7670e-02, -1.0920e-01],
        [ 6.8132e-03, -9.6771e-02,  1.2524e-01],
        [ 5.7307e-02, -5.3292e-02, -3.4786e-04],
        [ 2.9323e-02,  1.8320e-02, -1.1713e-01],
        [ 6.8458e-02, -5.4624e-02,  1.1097e-01],
        [ 1.0822e-02,  3.8762e-02, -1.2169e-01],
        [ 2.6609e-02,  3.5093e-02, -1.0224e-01],
        [ 1.7691e-02,  3.4774e-02, -1.1857e-01],
        [ 5.0993e-02, -1.5898e-02, -1.2743e-01],
        [ 2.5518e-02, -8.4893e-02,  1.3786e-01],
        [ 4.7998e-02, -7.1266e-03, -6.1664e-02],
        [ 1.1329e-02,  3.7409e-02, -1.0620e-01],
        [ 1.0624e-02, -4.7041e-02,  3.3555e-02],
        [ 6.0960e-02, -7.2726e-03, -5.9616e-02],
        [-2.3761e-02, -1.0075e-01,  1.5376e-01],
        [ 7.0428e-03, -3.6148e-02, -1.6222e-02],
        [ 8.2411e-02, -7.7780e-02, -7.5393e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000038
penultimate_layer.0.bias: grad mean = 0.001414
output_layer.0.weight: grad mean = 0.004002
output_layer.0.bias: grad mean = 0.040356
[correct] no_actions is False
task_labels:  tensor([2, 1, 0, 1, 1, 2, 2, 1, 1, 1, 0, 2, 2, 2, 1, 0, 1, 2, 2, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 2, 2, 0, 1, 1, 2, 1, 0, 1, 2, 0,
        0, 2, 2, 0, 1, 2, 2, 2, 0, 1, 0, 2, 2, 1, 1, 2, 2, 1, 2, 0, 1, 2, 0, 2,
        0, 0, 1, 2, 0, 0, 2, 1, 2, 0, 1, 2, 1, 1, 2, 1, 2, 2, 1, 0, 1, 1, 2, 0,
        1, 0, 2, 0, 2, 2, 2, 2, 0, 2, 1, 0, 0, 0, 1, 1, 1, 0, 2, 1, 1, 0, 0, 0,
        0, 1, 2, 2, 0, 1, 1, 0]) , task_preds: tensor([[ 0.0273, -0.0610,  0.1394],
        [-0.0075, -0.0920,  0.1630],
        [ 0.0317,  0.0188, -0.1236],
        [ 0.0337, -0.0609, -0.0212],
        [ 0.0092,  0.0339, -0.1184],
        [ 0.0278, -0.1088,  0.0585],
        [ 0.0078, -0.1126,  0.1658],
        [ 0.0150,  0.0317, -0.1117],
        [ 0.0047,  0.0353, -0.1132],
        [ 0.0264,  0.0418, -0.1280],
        [ 0.0834, -0.0784, -0.0758],
        [ 0.0444, -0.0877,  0.1114],
        [ 0.0642, -0.0357, -0.0204],
        [ 0.0055,  0.0278, -0.1097],
        [ 0.0432, -0.0301, -0.0618],
        [ 0.0229, -0.0776,  0.1569],
        [ 0.0121,  0.0403, -0.1251],
        [ 0.0391, -0.0627, -0.0348],
        [ 0.0519, -0.0788, -0.0011],
        [ 0.0303,  0.0101, -0.1263],
        [ 0.0193, -0.0525, -0.0537],
        [ 0.0256,  0.0142, -0.1209],
        [ 0.0215,  0.0119, -0.1191],
        [ 0.0185, -0.1204,  0.0994],
        [ 0.0112,  0.0289, -0.1107],
        [ 0.0360, -0.1029,  0.1558],
        [ 0.0170,  0.0381, -0.1279],
        [ 0.0128,  0.0319, -0.1165],
        [ 0.0427, -0.0142, -0.0519],
        [ 0.0117, -0.0811,  0.0340],
        [ 0.0114,  0.0396, -0.1161],
        [ 0.0633, -0.0676,  0.0359],
        [ 0.0543, -0.0058, -0.0580],
        [ 0.0093, -0.0925,  0.0711],
        [ 0.0255,  0.0419, -0.1171],
        [-0.0232, -0.1017,  0.1545],
        [ 0.0683, -0.0157, -0.0546],
        [ 0.0574, -0.0990,  0.1172],
        [ 0.0090, -0.0984,  0.1375],
        [ 0.0020, -0.1184,  0.1473],
        [ 0.0229,  0.0378, -0.1076],
        [ 0.0307, -0.0392,  0.0084],
        [ 0.0214, -0.0569,  0.0090],
        [ 0.0125,  0.0367, -0.1199],
        [ 0.0957, -0.0490,  0.0486],
        [ 0.0298,  0.0462, -0.1064],
        [ 0.0208,  0.0494, -0.1135],
        [ 0.0805, -0.0369, -0.0973],
        [ 0.0259,  0.0354, -0.1103],
        [ 0.0174,  0.0482, -0.1257],
        [ 0.0137, -0.0394,  0.0338],
        [ 0.0061,  0.0388, -0.1208],
        [ 0.0169, -0.0738,  0.1543],
        [ 0.0124, -0.1058,  0.1494],
        [ 0.0520, -0.0198, -0.0800],
        [ 0.0178,  0.0265, -0.1134],
        [ 0.0762, -0.0301, -0.0643],
        [ 0.0352, -0.0122, -0.0823],
        [ 0.0412,  0.0092, -0.1291],
        [ 0.0036, -0.1097,  0.1349],
        [ 0.0169,  0.0263, -0.1198],
        [ 0.0409, -0.0717, -0.0408],
        [ 0.0234,  0.0365, -0.1331],
        [ 0.0151, -0.0878,  0.1414],
        [ 0.0168,  0.0328, -0.1134],
        [-0.0079, -0.0266,  0.1031],
        [ 0.0109, -0.0524,  0.0895],
        [ 0.0249,  0.0233, -0.1326],
        [ 0.0338, -0.0386,  0.0411],
        [-0.0070, -0.0647,  0.1579],
        [ 0.0505, -0.0323, -0.0446],
        [ 0.0354, -0.1010,  0.1580],
        [ 0.0791, -0.0906,  0.0179],
        [ 0.0844, -0.0622,  0.0255],
        [ 0.0207, -0.0422,  0.0897],
        [ 0.0370, -0.0703,  0.0284],
        [ 0.0444, -0.1200,  0.1394],
        [ 0.0172,  0.0266, -0.1222],
        [ 0.0304, -0.0427, -0.0067],
        [ 0.0233,  0.0217, -0.1157],
        [ 0.0633, -0.0626, -0.0309],
        [ 0.0196,  0.0164, -0.1292],
        [ 0.0137,  0.0377, -0.1146],
        [ 0.0119,  0.0377, -0.1121],
        [ 0.0483, -0.0080, -0.1007],
        [ 0.0306, -0.0511, -0.0592],
        [ 0.0045, -0.0712,  0.1648],
        [ 0.0180,  0.0391, -0.1183],
        [ 0.0434, -0.0559, -0.0355],
        [ 0.0341, -0.0819,  0.1675],
        [ 0.0125,  0.0171, -0.1389],
        [ 0.0204,  0.0300, -0.1134],
        [ 0.0057,  0.0399, -0.1254],
        [ 0.0382,  0.0099, -0.1342],
        [ 0.0152,  0.0293, -0.1032],
        [ 0.0272,  0.0226, -0.1043],
        [-0.0013, -0.0836,  0.1229],
        [ 0.0525, -0.0003, -0.1103],
        [ 0.0415, -0.0155, -0.1183],
        [ 0.0539, -0.1069,  0.1207],
        [ 0.0535, -0.0516,  0.0208],
        [ 0.0072,  0.0336, -0.1224],
        [-0.0486, -0.0582,  0.1199],
        [ 0.0160, -0.0360, -0.0697],
        [ 0.0527, -0.0695,  0.0547],
        [-0.0223, -0.0806,  0.1487],
        [ 0.0058,  0.0189, -0.1155],
        [ 0.0116,  0.0388, -0.1227],
        [ 0.0569, -0.0824,  0.0593],
        [ 0.0241,  0.0331, -0.1215],
        [ 0.0414, -0.1083,  0.1459],
        [-0.0191, -0.0932,  0.1020],
        [ 0.0135,  0.0337, -0.1289],
        [ 0.0111,  0.0310, -0.1109],
        [ 0.0112,  0.0351, -0.1208],
        [ 0.0156,  0.0298, -0.1292],
        [ 0.0569,  0.0006, -0.1457],
        [ 0.0157,  0.0388, -0.1236],
        [ 0.0214, -0.0894,  0.1321],
        [ 0.0418, -0.1194,  0.1209],
        [ 0.0718, -0.0138, -0.0780],
        [ 0.0134,  0.0360, -0.1312],
        [ 0.0388, -0.0897,  0.0200],
        [ 0.0156,  0.0309, -0.0997],
        [ 0.0982, -0.0910,  0.0866],
        [ 0.0068,  0.0373, -0.1097],
        [ 0.0182,  0.0315, -0.1137],
        [ 0.0342, -0.0829,  0.0959]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000032
penultimate_layer.0.bias: grad mean = 0.001272
output_layer.0.weight: grad mean = 0.003351

MLP Fine-Tuning: task_loss = 1.0852:  80%|████████  | 12/15 [01:03<00:04,  1.45s/it]
[correct] no_actions is False
task_labels:  tensor([1, 2, 1, 0, 2, 0, 2, 1, 0, 1, 0, 0, 2, 1, 0, 1, 1, 2, 1, 1, 2, 2, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 0, 2, 2, 2, 1, 0, 1, 2, 1, 1, 1, 0, 2, 1,
        0, 1, 1, 2, 0, 0, 1, 1, 2, 1, 2, 2, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1,
        0, 2, 1, 2, 1, 2, 0, 0, 2, 2, 2, 1, 0, 1, 1, 1, 1, 2, 0, 1, 0, 2, 0, 1,
        1, 0, 2, 1, 2, 1, 1, 0, 1, 2, 2, 2, 0, 1, 2, 1, 0, 1, 1, 0, 1, 2, 1, 2,
        1, 0, 2, 0, 2, 1, 0, 1]) , task_preds: tensor([[ 6.7531e-02, -3.4551e-02, -4.0097e-02],
        [ 4.2011e-02, -8.6595e-03, -7.5875e-02],
        [ 2.9908e-02,  7.7056e-03, -1.0486e-01],
        [ 1.9647e-02,  2.8511e-02, -1.2429e-01],
        [ 1.7739e-02,  3.8119e-02, -1.1898e-01],
        [ 2.3360e-02,  3.3381e-02, -1.3456e-01],
        [ 3.2094e-02, -2.6522e-02, -8.6653e-02],
        [-1.6664e-02, -5.7510e-02,  1.0424e-01],
        [ 1.9507e-02,  4.2735e-02, -1.2221e-01],
        [ 1.6525e-02,  3.5969e-02, -1.2668e-01],
        [ 6.7936e-02, -8.0777e-02,  1.8600e-02],
        [ 9.6182e-03,  3.9440e-02, -1.1808e-01],
        [ 2.4901e-02,  4.0570e-02, -1.1185e-01],
        [ 2.6542e-02, -8.6630e-02,  1.3908e-01],
        [ 1.4471e-02,  2.1575e-02, -1.1837e-01],
        [ 3.1244e-02,  3.1868e-02, -1.0575e-01],
        [-1.6329e-02, -8.1067e-02,  1.4896e-01],
        [ 4.2588e-02, -7.2734e-03, -9.9224e-02],
        [ 2.1130e-02, -2.4802e-02, -5.9807e-02],
        [ 2.1066e-02,  3.2537e-02, -1.2351e-01],
        [ 4.6848e-02, -9.1637e-02,  1.1240e-01],
        [-8.5211e-03, -9.4225e-02,  1.6279e-01],
        [ 1.1979e-02,  3.8020e-02, -1.1267e-01],
        [ 6.5360e-02, -3.5013e-02, -4.5675e-02],
        [ 4.5137e-02,  6.3756e-03, -1.0971e-01],
        [ 1.1979e-02,  3.8020e-02, -1.1267e-01],
        [ 3.0348e-02, -6.3983e-02,  9.0224e-02],
        [ 2.1470e-02,  2.6846e-02, -1.2201e-01],
        [ 2.0316e-02,  2.9860e-02, -1.1131e-01],
        [ 4.8063e-02, -2.3073e-02, -7.6456e-02],
        [ 6.9209e-02, -4.4641e-02, -6.1773e-02],
        [ 2.6996e-02,  2.9614e-02, -1.2537e-01],
        [ 1.8277e-02,  3.8756e-02, -1.1854e-01],
        [-8.0317e-03, -6.9498e-02,  1.7543e-01],
        [ 9.8867e-02, -1.1589e-01,  7.9382e-02],
        [ 6.2925e-03, -6.0860e-02,  1.8567e-02],
        [-1.6755e-02, -1.0461e-01,  1.1816e-01],
        [ 3.3425e-02, -4.9044e-02, -3.4836e-02],
        [-3.8615e-03, -1.0503e-01,  1.8066e-01],
        [ 1.9655e-02,  3.2559e-02, -1.1980e-01],
        [-9.9100e-03, -1.1139e-01,  1.3070e-01],
        [ 5.2447e-02,  6.1886e-03, -1.0608e-01],
        [ 3.2408e-03,  3.7154e-02, -1.0568e-01],
        [ 5.2247e-03,  1.4164e-02, -1.0178e-01],
        [ 9.4713e-03,  3.3476e-02, -1.1749e-01],
        [ 6.0002e-02, -4.3680e-02, -1.4830e-02],
        [ 2.0411e-02,  2.7795e-02, -1.2281e-01],
        [-8.8960e-04, -8.8256e-02,  1.5927e-01],
        [ 4.1051e-02, -1.2940e-01,  1.3936e-01],
        [ 4.5469e-03, -7.8685e-02,  1.6565e-01],
        [ 1.5928e-02,  4.5008e-02, -1.0003e-01],
        [ 1.5655e-02,  2.0035e-02, -1.1919e-01],
        [ 5.9485e-02, -1.0305e-01,  5.9542e-02],
        [ 3.9534e-02, -1.1684e-02, -1.2798e-01],
        [ 3.7023e-02, -1.1380e-01,  1.2431e-01],
        [ 4.0025e-02, -6.5864e-03, -1.0417e-01],
        [ 5.0209e-03,  2.6913e-02, -1.0400e-01],
        [ 2.4850e-02,  2.9883e-02, -1.2140e-01],
        [-3.0571e-03, -7.7612e-02,  1.2485e-01],
        [ 2.0567e-02, -1.2702e-03, -4.3863e-02],
        [ 1.8713e-02,  2.6401e-02, -1.2058e-01],
        [ 2.3064e-02,  2.1275e-02, -1.1503e-01],
        [ 4.9209e-02, -8.7706e-02,  5.9181e-02],
        [ 2.8004e-02,  4.8792e-02, -1.2219e-01],
        [ 4.1014e-02, -6.7876e-02, -2.8586e-02],
        [ 4.9306e-03, -1.1298e-01,  1.4494e-01],
        [ 1.2520e-02,  3.2485e-02, -1.3745e-01],
        [ 1.1979e-02,  3.8020e-02, -1.1267e-01],
        [ 1.1979e-02,  3.8020e-02, -1.1267e-01],
        [ 2.7223e-02,  2.6221e-03, -1.1365e-01],
        [ 1.8601e-02,  2.6032e-02, -1.1772e-01],
        [ 4.3436e-02, -3.6119e-02,  1.8708e-07],
        [ 2.1458e-02, -1.0798e-01,  1.0921e-01],
        [-2.4414e-02, -8.4858e-02,  1.3088e-01],
        [ 1.0288e-02,  1.7063e-02, -1.1298e-01],
        [ 1.1202e-02, -4.1870e-02, -7.2398e-02],
        [ 8.9545e-03, -6.3236e-02,  1.4610e-01],
        [ 6.0405e-02, -7.7671e-02,  1.8411e-02],
        [ 3.4141e-02,  3.2138e-03, -1.2386e-01],
        [ 2.7796e-02, -5.7903e-02,  8.3134e-02],
        [ 1.2934e-02, -7.9409e-02,  1.3243e-01],
        [ 4.8965e-02, -7.4763e-02,  1.7561e-02],
        [ 4.8050e-02, -7.9068e-02,  8.7938e-02],
        [ 3.6307e-02, -1.4673e-02, -6.7368e-02],
        [ 1.9499e-02,  3.2823e-02, -1.2372e-01],
        [ 6.0509e-03,  3.6048e-02, -1.1261e-01],
        [ 5.0935e-02, -1.0393e-01,  1.2786e-01],
        [ 1.0597e-02,  3.4737e-02, -1.1820e-01],
        [ 5.1070e-02, -1.1468e-02, -8.0543e-02],
        [ 3.4631e-02, -1.0921e-01,  1.3303e-01],
        [ 1.5477e-02,  3.5168e-02, -1.1185e-01],
        [ 4.6850e-02, -8.5564e-02,  1.2200e-01],
        [ 4.0775e-02, -5.6696e-02, -5.7474e-03],
        [ 1.1979e-02,  3.8020e-02, -1.1267e-01],
        [ 3.2078e-02, -1.4570e-01,  1.0652e-01],
        [ 3.8357e-02, -6.7065e-02,  4.5042e-02],
        [ 1.6710e-02,  4.1811e-02, -1.1184e-01],
        [ 1.0295e-02,  3.6786e-02, -1.1279e-01],
        [ 2.7504e-03,  3.0678e-02, -1.0430e-01],
        [ 3.7835e-02, -9.4012e-03, -3.4331e-02],
        [ 3.3477e-02, -1.1695e-01,  8.8378e-02],
        [ 1.3735e-02,  3.1669e-02, -1.2252e-01],
        [ 6.6647e-02, -3.9462e-02, -1.5326e-02],
        [ 5.2194e-02, -8.6619e-02,  5.9892e-02],
        [ 5.5525e-02, -1.7236e-02, -9.0099e-02],
        [ 5.0344e-02, -1.0534e-01,  1.1143e-01],
        [ 4.2102e-02, -2.4234e-02, -4.1492e-02],
        [ 2.8982e-02, -9.1644e-02,  1.6440e-01],
        [ 6.7568e-02, -3.9213e-03, -1.0372e-01],
        [ 1.7361e-02,  3.1313e-02, -1.1967e-01],
        [ 3.4543e-02, -4.1239e-02, -5.5976e-02],
        [ 7.1187e-03,  2.3960e-02, -1.1452e-01],
        [ 6.2962e-02, -6.9872e-02, -1.8756e-02],
        [ 6.4755e-02, -6.7774e-02,  9.0468e-02],
        [ 2.0051e-03, -8.6936e-02,  1.1512e-01],
        [ 5.0813e-02, -6.2578e-02, -8.6197e-03],
        [ 5.0472e-02,  6.1402e-03, -4.5272e-02],
        [-5.0701e-03, -5.8203e-02,  9.8471e-02],
        [ 3.1829e-02, -6.8794e-03, -1.2081e-01],
        [ 1.7824e-02,  3.1492e-02, -1.3201e-01],
        [ 1.5072e-03,  3.0777e-02, -1.3578e-01],
        [ 3.6028e-02, -4.0645e-02,  2.5020e-02],
        [ 1.2098e-02,  3.7277e-02, -1.1262e-01],
        [ 1.9466e-02,  3.4823e-02, -1.1613e-01],
        [ 1.5743e-02,  2.9596e-02, -1.2236e-01],
        [ 6.9954e-02, -9.3033e-03, -1.1647e-01],
        [ 2.2716e-02, -9.7918e-02,  1.3884e-01],
        [ 1.5428e-02,  2.3000e-02, -1.1397e-01]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000025
penultimate_layer.0.bias: grad mean = 0.000708
output_layer.0.weight: grad mean = 0.003658
output_layer.0.bias: grad mean = 0.022236
[correct] no_actions is False
task_labels:  tensor([2, 2, 2, 0, 1, 2, 1, 1, 0, 0, 0, 0, 1, 1, 2, 0, 2, 0, 1, 0, 1, 1, 2, 0,
        2, 1, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0, 1, 0, 1, 0, 2, 0,
        1, 1, 0, 0, 1, 1, 2, 0, 1, 0, 1, 1, 2, 1, 0, 1, 2, 2, 1, 0, 2, 2, 2, 2,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 1, 1, 0, 1, 1, 2, 2, 0, 1, 1, 1, 0,
        1, 0, 2, 0, 1, 2, 2, 1, 1, 1, 2, 0, 2, 0, 0, 1, 0, 1, 0, 1, 2, 1, 0, 2,
        0, 0, 2, 1, 0, 1, 1, 2]) , task_preds: tensor([[ 8.3490e-03,  4.0635e-02, -1.0323e-01],
        [ 4.1754e-02, -2.3946e-02, -4.1474e-02],
        [ 4.2588e-02, -6.1392e-02,  4.9792e-02],
        [ 7.0404e-02, -5.7216e-02,  1.1229e-01],
        [-2.8433e-02, -8.4363e-02,  1.0193e-01],
        [ 3.8397e-02, -1.2679e-02, -9.3035e-02],
        [ 9.9339e-03,  1.7853e-02, -1.1367e-01],
        [ 3.0046e-02, -6.4004e-02,  9.0622e-02],
        [ 5.1952e-02,  6.2522e-03, -1.2814e-01],
        [ 5.3583e-02, -9.5757e-02,  1.4809e-01],
        [ 2.1725e-02, -9.0746e-02,  1.3358e-01],
        [-5.4924e-02, -6.4588e-02,  1.0622e-01],
        [ 8.8470e-03, -6.3427e-02,  1.4650e-01],
        [ 1.0562e-02,  1.3217e-02, -1.1413e-01],
        [ 2.7865e-02, -1.6469e-02, -3.8547e-02],
        [ 6.9238e-02, -4.4421e-02, -6.2123e-02],
        [ 2.7511e-02, -1.0954e-01,  5.9943e-02],
        [ 2.1529e-02,  3.2832e-02, -1.2727e-01],
        [ 3.5775e-02, -1.1758e-02, -6.0631e-02],
        [ 5.9724e-02, -1.0346e-01,  5.9837e-02],
        [ 1.1428e-02, -8.1506e-02,  3.5025e-02],
        [ 7.0004e-02, -1.3832e-02, -1.0300e-01],
        [ 3.8416e-02, -4.6764e-02,  1.3230e-02],
        [ 6.6622e-03,  3.9633e-02, -1.1828e-01],
        [ 1.6142e-02,  2.6954e-02, -1.2749e-01],
        [ 2.4536e-02,  3.0617e-02, -1.2206e-01],
        [ 1.3319e-02, -6.1312e-02, -4.0263e-02],
        [ 3.5998e-02, -7.1799e-02,  1.6749e-01],
        [ 2.3190e-02,  2.8673e-02, -1.1942e-01],
        [ 7.0021e-03,  4.4375e-02, -1.2833e-01],
        [ 1.4052e-02,  2.9141e-02, -1.4488e-01],
        [ 6.5762e-02, -3.8690e-02, -1.2188e-02],
        [ 6.0629e-02, -4.4685e-02, -5.0806e-02],
        [ 2.3558e-02,  2.6278e-02, -1.1879e-01],
        [ 5.0455e-02, -9.7952e-02,  7.4448e-02],
        [ 3.8355e-02, -8.6628e-02,  1.2161e-01],
        [ 2.1670e-02, -1.0187e-01,  1.3258e-01],
        [ 3.3466e-02, -6.0979e-02, -2.0835e-02],
        [ 5.0396e-03,  2.9024e-02, -1.1095e-01],
        [ 5.0526e-02, -3.2121e-02, -4.5003e-02],
        [ 2.1718e-02,  3.4696e-02, -1.2074e-01],
        [ 1.9178e-02,  3.1829e-02, -1.1907e-01],
        [-1.5184e-03, -8.4273e-02,  1.2407e-01],
        [ 2.0546e-02, -9.4815e-02,  1.0428e-01],
        [ 5.6942e-02, -2.4942e-02, -9.0675e-02],
        [ 6.1114e-02, -6.8166e-02, -2.8829e-02],
        [ 3.1619e-02, -7.7544e-02,  1.0148e-01],
        [ 4.4277e-02,  9.4489e-03, -1.2280e-01],
        [ 3.3668e-02,  2.3250e-02, -1.1409e-01],
        [ 6.6360e-03, -2.8945e-02,  9.7188e-02],
        [ 4.2446e-02, -1.2083e-01,  1.2213e-01],
        [ 7.7002e-02, -1.1120e-01,  7.4460e-02],
        [ 2.1654e-02,  4.7770e-02, -1.2438e-01],
        [ 8.4708e-03,  2.9837e-02, -1.2146e-01],
        [ 6.3787e-02, -3.5584e-02, -2.0021e-02],
        [ 1.6056e-02, -5.6564e-02,  1.7082e-01],
        [ 1.3505e-02,  3.6340e-02, -1.3483e-01],
        [ 7.5017e-02,  5.1818e-06, -9.9589e-02],
        [ 1.1567e-02, -5.3722e-02,  2.8143e-02],
        [ 1.6703e-02, -1.1029e-01,  1.4421e-01],
        [ 6.0095e-02, -7.7689e-02,  1.8818e-02],
        [ 3.6084e-02, -7.3637e-02, -1.6608e-02],
        [ 1.8492e-02,  2.7105e-02, -1.2131e-01],
        [ 4.3130e-02, -3.5873e-02,  2.2280e-05],
        [ 9.4872e-03, -4.2164e-02, -5.8616e-02],
        [ 1.1961e-02,  3.3445e-02, -1.1469e-01],
        [ 2.0033e-02,  3.8538e-02, -1.2582e-01],
        [ 6.2440e-03, -9.0611e-02,  1.2769e-01],
        [ 4.0739e-02, -4.0412e-03, -8.6720e-02],
        [ 4.4949e-02,  1.2501e-03, -1.3649e-01],
        [ 1.0954e-02,  3.6214e-02, -1.2204e-01],
        [ 3.5917e-02, -8.5203e-02,  9.6394e-02],
        [ 7.3240e-03, -9.8905e-02,  1.2766e-01],
        [ 5.7455e-02, -1.1621e-01,  1.1237e-01],
        [ 1.7509e-02,  2.3051e-02, -1.1418e-01],
        [ 5.9230e-03,  3.8524e-02, -1.1101e-01],
        [ 2.0309e-02,  4.0021e-02, -1.2218e-01],
        [ 1.3417e-02,  3.4241e-02, -1.1376e-01],
        [ 1.1118e-02,  2.6173e-03, -1.0023e-01],
        [-3.6632e-02, -8.2414e-02,  1.0168e-01],
        [ 1.5041e-02,  2.8750e-02, -1.1151e-01],
        [ 2.7626e-02,  3.8622e-02, -1.2385e-01],
        [-1.1801e-04, -5.8022e-02,  1.1158e-01],
        [ 5.2770e-02, -2.3245e-02, -4.3673e-02],
        [ 3.1630e-02, -6.7046e-02, -7.1929e-02],
        [ 1.3988e-02,  3.6995e-02, -1.1861e-01],
        [ 7.9802e-02, -6.2896e-02, -7.0396e-02],
        [ 2.9634e-02,  8.4116e-03, -1.0553e-01],
        [ 4.7470e-02, -9.5874e-02,  1.0670e-01],
        [ 2.5343e-02, -8.4172e-02,  1.0655e-01],
        [ 2.0991e-02,  4.2662e-02, -1.1578e-01],
        [ 1.9424e-02,  3.3268e-02, -1.2053e-01],
        [ 8.8388e-02, -5.0413e-02, -6.1583e-02],
        [ 4.4789e-03,  3.6402e-02, -1.1451e-01],
        [ 2.7221e-02,  2.9056e-02, -1.4182e-01],
        [ 3.8589e-02,  3.3285e-03, -7.9874e-02],
        [ 1.8257e-02,  2.8613e-02, -1.1463e-01],
        [ 8.1278e-03, -8.0836e-02,  9.6197e-02],
        [-1.2170e-02, -8.1762e-02,  1.4571e-01],
        [ 4.6286e-02, -4.6679e-02, -8.6365e-03],
        [ 8.7159e-03,  3.3967e-02, -1.3672e-01],
        [ 9.1428e-03, -9.9587e-02,  1.3905e-01],
        [-4.8905e-02, -5.9393e-02,  1.2206e-01],
        [ 1.2708e-02,  4.6411e-02, -1.1196e-01],
        [ 1.6114e-02,  3.4031e-02, -1.1636e-01],
        [ 3.1626e-02,  1.5606e-02, -1.0932e-01],
        [ 6.2405e-02, -3.8613e-02, -4.0132e-02],
        [ 5.6501e-02, -3.8696e-02, -4.3496e-02],
        [-2.1970e-02, -8.1852e-02,  1.5014e-01],
        [ 9.1111e-02, -8.0291e-02, -6.7901e-02],
        [ 1.8289e-02,  3.0575e-02, -1.2445e-01],
        [-1.2214e-02, -7.8899e-02,  1.2178e-01],
        [ 5.2368e-02,  4.9196e-03, -1.1068e-01],
        [ 1.0402e-02,  3.9143e-02, -1.2444e-01],
        [ 6.1842e-02, -7.0663e-03, -6.0980e-02],
        [ 4.0722e-02, -1.9588e-02, -7.1930e-02],
        [ 1.1736e-02,  3.8731e-02, -1.1338e-01],
        [ 1.3385e-02,  3.2455e-02, -1.2321e-01],
        [ 2.0861e-02,  4.4089e-02, -1.1378e-01],
        [ 2.5035e-02, -5.3491e-02,  2.8455e-02],
        [ 1.4857e-02,  3.0062e-02, -1.1749e-01],
        [ 2.2956e-02,  3.9313e-02, -1.3642e-01],
        [ 2.2544e-02, -6.8768e-02, -5.5686e-02],
        [ 2.6195e-02,  2.7046e-02, -1.1450e-01],
        [ 5.8028e-03, -8.7082e-02,  1.5614e-01],
        [ 4.1799e-02, -1.0934e-01,  1.4704e-01],
        [ 3.7360e-02, -8.9791e-03, -3.4351e-02],
        [ 4.0389e-02, -8.2830e-02,  1.6131e-01]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000024
penultimate_layer.0.bias: grad mean = 0.000685
output_layer.0.weight: grad mean = 0.002592
output_layer.0.bias: grad mean = 0.022601
[correct] no_actions is False
task_labels:  tensor([2, 1, 1, 1, 2, 0, 0, 0, 1, 2, 0, 2, 2, 2, 2, 2, 0, 0, 1, 1, 1, 2, 1, 1,
        2, 1, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 1,
        2, 2, 1, 1, 2, 2, 1, 2, 2, 0, 0, 1, 1, 2, 1, 0, 2, 2, 0, 2, 1, 0, 0, 2,
        0, 1, 2, 1, 2, 2, 1, 0, 1, 0, 0, 1, 2, 0, 0, 2, 1, 2, 0, 1, 0, 1, 1, 0,
        2, 1, 2, 0, 0, 0, 2, 1, 1, 0, 2, 1, 0, 2, 0, 1, 0, 1, 1, 1, 0, 1, 2, 0,
        0, 1, 2, 2, 0, 0, 0, 2]) , task_preds: tensor([[ 6.8344e-02, -1.5511e-02, -8.6956e-02],
        [ 1.1470e-02,  3.9546e-02, -1.1422e-01],
        [ 3.9116e-02,  9.1661e-03, -1.1835e-01],
        [ 1.9478e-02, -5.4812e-02,  5.1314e-02],
        [ 3.6515e-02, -7.0849e-02,  2.9893e-02],
        [ 7.4269e-02, -8.7055e-02,  7.4639e-04],
        [ 6.1970e-02, -1.4819e-03, -9.3410e-02],
        [ 6.8085e-02, -8.1573e-03, -1.1667e-01],
        [ 3.8979e-02,  1.1569e-02, -8.1866e-02],
        [ 2.9797e-02, -5.7829e-03, -6.1809e-02],
        [ 1.1235e-02,  4.0637e-02, -1.2485e-01],
        [ 4.2855e-03, -7.2819e-02,  1.6743e-01],
        [ 1.9664e-02, -9.3216e-02,  1.6513e-01],
        [ 2.6175e-02,  1.0524e-02, -1.1271e-01],
        [ 4.0244e-03, -3.9170e-02,  4.4247e-02],
        [ 1.1470e-02,  3.9546e-02, -1.1422e-01],
        [ 5.8093e-02, -1.8306e-02, -4.4979e-02],
        [ 1.3568e-02,  3.7012e-02, -1.1074e-01],
        [ 3.5132e-02, -2.0683e-02, -5.6468e-02],
        [ 1.3016e-02,  3.5175e-02, -1.3109e-01],
        [ 6.7578e-02, -6.1318e-02, -3.5639e-02],
        [ 3.6494e-02, -5.7223e-02,  1.8886e-01],
        [ 6.2545e-02, -1.5277e-04, -1.0716e-01],
        [ 1.1470e-02,  3.9546e-02, -1.1422e-01],
        [ 2.7296e-02, -8.0918e-02,  1.3676e-01],
        [ 1.2515e-02,  2.1943e-03, -1.2001e-01],
        [ 1.8635e-02, -9.0020e-02,  1.3563e-01],
        [ 5.4543e-02, -1.0508e-01,  1.2152e-01],
        [ 5.3316e-02, -5.8643e-02, -3.7067e-02],
        [ 2.8694e-02, -1.0967e-01,  1.4458e-01],
        [ 5.2537e-02, -8.7149e-02,  6.0228e-02],
        [ 1.1670e-02,  3.4122e-02, -1.1622e-01],
        [ 3.2487e-02, -1.4634e-01,  1.0692e-01],
        [ 7.9783e-02, -6.3367e-02, -5.4957e-02],
        [ 6.2123e-02, -9.8401e-03, -5.6949e-02],
        [ 2.2933e-02, -9.8503e-02,  1.3940e-01],
        [ 6.9782e-02, -1.3556e-02, -6.7880e-02],
        [ 3.7327e-02, -1.1435e-01,  1.2471e-01],
        [ 1.5134e-02,  3.3772e-02, -1.2297e-01],
        [ 4.0882e-02, -1.0135e-01,  1.1300e-01],
        [ 2.1590e-02,  4.4272e-02, -1.2318e-01],
        [ 2.0582e-02, -2.3896e-02, -6.0435e-02],
        [ 3.2670e-02, -1.0593e-01,  1.3481e-01],
        [ 1.3869e-02,  8.8674e-03, -1.2124e-01],
        [ 1.7181e-02, -9.0079e-02,  8.3333e-02],
        [ 7.9069e-03, -9.6272e-02,  7.7492e-02],
        [ 1.6433e-02,  4.6673e-02, -1.2013e-01],
        [ 3.6019e-02, -1.0414e-01,  1.5744e-01],
        [ 5.0090e-02, -9.0765e-02,  7.9485e-02],
        [-5.4665e-02, -8.3496e-02,  9.4710e-02],
        [-1.1332e-03, -8.8597e-02,  1.6005e-01],
        [ 1.8366e-02, -9.2420e-02,  1.3550e-01],
        [ 3.0061e-02, -9.4806e-03, -3.6649e-02],
        [ 8.6938e-03, -5.8786e-02,  1.6920e-02],
        [ 1.2829e-02,  3.5806e-02, -1.3118e-01],
        [ 2.0167e-02,  5.1334e-02, -1.1558e-01],
        [ 1.4487e-02,  3.4106e-02, -1.1527e-01],
        [ 2.8337e-02,  2.8577e-02, -1.3599e-01],
        [ 5.6493e-02, -1.3217e-02, -1.0069e-01],
        [ 3.8465e-02,  1.9260e-03, -1.0933e-01],
        [ 3.9262e-02, -5.8759e-02, -8.1855e-02],
        [ 3.3420e-02,  3.4465e-02, -1.2862e-01],
        [ 4.3427e-02, -8.3804e-02,  8.2723e-02],
        [ 4.3176e-02, -3.4762e-03, -9.4894e-02],
        [ 1.1470e-02,  3.9546e-02, -1.1422e-01],
        [ 4.1008e-02, -1.4053e-02, -1.1972e-01],
        [ 7.1427e-02, -1.0714e-01,  9.4693e-03],
        [ 1.1910e-02,  3.5471e-02, -1.1404e-01],
        [ 3.5167e-02,  1.0679e-02, -1.1529e-01],
        [ 5.2548e-02, -5.5196e-02, -1.7766e-02],
        [ 5.6922e-02, -8.4564e-02,  6.4380e-02],
        [ 2.3190e-02,  3.7522e-02, -1.2163e-01],
        [ 5.9156e-02, -9.4905e-02,  2.5579e-02],
        [ 5.8843e-03,  3.8693e-02, -1.1191e-01],
        [ 3.4904e-02, -9.9304e-02,  1.6302e-01],
        [ 1.1611e-02,  3.9987e-02, -1.1921e-01],
        [ 1.1633e-03, -1.0074e-01,  1.3199e-01],
        [ 2.6029e-02,  4.1552e-02, -1.0862e-01],
        [ 2.0750e-02,  1.0295e-03, -9.8819e-02],
        [ 2.4795e-02,  4.6050e-02, -1.0670e-01],
        [ 5.7478e-02, -5.4063e-02,  5.4558e-04],
        [ 7.8724e-02, -6.0014e-02,  7.4360e-04],
        [ 8.6285e-02, -2.5511e-02, -7.6008e-02],
        [ 5.8388e-02, -6.9929e-02,  1.0304e-01],
        [ 4.6141e-03,  3.2040e-02, -1.2977e-01],
        [ 5.5584e-02, -9.0030e-02,  1.5671e-01],
        [ 2.6087e-02,  2.3639e-02, -1.2356e-01],
        [ 5.0652e-02, -7.3801e-02,  2.7322e-02],
        [ 2.2195e-02,  3.9834e-02, -1.0962e-01],
        [ 5.5795e-02, -5.5440e-02,  1.4825e-01],
        [ 1.3276e-02,  3.5205e-02, -1.1860e-01],
        [ 4.3482e-03, -7.9048e-02,  1.6640e-01],
        [ 2.7075e-02,  2.7448e-02, -1.1556e-01],
        [ 1.9612e-03, -8.7226e-02,  1.1559e-01],
        [ 3.6801e-02, -3.3883e-02, -4.1145e-02],
        [ 5.1499e-02, -2.8876e-02, -7.1685e-02],
        [ 1.3949e-02,  3.6694e-02, -1.1638e-01],
        [ 1.1090e-02, -5.6779e-02,  1.4277e-01],
        [ 3.6385e-03, -1.1115e-01,  1.3696e-01],
        [ 4.8010e-02,  5.0247e-03, -1.4039e-01],
        [ 1.4366e-02,  3.3779e-02, -1.2426e-01],
        [ 8.6064e-03,  3.4081e-02, -1.2084e-01],
        [-3.0595e-04, -7.4509e-02,  1.7193e-01],
        [ 1.1149e-03, -7.8172e-02,  1.1442e-01],
        [ 1.2734e-02,  4.0192e-02, -1.2016e-01],
        [ 2.2976e-02,  2.9180e-02, -1.3589e-01],
        [ 2.3944e-02,  2.9638e-02, -1.2968e-01],
        [ 1.7069e-02,  3.4978e-02, -1.1781e-01],
        [ 2.2657e-02,  3.6984e-02, -1.1083e-01],
        [ 5.1988e-02,  7.5403e-03, -1.0745e-01],
        [ 1.0679e-02, -9.3866e-02,  1.2468e-01],
        [ 1.5921e-02,  3.5483e-02, -1.1264e-01],
        [ 3.7804e-02, -6.7587e-02, -5.0669e-03],
        [ 3.3428e-02,  4.3750e-02, -1.1872e-01],
        [ 5.1142e-03,  2.0946e-02, -1.1758e-01],
        [ 1.3363e-02,  4.6473e-02, -1.0784e-01],
        [ 6.4101e-02, -1.2659e-01,  1.2900e-01],
        [ 1.4869e-02,  2.4417e-02, -1.1531e-01],
        [ 1.3441e-02,  3.9615e-02, -1.1310e-01],
        [ 1.4809e-02,  4.1032e-03, -1.0769e-01],
        [ 2.7086e-02,  3.5954e-02, -1.2721e-01],
        [ 8.2323e-03, -6.8495e-02,  1.2861e-01],
        [ 9.2770e-04, -1.0992e-01,  1.4611e-01],
        [ 5.7839e-02, -6.3796e-02, -2.9914e-02],
        [ 2.7766e-02,  6.0858e-03, -1.0792e-01],
        [ 6.5158e-02, -3.4631e-02, -4.5972e-02],
        [ 1.4662e-02,  3.5420e-02, -1.1470e-01],
        [ 3.4094e-02, -7.3948e-02,  3.2505e-02]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000025
penultimate_layer.0.bias: grad mean = 0.000565
output_layer.0.weight: grad mean = 0.003241
output_layer.0.bias: grad mean = 0.019263
[correct] no_actions is False
task_labels:  tensor([1, 0, 2, 2, 2, 1, 2, 2, 0, 0, 0, 1, 0, 2, 2, 0, 0, 1, 0, 0, 2, 0, 1, 2,
        2, 0, 1, 2, 1, 2, 0, 1, 0, 1, 1, 1, 1, 2, 2, 2, 0, 2, 0, 2, 1, 2, 2, 2,
        2, 2, 1, 0, 1, 1, 0, 0, 1, 0, 0, 2, 2, 1, 0, 1, 1, 0, 0, 0, 0, 1, 2, 1,
        1, 1, 2, 1, 1, 2, 0, 0, 0, 0, 2, 2, 1, 0, 0, 2, 0, 0, 2, 0, 2, 2, 1, 0,
        0, 2, 1, 2, 0, 0, 0, 0, 1, 1, 1, 2, 2, 1, 0, 0, 1, 2, 0, 2, 0, 0, 0, 1,
        1, 0, 1, 2, 1, 2, 0, 1]) , task_preds: tensor([[ 0.0419, -0.0132, -0.0523],
        [ 0.0162, -0.0570,  0.1712],
        [ 0.0048, -0.1135,  0.1459],
        [ 0.0042,  0.0367, -0.1257],
        [ 0.0055, -0.0605,  0.0190],
        [ 0.0165, -0.0322, -0.1038],
        [-0.0293, -0.0884,  0.1411],
        [ 0.0635, -0.0352, -0.0202],
        [ 0.0324, -0.1079,  0.1292],
        [ 0.0233,  0.0407, -0.1281],
        [ 0.0281, -0.0293, -0.0607],
        [ 0.0344, -0.1007,  0.0984],
        [ 0.0673, -0.0027, -0.1051],
        [ 0.0232,  0.0378, -0.1220],
        [ 0.0394, -0.0850,  0.1145],
        [ 0.0389, -0.0072, -0.1000],
        [ 0.0755, -0.0297, -0.0745],
        [ 0.0084,  0.0397, -0.1181],
        [ 0.0293,  0.0157, -0.1157],
        [ 0.0155,  0.0452, -0.1282],
        [ 0.0106,  0.0373, -0.1232],
        [ 0.0604,  0.0020, -0.0938],
        [ 0.0116,  0.0332, -0.1243],
        [ 0.0193,  0.0157, -0.1073],
        [-0.0003,  0.0277, -0.1324],
        [ 0.0654, -0.0732, -0.0287],
        [ 0.0697, -0.0376, -0.0390],
        [ 0.0295, -0.1129,  0.1494],
        [ 0.0564,  0.0120, -0.0937],
        [-0.0063, -0.0882,  0.0715],
        [ 0.0773, -0.1116,  0.0746],
        [ 0.0125,  0.0024, -0.1203],
        [ 0.0319,  0.0031, -0.1252],
        [ 0.0351,  0.0109, -0.1156],
        [ 0.0197, -0.0104, -0.0684],
        [ 0.0114,  0.0398, -0.1146],
        [ 0.0464,  0.0057, -0.0706],
        [ 0.0270, -0.0627,  0.1423],
        [ 0.0201,  0.0516, -0.1159],
        [ 0.0590, -0.0684, -0.0578],
        [ 0.0195,  0.0321, -0.1212],
        [ 0.0156, -0.0893,  0.1429],
        [ 0.0209, -0.0953,  0.1046],
        [ 0.0166, -0.0845,  0.1837],
        [ 0.0374, -0.1144,  0.1247],
        [-0.0491, -0.0598,  0.1228],
        [ 0.0039, -0.0392,  0.0444],
        [ 0.0172,  0.0352, -0.1168],
        [ 0.0232,  0.0192, -0.1148],
        [ 0.0655, -0.0442, -0.0035],
        [ 0.0562,  0.0025, -0.1475],
        [ 0.0056,  0.0410, -0.1233],
        [ 0.0590, -0.0672, -0.0380],
        [ 0.0333,  0.0243, -0.1151],
        [ 0.0505, -0.0623, -0.0087],
        [ 0.0210,  0.0285, -0.1239],

MLP Fine-Tuning: task_loss = 1.0863:  93%|█████████▎| 14/15 [01:05<00:01,  1.30s/it]
        [-0.0547, -0.0836,  0.0950],
        [ 0.0092, -0.0999,  0.1393],
        [-0.0171, -0.0575,  0.1047],
        [ 0.0621, -0.0465, -0.0595],
        [ 0.0112,  0.0409, -0.1252],
        [ 0.0461, -0.0746,  0.0465],
        [ 0.0387,  0.0155, -0.1120],
        [ 0.0479, -0.0726,  0.0737],
        [ 0.0288, -0.1098,  0.1447],
        [ 0.0531, -0.0771,  0.0390],
        [ 0.0326, -0.1465,  0.1069],
        [ 0.0114,  0.0398, -0.1146],
        [ 0.0356,  0.0132, -0.1238],
        [ 0.0110,  0.0277, -0.1204],
        [ 0.0342, -0.0148, -0.0622],
        [ 0.0495, -0.0792,  0.0015],
        [ 0.0349, -0.0994,  0.1631],
        [ 0.0136, -0.0415, -0.0246],
        [ 0.0680, -0.0809,  0.0187],
        [ 0.0405,  0.0270, -0.1250],
        [ 0.0137,  0.0277, -0.1212],
        [ 0.0413, -0.0822,  0.0921],
        [ 0.0428, -0.1033,  0.1566],
        [ 0.0598, -0.0777,  0.0191],
        [ 0.0221,  0.0282, -0.1257],
        [ 0.0002, -0.0585,  0.1118],
        [ 0.0565, -0.0131, -0.1009],
        [ 0.0172,  0.0241, -0.1153],
        [ 0.0298, -0.0639,  0.0908],
        [ 0.0684, -0.0409, -0.0020],
        [-0.0284, -0.0847,  0.1024],
        [ 0.0080,  0.0418, -0.1044],
        [ 0.0899, -0.0641, -0.0681],
        [ 0.0478, -0.0644,  0.0623],
        [ 0.0415, -0.1302,  0.1399],
        [ 0.0547, -0.1052,  0.1215]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000023
penultimate_layer.0.bias: grad mean = 0.000318
output_layer.0.weight: grad mean = 0.002746
output_layer.0.bias: grad mean = 0.009949
[correct] no_actions is False
task_labels:  tensor([1, 1, 0, 2, 2, 2, 1, 0, 0, 1, 2, 1, 1, 0, 0, 0, 1, 1, 1, 2, 1, 0, 0, 0,
        0, 1, 0, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 0, 1, 1, 1, 2, 2, 2, 1, 2, 0,
        1, 0, 2, 2, 1, 0, 0, 0, 1, 2, 0, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1,
        1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 0, 2, 1, 0, 1, 0, 2, 2, 2, 0, 0, 1, 1, 0,
        1, 2, 2, 2, 1, 1, 0, 1, 0, 1, 2, 1, 0, 2, 2, 0, 0, 1, 2, 1, 1, 0, 0, 2,
        2, 1, 2, 0, 1, 2, 1, 0]) , task_preds: tensor([[ 0.0473,  0.0015, -0.0631],
        [ 0.0613, -0.0438, -0.0270],
        [ 0.0923, -0.0795, -0.0461],
        [ 0.0098,  0.0404, -0.1185],
        [ 0.0608, -0.0622, -0.0163],
        [ 0.0273,  0.0099, -0.1067],
        [ 0.0701, -0.0133, -0.1039],
        [ 0.0110, -0.0943,  0.1248],
        [ 0.0697, -0.0443, -0.0629],
        [ 0.0116,  0.0426, -0.1279],
        [ 0.0372, -0.0218, -0.0795],
        [-0.0010, -0.0889,  0.1603],
        [ 0.0267, -0.0872,  0.1396],
        [ 0.0620, -0.0066, -0.0619],
        [ 0.0261,  0.0455, -0.1239],
        [ 0.0397, -0.0798,  0.1317],
        [ 0.0084, -0.0689,  0.1290],
        [ 0.0076,  0.0414, -0.1111],
        [ 0.0644,  0.0119, -0.0974],
        [ 0.0200,  0.0295, -0.1248],
        [ 0.0120, -0.0665, -0.0009],
        [ 0.0255,  0.0375, -0.1129],
        [ 0.0807, -0.0924,  0.0186],
        [ 0.0521, -0.0676, -0.0098],
        [ 0.0730, -0.0374, -0.0449],
        [ 0.0231,  0.0357, -0.1278],
        [ 0.0507, -0.0318, -0.0457],
        [ 0.0503, -0.1061,  0.1127],
        [ 0.0580, -0.0639, -0.0299],
        [ 0.0404,  0.0181, -0.1232],
        [ 0.0172,  0.0347, -0.1218],
        [ 0.0252,  0.0337, -0.1264],
        [ 0.0393, -0.0588, -0.0819],
        [ 0.0264,  0.0106, -0.1132],
        [ 0.0366, -0.0712,  0.0302],
        [ 0.0335, -0.0609, -0.0210],
        [ 0.0143,  0.0229, -0.1252],
        [ 0.0118,  0.0298, -0.1245],
        [-0.0098, -0.0982,  0.1097],
        [ 0.0156,  0.0370, -0.1234],
        [ 0.0044,  0.0374, -0.1159],
        [ 0.0125,  0.0340, -0.1191],
        [ 0.0224, -0.0684, -0.0560],
        [ 0.0117,  0.0397, -0.1147],
        [ 0.0289, -0.0671, -0.0632],
        [ 0.0460,  0.0085, -0.1117],
        [ 0.0534, -0.0747,  0.0167],
        [ 0.0205,  0.0415, -0.1280],
        [ 0.0132,  0.0354, -0.1316],
        [ 0.0547, -0.1093,  0.1231],
        [ 0.0450, -0.0054, -0.0728],
        [-0.0245, -0.1059,  0.1435],
        [-0.0068, -0.0937,  0.1646],
        [ 0.0130,  0.0365, -0.1190],
        [ 0.0753, -0.1034,  0.0434],
        [ 0.1132, -0.1029,  0.0295],
        [ 0.0379, -0.0578, -0.0336],
        [ 0.0152,  0.0307, -0.1219],
        [ 0.0030, -0.1205,  0.1491],
        [-0.0002, -0.0748,  0.1722],
        [ 0.0129,  0.0384, -0.1340],
        [-0.0349, -0.0802,  0.1402],
        [-0.0229, -0.1033,  0.1564],
        [ 0.0299,  0.0403, -0.1030],
        [ 0.0083, -0.1147,  0.1682],
        [ 0.0139,  0.0387, -0.1214],
        [ 0.0305, -0.0596,  0.0652],
        [ 0.0322,  0.0356, -0.1329],
        [ 0.0336,  0.0440, -0.1192],
        [-0.0015, -0.0845,  0.1244],
        [ 0.0155,  0.0468, -0.1021],
        [ 0.0119,  0.0302, -0.1133],
        [ 0.0122,  0.0383, -0.1126],
        [ 0.0466, -0.0306, -0.0721],
        [ 0.0117,  0.0397, -0.1147],
        [ 0.0078,  0.0431, -0.1070],
        [ 0.0177, -0.0754,  0.1555],
        [ 0.0185,  0.0302, -0.1252],
        [ 0.0124, -0.0803,  0.1343],
        [ 0.0241,  0.0325, -0.1089],
        [ 0.0591,  0.0263, -0.0738],
        [ 0.0357, -0.0113, -0.0613],
        [ 0.0754,  0.0041, -0.1251],
        [ 0.0155,  0.0420, -0.1390],
        [ 0.0130,  0.0360, -0.1317],
        [ 0.0158,  0.0349, -0.1215],
        [ 0.0541, -0.0047, -0.1269],
        [-0.0043, -0.0800,  0.1098],
        [-0.0089, -0.0496,  0.1432],
        [ 0.0063, -0.0892,  0.0416],
        [ 0.0115,  0.0316, -0.1097],
        [ 0.0223, -0.0914,  0.1339],
        [ 0.0328, -0.1090,  0.0664],
        [ 0.0173, -0.0704,  0.1380],
        [ 0.0405, -0.0324,  0.0386],
        [ 0.0709, -0.0952,  0.0006],
        [ 0.0391, -0.0018, -0.1087],
        [ 0.0380, -0.0885,  0.0783],
        [ 0.0143,  0.0379, -0.1177],
        [ 0.0522,  0.0077, -0.1079],
        [ 0.0628, -0.0011, -0.1119],
        [ 0.0021, -0.0874,  0.1157],
        [ 0.0459, -0.1221,  0.1406],
        [ 0.0354, -0.0207, -0.0567],
        [ 0.0437, -0.0089, -0.0889],
        [ 0.0124,  0.0357, -0.1100],
        [ 0.0412, -0.0139, -0.1201],
        [ 0.0319, -0.0730,  0.0548],
        [ 0.0415,  0.0106, -0.1313],
        [ 0.0286, -0.0927,  0.1663],
        [ 0.0153,  0.0414, -0.1208],
        [ 0.0139,  0.0385, -0.1121],
        [ 0.0194,  0.0343, -0.1219],
        [ 0.0423, -0.1098,  0.1471],
        [ 0.0227,  0.0537, -0.1193],
        [ 0.0088,  0.0498, -0.1213],
        [ 0.0117,  0.0397, -0.1147],
        [ 0.0445,  0.0102, -0.1241],
        [ 0.0859, -0.0670, -0.1044],
        [ 0.0117,  0.0397, -0.1147],
        [ 0.0349, -0.1101,  0.1340],
        [ 0.0161,  0.0357, -0.1131],
        [-0.0194, -0.0654,  0.1812],
        [ 0.0272,  0.0242, -0.1066],
        [ 0.0378, -0.0561, -0.0249],
        [ 0.0216,  0.0429, -0.1384],
        [ 0.0059,  0.0346, -0.1192],
        [ 0.0683,  0.0061, -0.1067]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000034
penultimate_layer.0.bias: grad mean = 0.001337
output_layer.0.weight: grad mean = 0.003955
output_layer.0.bias: grad mean = 0.042308
[correct] no_actions is False
task_labels:  tensor([1, 1, 2, 2, 1, 0, 2, 1, 2, 1, 1, 0, 1, 2, 1, 2, 1, 0, 0, 1, 2, 2, 1, 2,
        0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 2, 2, 1,
        1, 0, 2, 2, 1, 1, 0, 1, 1, 0, 2, 1, 2, 0, 0, 1, 1, 2, 0, 0, 0, 2, 2, 0,
        2, 0, 2, 0, 1, 0, 2, 0, 1, 0, 1, 1, 2, 2, 0, 2, 2, 1, 1, 0, 2, 0, 0, 1,
        2, 0, 1, 0, 2, 2, 0, 1, 1, 2, 0, 2, 1, 2, 1, 0, 2, 1, 2, 1, 1, 2, 0, 0,
        1, 0, 0, 1, 2, 1, 2, 2]) , task_preds: tensor([[ 4.6617e-02, -9.4660e-03, -7.6277e-02],
        [ 8.8827e-02, -5.0489e-02, -6.2012e-02],
        [ 2.7576e-02, -8.1397e-02,  1.3709e-01],
        [ 5.3046e-03,  3.0328e-02, -1.1840e-01],
        [ 2.9059e-02, -2.4344e-02, -1.0183e-01],
        [ 7.2018e-02, -1.0768e-01,  9.5157e-03],
        [ 5.7919e-02, -6.3877e-02, -2.9895e-02],
        [ 1.7328e-02,  2.4141e-02, -1.1552e-01],
        [ 3.4339e-02, -8.4382e-02,  1.7071e-01],
        [-2.8313e-02, -8.4972e-02,  1.0265e-01],
        [ 1.1120e-02, -5.0422e-02,  2.6348e-02],
        [ 2.8224e-02,  2.1336e-02, -1.2029e-01],
        [ 5.1530e-02, -1.0494e-01,  1.2856e-01],
        [ 4.2409e-02, -6.1569e-02,  5.0264e-02],
        [ 3.2290e-02,  2.0597e-02, -5.8378e-02],
        [ 6.0507e-03,  1.8246e-02, -1.1931e-01],
        [ 6.6318e-02, -3.8912e-02, -1.5711e-02],
        [ 3.2922e-02, -8.1823e-03, -9.1153e-02],
        [ 2.7710e-02,  3.1374e-02, -1.1918e-01],
        [ 3.6731e-02, -3.3812e-02, -4.1160e-02],
        [ 1.2619e-02, -1.0742e-01,  1.5152e-01],
        [-6.2568e-03, -8.8534e-02,  7.1847e-02],
        [ 2.4099e-02, -4.5041e-02, -1.8289e-02],
        [ 2.1990e-02, -1.0274e-01,  1.3344e-01],
        [ 8.5975e-02, -6.3920e-02,  2.6106e-02],
        [ 4.4368e-02, -3.6473e-02, -3.1763e-02],
        [ 1.8988e-02, -9.0483e-02,  1.3586e-01],
        [ 1.4893e-02,  2.9840e-02, -1.1288e-01],
        [ 4.6689e-02, -4.1204e-02, -7.0780e-02],
        [ 1.5714e-02,  2.7424e-02, -1.2581e-01],
        [ 2.9770e-02,  4.0428e-02, -1.0302e-01],
        [ 1.1232e-02,  4.1883e-02, -1.1312e-01],
        [ 3.8262e-02, -9.9325e-02,  6.7237e-02],
        [ 2.5080e-02,  4.4094e-02, -1.1978e-01],
        [ 6.7912e-02, -1.4264e-02, -6.7233e-02],
        [ 2.7426e-02,  5.0779e-02, -1.2431e-01],
        [ 1.2592e-02,  2.4387e-03, -1.2045e-01],
        [ 4.7450e-02, -6.2284e-02,  1.4944e-02],
        [ 1.3954e-02,  9.1390e-03, -1.2173e-01],
        [-6.7872e-03, -9.3756e-02,  1.6461e-01],
        [ 4.9419e-03, -5.1080e-02,  4.3500e-02],
        [ 1.2215e-02,  5.0926e-02, -1.2036e-01],
        [ 1.1714e-02,  4.0289e-02, -1.1976e-01],
        [ 4.3028e-03,  3.6780e-02, -1.2590e-01],
        [ 4.3751e-02, -6.8739e-03, -6.8728e-02],
        [ 1.5245e-02,  4.1560e-02, -1.2087e-01],
        [ 3.5730e-02, -3.3760e-02, -6.4329e-02],
        [ 6.9574e-02, -7.7884e-03, -1.1817e-01],
        [ 3.5663e-02, -1.1193e-02, -6.1301e-02],
        [ 3.7120e-03, -1.0550e-01,  9.0106e-02],
        [-2.5334e-02, -8.5350e-02,  1.3272e-01],
        [ 2.4701e-02, -8.6334e-02,  7.2137e-02],
        [ 1.4940e-02,  2.4709e-02, -1.1581e-01],
        [ 1.1371e-02, -8.1653e-02,  3.5302e-02],
        [ 7.1223e-02, -5.8032e-02,  1.1244e-01],
        [ 4.0300e-02,  1.8227e-02, -1.2325e-01],
        [ 1.4192e-02,  4.5057e-02, -1.2118e-01],
        [ 1.8603e-02,  3.4668e-02, -1.1709e-01],
        [ 2.7443e-02, -1.0995e-01,  6.0605e-02],
        [ 2.8774e-02, -4.1144e-02,  8.5088e-02],
        [ 1.2010e-02,  3.5757e-02, -1.1457e-01],
        [ 3.0010e-02, -9.5891e-02,  1.1276e-01],
        [ 6.5208e-02, -3.2137e-02, -8.2012e-02],
        [-5.1144e-03,  2.4819e-02, -1.2386e-01],
        [ 1.1589e-02, -7.1145e-02,  6.1256e-02],
        [ 2.4397e-02,  4.2405e-02, -1.1385e-01],
        [ 7.3529e-02, -2.0767e-03, -9.1269e-02],
        [ 5.6789e-02, -1.0804e-01,  1.1983e-01],
        [ 1.4876e-02,  3.7816e-02, -1.2604e-01],
        [ 9.5069e-03, -9.4421e-02,  7.3661e-02],
        [ 6.0650e-02, -6.0172e-02, -2.1793e-02],
        [ 1.0035e-01, -1.1755e-01,  7.9921e-02],
        [ 1.9939e-02,  2.9612e-02, -1.2482e-01],
        [ 4.6452e-02, -5.9114e-02, -3.7167e-02],
        [-8.1618e-03, -9.5106e-02,  1.6359e-01],
        [ 6.8269e-02, -8.1206e-02,  1.8809e-02],
        [ 1.5494e-02,  3.5769e-02, -1.1995e-01],
        [ 2.6602e-02,  2.3286e-02, -1.3299e-01],
        [ 3.4095e-02,  3.4065e-02, -1.2467e-01],
        [ 6.1977e-02, -6.5480e-03, -6.1884e-02],
        [ 2.9494e-02,  9.4643e-03, -1.0686e-01],
        [ 5.0714e-02, -6.2513e-02, -8.5934e-03],
        [ 5.6186e-02,  9.5621e-03, -7.1086e-02],
        [ 3.7773e-02, -1.1479e-01,  1.2479e-01],
        [ 1.7125e-02,  3.6067e-02, -1.2183e-01],
        [ 3.6821e-02, -1.4993e-03, -8.2462e-02],
        [ 5.4761e-02, -1.0935e-01,  1.2314e-01],
        [-1.2227e-02, -8.2444e-02,  1.4675e-01],
        [ 1.6844e-02, -8.4943e-02,  1.8405e-01],
        [ 1.2870e-02,  3.6177e-02, -1.3175e-01],
        [ 3.1976e-02,  9.7357e-03, -1.4446e-01],
        [ 5.8193e-02, -1.1706e-01,  1.1268e-01],
        [ 8.7412e-03, -5.9027e-02,  1.7206e-02],
        [ 3.3044e-02, -1.4689e-01,  1.0702e-01],
        [ 3.2731e-02, -1.0831e-01,  1.2929e-01],
        [ 1.3104e-02,  3.5487e-02, -1.3164e-01],
        [-2.0876e-02, -9.6431e-02,  1.4152e-01],
        [ 8.0758e-02, -9.2472e-02,  1.8620e-02],
        [ 6.9449e-02, -7.7086e-02, -2.5588e-02],
        [ 5.3864e-02, -9.6440e-02,  1.4871e-01],
        [ 3.0514e-02,  1.7648e-03, -6.6670e-02],
        [-5.4591e-02, -8.3963e-02,  9.5287e-02],
        [ 4.1872e-02, -5.0384e-02, -7.7009e-02],
        [ 2.5935e-02,  4.4189e-02, -1.3086e-01],
        [ 5.5021e-02, -1.0554e-01,  1.2158e-01],
        [ 2.0631e-02,  4.3878e-02, -1.1708e-01],
        [ 2.5442e-02,  3.7587e-02, -1.1290e-01],
        [ 5.7154e-03, -8.2907e-02,  1.1227e-01],
        [ 1.4651e-02, -5.7561e-02,  1.0237e-01],
        [ 1.4588e-02,  3.6018e-02, -1.2665e-01],
        [ 1.2787e-02,  3.8513e-02, -1.3399e-01],
        [ 5.0690e-02, -3.1791e-02, -4.5676e-02],
        [ 2.4575e-02, -3.3023e-02, -4.8423e-02],
        [ 6.3119e-03,  4.1828e-02, -1.1290e-01],
        [ 2.7144e-02, -7.8558e-02,  1.6667e-01],
        [ 5.6283e-02,  2.5389e-03, -1.4766e-01],
        [ 1.7791e-02, -7.5409e-02,  1.5549e-01],
        [ 1.0357e-02,  4.2509e-02, -1.1864e-01],
        [ 6.8568e-03, -9.1298e-02,  1.2792e-01],
        [ 6.9442e-02, -4.9043e-02, -5.0156e-02],
        [ 4.2186e-02, -1.0540e-01,  9.0861e-02],
        [ 2.8533e-02,  2.8131e-02, -1.2478e-01],
        [ 1.8938e-02,  3.0577e-02, -1.3315e-01],
        [ 4.2992e-02, -3.5618e-02, -1.7699e-04],
        [ 6.5589e-02, -4.4385e-02, -3.4128e-03],
        [ 3.1514e-02, -3.8465e-02, -1.9997e-03],
        [ 4.1557e-02, -2.3677e-02, -4.1616e-02],
        [ 1.8167e-02,  4.3380e-02, -1.1708e-01]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000039
penultimate_layer.0.bias: grad mean = 0.001181
output_layer.0.weight: grad mean = 0.004155
output_layer.0.bias: grad mean = 0.033697
[correct] no_actions is False
task_labels:  tensor([2, 0, 0, 0, 1, 2, 1, 0, 2, 2, 0, 2, 0, 1, 0, 2, 0, 1, 1, 1, 2, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 1,
        0, 2, 0, 0, 2, 0, 2, 1, 0, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        2, 1, 2, 2, 2, 0, 1, 1, 1, 2, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 2, 2, 0, 2, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 2, 0, 2, 1, 0, 1, 1,
        1, 2, 2, 1, 1, 1, 2, 2]) , task_preds: tensor([[ 0.0356, -0.1026,  0.1599],
        [ 0.0923, -0.0795, -0.0461],
        [ 0.0226,  0.0374, -0.1114],
        [ 0.0401, -0.0950,  0.0427],
        [ 0.0159,  0.0359, -0.1132],
        [ 0.0175,  0.0383, -0.1172],
        [ 0.0351,  0.0110, -0.1157],
        [ 0.0006, -0.0589,  0.1119],
        [ 0.0480, -0.0729,  0.0740],
        [ 0.0502, -0.1062,  0.1128],
        [ 0.0517,  0.0072, -0.1292],
        [ 0.0121,  0.0444, -0.1183],
        [ 0.0111,  0.0311, -0.1166],
        [ 0.0378, -0.0577, -0.0336],
        [ 0.0282, -0.0293, -0.0608],
        [ 0.0199, -0.0937,  0.1655],
        [ 0.0680, -0.0162, -0.0888],
        [ 0.0318, -0.0730,  0.0549],
        [ 0.0093, -0.0639,  0.1466],
        [ 0.0084, -0.0690,  0.1292],
        [ 0.0464,  0.0082, -0.0784],
        [ 0.0115,  0.0399, -0.1147],
        [ 0.0105,  0.0349, -0.1380],
        [ 0.0279,  0.0478, -0.1349],
        [ 0.0587, -0.1429,  0.1085],
        [ 0.0636, -0.0805,  0.0187],
        [ 0.0191,  0.0354, -0.1319],
        [ 0.0115,  0.0399, -0.1147],
        [ 0.0413,  0.0108, -0.1314],
        [ 0.0646, -0.1272,  0.1293],
        [ 0.0467,  0.0014, -0.1152],
        [ 0.0336, -0.0664, -0.0251],
        [-0.0173, -0.1055,  0.1202],
        [ 0.0377, -0.0901,  0.0854],
        [ 0.0188,  0.0256, -0.1149],
        [ 0.0222,  0.0358, -0.1273],
        [ 0.0596, -0.0954,  0.0258],
        [ 0.0192,  0.0345, -0.1219],
        [ 0.0359, -0.0851,  0.0970],
        [ 0.0427, -0.0554, -0.0354],
        [ 0.0115,  0.0399, -0.1147],
        [ 0.0629, -0.0695, -0.0192],
        [ 0.0270,  0.0244, -0.1066],
        [ 0.0830, -0.0658, -0.0552],
        [ 0.0164, -0.0574,  0.1715],
        [ 0.0352, -0.0997,  0.1632],
        [ 0.0234, -0.0991,  0.1396],
        [ 0.0126,  0.0324, -0.1199],
        [ 0.0865, -0.0255, -0.0763],
        [ 0.0468, -0.0114, -0.0944],
        [ 0.1010, -0.0940,  0.0874],
        [ 0.0057,  0.0411, -0.1235],
        [ 0.0676, -0.1050,  0.0300],
        [ 0.0842, -0.0787, -0.0762],
        [ 0.0179,  0.0353, -0.1385],
        [ 0.0083,  0.0366, -0.1210],
        [ 0.0245,  0.0381, -0.1176],
        [ 0.0178,  0.0281, -0.1197],
        [ 0.0738, -0.0895, -0.0006],
        [ 0.0126,  0.0379, -0.1179],
        [ 0.0801, -0.0635, -0.0551],
        [ 0.0364, -0.0347, -0.0745],
        [ 0.0387, -0.0438, -0.0247],
        [ 0.0314, -0.0489,  0.0016],
        [ 0.0368, -0.0083, -0.0346],
        [ 0.0371, -0.0007, -0.0838],
        [ 0.0242,  0.0318, -0.1233],
        [ 0.0229,  0.0405, -0.1224],
        [ 0.0234,  0.0291, -0.1327],
        [ 0.0602, -0.0598, -0.0444],
        [ 0.0101, -0.0472,  0.1027],
        [ 0.0334,  0.0060, -0.1093],
        [ 0.0255, -0.0849,  0.1073],
        [ 0.0075,  0.0347, -0.1223],
        [ 0.0084, -0.1148,  0.1683],
        [ 0.0123, -0.0803,  0.1345],
        [ 0.0215,  0.0404, -0.1290],
        [ 0.0432, -0.1037,  0.1567],
        [ 0.0394, -0.0050, -0.1058],
        [ 0.0377, -0.0561, -0.0248],
        [ 0.0048, -0.0770,  0.1030],
        [ 0.0287, -0.0425, -0.0065],
        [ 0.0230,  0.0436, -0.1256],
        [ 0.0727, -0.0890,  0.0120],
        [ 0.0187,  0.0344, -0.1251],
        [ 0.0224,  0.0018, -0.0851],
        [ 0.0431, -0.1004,  0.0957],
        [ 0.0700, -0.0132, -0.1039],
        [ 0.0580, -0.1015,  0.1202],
        [ 0.0111, -0.0572,  0.1434],
        [ 0.0834, -0.0730, -0.0906],
        [ 0.0344, -0.0149, -0.0623],
        [ 0.0042,  0.0376, -0.1159],
        [ 0.0408, -0.0365, -0.0498],
        [ 0.0285,  0.0195, -0.1344],
        [ 0.0120, -0.1102,  0.0923],
        [ 0.0161,  0.0410, -0.1214],
        [ 0.0685, -0.0411, -0.0019],
        [ 0.0147,  0.0279, -0.1214],
        [ 0.0599, -0.0779,  0.0194],
        [ 0.0491, -0.0431, -0.0658],
        [ 0.0218,  0.0240, -0.1186],
        [-0.0194, -0.0655,  0.1813],
        [ 0.0278,  0.0063, -0.1084],
        [ 0.0271,  0.0442, -0.1196],
        [ 0.0128,  0.0196, -0.1306],
        [ 0.0031, -0.1206,  0.1491],
        [ 0.0124,  0.0359, -0.1235],
        [ 0.0392, -0.0587, -0.0819],
        [ 0.0039, -0.0394,  0.0447],
        [-0.0041, -0.0802,  0.1098],
        [ 0.0127,  0.0434, -0.1013],
        [ 0.0729, -0.0374, -0.0449],
        [-0.0291, -0.0888,  0.1413],
        [ 0.0200,  0.0135, -0.1245],
        [-0.0075, -0.0669,  0.1616],
        [ 0.0135,  0.0398, -0.1158],
        [ 0.0697, -0.0442, -0.0629],
        [ 0.0171, -0.0827,  0.0266],
        [ 0.0466, -0.1035,  0.1318],
        [ 0.0665, -0.0195, -0.0815],
        [ 0.0272, -0.0631,  0.1427],
        [ 0.0530, -0.0399, -0.0547],
        [ 0.0299, -0.0641,  0.0910],
        [ 0.0253,  0.0460, -0.1242],
        [ 0.0228, -0.0452, -0.0580],
        [ 0.0390, -0.0926,  0.1286],
        [ 0.0271,  0.0100, -0.1068]], grad_fn=<AddmmBackward0>)
class_weights:  [1.0833334 0.8666667 1.0833334]
penultimate_layer.0.weight: grad mean = 0.000029
penultimate_layer.0.bias: grad mean = 0.000843
output_layer.0.weight: grad mean = 0.003114
output_layer.0.bias: grad mean = 0.025429
[correct] no_actions is False
class_weights:  [1.0833334 0.8666667 1.0833334]
task_preds:  tensor([[ 0.0627, -0.0546, -0.0466],
        [ 0.0245,  0.0031, -0.1283],
        [ 0.0202,  0.0339, -0.1315],
        [ 0.0373, -0.0051, -0.0933],
        [ 0.0281, -0.0903,  0.1201],
        [ 0.0525, -0.0063, -0.0489],
        [ 0.0137,  0.0438, -0.1167],
        [ 0.0372, -0.1200,  0.1219],
        [ 0.0628, -0.0002, -0.0734],
        [ 0.0099, -0.0631,  0.1611],
        [ 0.0573, -0.0264, -0.0279],
        [ 0.0372, -0.0233, -0.0486],
        [ 0.0633, -0.0621, -0.0080],
        [ 0.0715, -0.0966,  0.0220],
        [ 0.0485, -0.0350,  0.0231],
        [ 0.0820, -0.0209, -0.1234],
        [ 0.0038,  0.0370, -0.1198],
        [ 0.0584, -0.0879,  0.0689],
        [ 0.0126, -0.0605,  0.1584],
        [ 0.0044, -0.0877,  0.1226],
        [ 0.0142, -0.0419, -0.0559],
        [ 0.0543, -0.0721, -0.0238],
        [ 0.0192,  0.0238, -0.1050],
        [ 0.0182, -0.0737,  0.1421],
        [ 0.0147,  0.0342, -0.1224],
        [ 0.0176, -0.1151,  0.1386],
        [ 0.0223, -0.0850,  0.0326],
        [ 0.0139,  0.0402, -0.1245],
        [ 0.0383, -0.0148, -0.0374],
        [ 0.0471,  0.0036, -0.1287],
        [ 0.0360, -0.0518,  0.0112],
        [ 0.0357, -0.0734,  0.0844],
        [ 0.0345, -0.0247, -0.0880],
        [ 0.0213,  0.0035, -0.1075],
        [ 0.0673, -0.0272, -0.0551],
        [ 0.0145,  0.0393, -0.1149],
        [ 0.0473,  0.0084, -0.1099],
        [ 0.0476, -0.0087, -0.0922],
        [ 0.0370, -0.1004,  0.1081],
        [ 0.0200, -0.0993,  0.0928],
        [ 0.0631,  0.0265, -0.1062],
        [ 0.0575, -0.0138, -0.0732],
        [ 0.0040, -0.0775,  0.0645],
        [ 0.0070, -0.0845,  0.1420],
        [ 0.0397,  0.0232, -0.1069],
        [ 0.0326, -0.1170,  0.1205],
        [ 0.0438, -0.1063,  0.1619],
        [ 0.0353, -0.0136, -0.0662],
        [ 0.0427, -0.0252, -0.0437],
        [ 0.0439, -0.0767,  0.1292],
        [ 0.0542, -0.0021, -0.0437],
        [ 0.0341, -0.0877,  0.1118],
        [ 0.0096,  0.0397, -0.1126],
        [ 0.0184,  0.0333, -0.1185],
        [ 0.0414, -0.0034, -0.1222],
        [ 0.0202,  0.0356, -0.1193],
        [ 0.0476, -0.0708,  0.0126],
        [ 0.0271,  0.0293, -0.1171],
        [ 0.0216,  0.0283, -0.1188],
        [ 0.0259, -0.1089,  0.1261],
        [ 0.0527,  0.0029, -0.1198],
        [ 0.0158,  0.0378, -0.1273],
        [ 0.0227,  0.0340, -0.1269],
        [ 0.0538, -0.0120, -0.1124],
        [ 0.0734, -0.0922, -0.0629],
        [ 0.0583, -0.1068, -0.0119],
        [ 0.0278,  0.0141, -0.1145],
        [ 0.0372,  0.0044, -0.0985],
        [ 0.0110, -0.1096,  0.1072],
        [ 0.0135, -0.0808,  0.0076],
        [-0.0069, -0.0690,  0.1125],
        [ 0.0527, -0.0484, -0.0184],
        [ 0.0249,  0.0368, -0.1011],
        [ 0.0509, -0.0101, -0.0496],
        [ 0.0458, -0.0409, -0.0453],
        [ 0.0157, -0.1002,  0.1104],
        [ 0.0166,  0.0153, -0.1379],
        [ 0.0210,  0.0169, -0.1218],
        [ 0.0228,  0.0553, -0.1152],
        [ 0.0166,  0.0386, -0.1275],
        [ 0.0695, -0.0491, -0.0372],
        [ 0.0357, -0.0886,  0.0311],
        [ 0.0903, -0.0855, -0.0652],
        [ 0.0450, -0.0015, -0.1154],
        [ 0.0224, -0.0177, -0.0570],
        [ 0.0021, -0.0609,  0.1124],
        [ 0.0305, -0.0835,  0.1620],
        [ 0.0157,  0.0119, -0.1140],
        [-0.0015, -0.1026,  0.1651],
        [ 0.0524, -0.0122, -0.1077],
        [ 0.0122,  0.0381, -0.1274],
        [ 0.0521, -0.0203, -0.0767],
        [ 0.0130,  0.0405, -0.1178],
        [ 0.0289, -0.0654,  0.0966],
        [ 0.0339,  0.0046, -0.0901],
        [ 0.0766, -0.0334, -0.0576],
        [ 0.0285,  0.0347, -0.1225],
        [ 0.0139,  0.0391, -0.1190],
        [ 0.0206, -0.1006,  0.1019],
        [ 0.0614, -0.0840,  0.0649],
        [ 0.0136,  0.0372, -0.1244],
        [ 0.0322,  0.0210, -0.1335],
        [ 0.0367,  0.0118, -0.1018],
        [ 0.0387, -0.1006,  0.1630],
        [ 0.0055,  0.0525, -0.1015],
        [ 0.0160,  0.0406, -0.1102],
        [ 0.0583, -0.0746,  0.0419],
        [ 0.0114,  0.0399, -0.1147],
        [ 0.0330,  0.0403, -0.1162],
        [ 0.0203,  0.0292, -0.1290],
        [ 0.0593, -0.0681, -0.0379],
        [ 0.0549, -0.0480, -0.0613],
        [ 0.0268,  0.0324, -0.1312],
        [ 0.0221,  0.0180, -0.1164],
        [ 0.0687, -0.0577, -0.0352],
        [ 0.0249,  0.0381, -0.1254],
        [ 0.0244, -0.0924,  0.1516],
        [ 0.0249, -0.1030,  0.1335],
        [ 0.0042,  0.0467, -0.1236],
        [ 0.0652, -0.0953,  0.0575],
        [ 0.0392,  0.0113, -0.0800],
        [ 0.0114,  0.0399, -0.1147],
        [ 0.0186,  0.0446, -0.1096],
        [ 0.0140,  0.0271, -0.1110],
        [ 0.0175,  0.0271, -0.1238],
        [ 0.0070,  0.0326, -0.1203],
        [ 0.0099,  0.0359, -0.1245],
        [ 0.0338, -0.0583, -0.0285]])
task_pred: tensor([0, 0, 1, 0, 2, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 1, 2,
        1, 2, 2, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 2, 2, 0, 2, 2, 0,
        0, 2, 0, 2, 1, 1, 0, 1, 0, 1, 1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 2, 0,
        1, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 2, 0, 0,
        1, 1, 2, 2, 1, 0, 0, 2, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 2, 2, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0]), task_id: tensor([0, 1, 1, 2, 1, 0, 1, 2, 0, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 2, 2, 0, 0, 2,
        1, 0, 0, 1, 2, 1, 1, 0, 2, 0, 0, 1, 2, 2, 0, 2, 2, 2, 0, 0, 1, 2, 2, 0,
        2, 1, 1, 0, 2, 2, 1, 0, 2, 1, 1, 1, 2, 2, 0, 0, 0, 1, 0, 1, 2, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 2, 1, 0, 0, 0, 0, 2, 2, 2, 0, 1, 2, 2, 1, 1, 0, 2, 0,
        0, 1, 0, 2, 1, 2, 0, 1, 1, 1, 2, 1, 1, 0, 0, 0, 2, 2, 2, 2, 0, 0, 1, 2,
        0, 1, 2, 0, 2, 1, 1, 1])
[correct] no_actions is False
class_weights:  [1.0833334 0.8666667 1.0833334]
task_preds:  tensor([[ 0.0288, -0.0167, -0.0644],
        [ 0.0164, -0.1003,  0.1052]])
MLP Fine-Tuning: task_loss = 1.0833: 100%|██████████| 15/15 [01:06<00:00,  4.43s/it]
 67%|██████▋   | 2/3 [00:01<00:00,  1.76it/s]
[correct] no_actions is False
class_weights:  [1.0833334 0.8666667 1.0833334]
task_preds:  tensor([[ 5.9332e-02, -6.8085e-02, -3.7925e-02],
        [ 1.9458e-02,  3.1348e-02, -1.2269e-01],
        [ 3.2733e-02, -8.1670e-02,  5.2870e-02],
        [ 5.2786e-03, -1.1878e-01,  1.4484e-01],
        [ 1.4525e-02,  3.9326e-02, -1.1493e-01],
        [ 1.3859e-02,  3.9118e-02, -1.1895e-01],
        [ 1.1278e-02,  3.6012e-02, -1.2985e-01],
        [ 2.2826e-02,  4.0042e-02, -1.2218e-01],
        [ 3.8605e-02, -2.9568e-04, -1.0239e-01],
        [ 2.9986e-02,  9.8018e-03, -1.2102e-01],
        [ 1.2235e-02,  4.1983e-02, -1.2409e-01],
        [ 1.8416e-02,  3.3333e-02, -1.1851e-01],
        [ 4.8778e-02, -7.6058e-03, -6.1053e-02],
        [ 5.2742e-03,  5.5291e-02, -1.1456e-01],
        [ 2.2948e-02,  2.9844e-02, -1.2360e-01],
        [ 2.2517e-02,  4.2585e-02, -1.2147e-01],
        [ 4.4841e-03, -9.2309e-02,  1.1894e-01],
        [ 1.2174e-02,  3.8145e-02, -1.2743e-01],
        [ 6.4092e-02, -5.9401e-02, -8.3177e-03],
        [ 7.2559e-02, -7.7313e-02,  5.6035e-02],
        [ 1.3897e-02,  4.0205e-02, -1.2449e-01],
        [ 6.3086e-02,  2.6483e-02, -1.0624e-01],
        [ 2.2691e-02,  3.4034e-02, -1.2685e-01],
        [ 9.6426e-02, -1.1300e-01, -3.1702e-02],
        [ 1.8241e-02, -7.3656e-02,  1.4209e-01],
        [ 4.9370e-02,  1.5778e-02, -9.8019e-02],
        [ 5.4912e-02, -4.8010e-02, -6.1273e-02],
        [ 3.4505e-02, -2.4679e-02, -8.7952e-02],
        [ 4.4998e-02, -1.7654e-02, -6.0172e-02],
        [ 4.0530e-02, -1.1082e-01,  1.4409e-01],
        [ 1.6677e-02, -7.6713e-02,  1.8649e-01],
        [ 5.1344e-02, -2.6226e-02, -7.7851e-02],
        [ 4.0515e-02, -5.0188e-02, -7.2046e-02],
        [ 2.3140e-02,  3.0077e-02, -1.2408e-01],
        [ 1.2195e-02,  1.7681e-02, -1.0757e-01],
        [ 2.8848e-02,  1.2977e-05, -1.2313e-01],
        [ 2.4929e-02,  3.6773e-02, -1.0105e-01],
        [ 5.8184e-02, -5.3173e-02,  1.9855e-02],
        [ 7.3359e-02, -9.2217e-02, -6.2922e-02],
        [ 3.5271e-02,  2.3771e-03, -1.0859e-01],
        [ 2.4892e-02, -1.0297e-01,  1.3348e-01],
        [ 1.3675e-02,  3.7079e-02, -1.2640e-01],
        [ 1.2332e-02,  4.0292e-02, -1.1230e-01],
        [ 6.5497e-02, -3.1870e-02, -5.1354e-02],
        [ 1.2996e-02,  4.0501e-02, -1.1780e-01],
        [ 1.7939e-02, -7.9317e-02,  1.5772e-01],
        [ 3.7120e-02, -5.4431e-02,  3.0868e-02],
        [-2.8248e-03, -6.1033e-02,  6.5047e-02],
        [ 1.2664e-02, -9.4052e-02,  1.6657e-01],
        [ 1.4104e-02, -9.9602e-02,  3.4433e-02],
        [ 6.8729e-02, -5.7742e-02, -3.5187e-02],
        [ 5.0937e-02, -1.0110e-02, -4.9572e-02],
        [ 1.1439e-02,  3.9946e-02, -1.1475e-01],
        [ 4.9519e-02, -5.2820e-02, -3.0699e-02],
        [ 2.5863e-02, -1.0893e-01,  1.2610e-01],
        [ 4.2655e-02, -2.1562e-02, -4.1235e-02],
        [ 9.6624e-03, -7.4576e-02,  1.4392e-01],
        [ 6.4223e-02, -8.5436e-02,  1.0257e-01],
        [ 1.0811e-02,  3.8781e-02, -1.1193e-01],
        [ 1.9417e-02,  3.3410e-02, -1.3144e-01],
        [ 3.7500e-02, -2.9833e-02, -4.3923e-02],
        [ 3.0497e-02, -8.3538e-02,  1.6198e-01],
        [ 7.6613e-02, -3.3443e-02, -5.7632e-02],
        [-2.2115e-03, -7.5997e-02,  1.1165e-01],
        [ 1.6054e-02, -7.2887e-02,  1.5895e-01],
        [ 1.0156e-02,  4.8258e-02, -1.2831e-01],
        [ 1.8096e-03,  3.0287e-02, -1.1246e-01],
        [ 5.4199e-02, -4.8578e-02,  9.6974e-03],
        [ 4.8673e-02, -2.0786e-02, -6.5445e-02],
        [ 2.1383e-02, -6.6822e-02,  4.4030e-02],
        [ 4.5877e-02, -4.4370e-02,  1.4963e-02],
        [ 4.1376e-02, -1.1834e-01,  1.0795e-01],
        [ 1.2218e-02, -6.8943e-02,  1.3092e-01],
        [ 1.4269e-02,  2.9409e-02, -1.2320e-01],
        [ 2.8103e-02, -9.0691e-02,  8.7488e-02],
        [ 2.8272e-02,  1.8547e-02, -1.2060e-01],
        [ 1.8969e-02, -5.1829e-02,  1.2227e-01],
        [ 7.6155e-02, -1.2281e-02, -1.1704e-01],
        [ 1.8952e-03, -7.4632e-02, -2.1403e-02],
        [ 9.5771e-03, -9.9542e-02,  8.7481e-02],
        [ 1.4639e-02, -7.9287e-02,  5.4129e-02],
        [-2.8736e-03, -8.6417e-02,  1.5689e-01],
        [ 5.7457e-02, -1.3831e-02, -7.3229e-02],
        [ 4.7052e-02,  3.6302e-03, -1.2868e-01],
        [ 1.6355e-02,  3.3101e-02, -1.3170e-01],
        [ 3.7477e-02, -9.6343e-02,  1.2134e-01],
        [ 3.3896e-02,  3.6814e-02, -1.1835e-01],
        [ 1.0834e-02,  3.9600e-02, -1.1829e-01],
        [ 4.9361e-02, -1.1166e-01,  4.4971e-02],
        [ 1.2569e-02, -6.0475e-02,  1.5838e-01],
        [ 2.4861e-02,  3.8121e-02, -1.2538e-01],
        [-8.8304e-03, -1.2060e-01,  1.4953e-01],
        [ 1.1439e-02,  3.9946e-02, -1.1475e-01],
        [ 2.1551e-02,  2.8284e-02, -1.1881e-01],
        [ 4.7323e-02,  8.3696e-03, -1.0988e-01],
        [ 5.2998e-02, -5.6108e-02, -6.2064e-02],
        [ 3.6569e-02, -9.3249e-02,  9.8853e-02],
        [ 1.4143e-02,  2.8231e-02, -1.2962e-01],
        [ 2.3791e-02,  2.7953e-02, -1.2089e-01],
        [ 3.0003e-02,  3.8036e-02, -1.2043e-01],
        [-1.6532e-03, -8.1842e-02,  9.5637e-02],
        [ 1.2830e-02, -3.6200e-03, -9.4217e-02],
        [ 5.3909e-02, -2.3846e-02, -5.6076e-02],
        [ 3.5496e-02,  1.9797e-02, -1.2831e-01],
        [ 5.7943e-02, -1.1780e-01,  1.5326e-01],
        [ 2.5159e-02,  3.2049e-02, -1.1565e-01],
        [ 3.9657e-02,  2.3203e-02, -1.0686e-01],
        [ 3.3074e-02, -9.3737e-02,  1.1505e-01],
        [ 1.7750e-02, -8.1575e-02,  1.1119e-01],
        [ 4.3135e-02,  2.1081e-03, -9.3995e-02],
        [ 1.8276e-02,  4.1203e-02, -1.2483e-01],
        [ 1.2019e-02,  1.6949e-02, -1.0688e-01],
        [ 6.1182e-02, -6.0064e-02, -2.7525e-02],
        [ 4.0177e-02,  9.6146e-03, -1.2835e-01],
        [ 2.0301e-02,  2.9248e-02, -1.2898e-01],
        [ 1.1439e-02,  3.9946e-02, -1.1475e-01],
        [ 2.4650e-02,  2.8618e-02, -1.3581e-01],
        [ 4.8516e-02, -3.4989e-02,  2.3095e-02],
        [ 4.9468e-02, -2.1039e-02, -3.1232e-02],
        [ 3.9823e-02, -1.2570e-01,  1.0447e-01],
        [ 2.2370e-02,  6.1640e-03, -7.9875e-02],
        [ 1.0997e-02, -3.6958e-04, -9.0576e-02],
        [ 1.2773e-02,  4.5265e-02, -1.1516e-01],
        [ 1.7975e-02,  3.3675e-02, -1.1300e-01],
        [-3.4932e-04,  2.9627e-02, -1.2883e-01],
        [ 4.5818e-02, -9.5248e-03, -5.2624e-02],
        [ 2.7764e-02,  1.4066e-02, -1.1455e-01],
        [ 3.4214e-02, -8.5339e-02,  9.1325e-02]])
task_pred: tensor([0, 1, 2, 2, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 2, 1, 0, 0, 1, 0, 1, 0,
        2, 0, 0, 0, 0, 2, 2, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 0, 1, 2, 0, 2,
        2, 2, 0, 0, 1, 0, 2, 0, 2, 2, 1, 1, 0, 2, 0, 2, 2, 1, 1, 0, 0, 2, 0, 2,
        2, 1, 2, 0, 2, 0, 0, 2, 2, 2, 0, 0, 1, 2, 1, 1, 0, 2, 1, 2, 1, 1, 0, 0,
        2, 1, 1, 1, 2, 0, 0, 0, 2, 1, 0, 2, 2, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 2,
        0, 0, 1, 1, 1, 0, 0, 2]), task_id: tensor([0, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 0, 2, 1, 0, 1, 2, 0, 0,
        2, 2, 0, 2, 1, 2, 0, 2, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 2, 1, 2,
        1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 0, 2, 2, 0, 0, 0, 2, 0, 0, 1, 1, 2, 2,
        1, 0, 0, 1, 2, 2, 2, 0, 2, 2, 2, 1, 0, 2, 0, 2, 2, 0, 2, 1, 1, 1, 2, 1,
        0, 1, 2, 1, 0, 1, 1, 2, 1, 2, 1, 0, 2, 0, 2, 1, 2, 1, 0, 1, 0, 1, 0, 0,
        2, 1, 1, 0, 0, 2, 0, 0])
[correct] no_actions is False
class_weights:  [1.0833334 0.8666667 1.0833334]
task_preds:  tensor([[ 0.0152,  0.0402, -0.1396],
        [ 0.0261, -0.1176,  0.1257]])
task_pred: tensor([1, 2]), task_id: tensor([2, 2])
[correct] no_actions is False
class_weights:  [1.0833334 0.8666667 1.0833334]
task_preds:  tensor([[ 2.4929e-02,  3.6773e-02, -1.0105e-01],
        [ 2.0063e-02,  1.1649e-02, -9.2545e-02],
        [ 2.4861e-02,  3.8121e-02, -1.2538e-01],
        [ 4.5364e-02, -8.0555e-03, -5.6589e-02],
        [ 2.2415e-02, -7.6484e-02,  1.5516e-01],
        [ 3.7890e-02, -3.2251e-02, -3.4238e-02],
        [ 1.3897e-02,  4.0205e-02, -1.2449e-01],
        [ 4.9519e-02, -5.2820e-02, -3.0699e-02],
        [ 2.5763e-02,  3.2744e-02, -1.1307e-01],
        [ 1.2315e-02,  1.0896e-02, -1.1178e-01],
        [ 1.1439e-02,  3.9946e-02, -1.1475e-01],
        [-1.6532e-03, -8.1842e-02,  9.5637e-02],
        [ 1.7591e-02, -1.1509e-01,  1.3856e-01],
        [ 3.2036e-02,  2.0007e-02, -1.2387e-01],
        [ 1.4639e-02, -7.9287e-02,  5.4129e-02],
        [ 2.4406e-02, -9.2365e-02,  1.5159e-01],
        [ 4.0530e-02, -1.1082e-01,  1.4409e-01],
        [ 2.0647e-02, -1.0056e-01,  1.0187e-01],
        [ 2.1551e-02,  2.8284e-02, -1.1881e-01],
        [ 4.6528e-02, -2.0991e-02, -9.7895e-02],
        [ 3.6008e-02, -5.1789e-02,  1.1161e-02],
        [ 3.4505e-02, -2.4679e-02, -8.7952e-02],
        [ 5.8449e-02, -8.7884e-02,  6.8934e-02],
        [ 5.1218e-02, -9.0094e-02, -4.9683e-02],
        [ 5.0179e-02, -5.6530e-02, -4.9243e-02],
        [ 5.7347e-02, -2.6377e-02, -2.7932e-02],
        [ 1.2383e-02,  3.8614e-02, -1.2443e-01],
        [ 3.0497e-02, -8.3538e-02,  1.6198e-01],
        [ 9.8756e-03,  3.5935e-02, -1.2454e-01],
        [ 1.5293e-02, -9.6529e-02,  1.7815e-01],
        [-8.8304e-03, -1.2060e-01,  1.4953e-01],
        [ 2.2724e-02,  3.7447e-02, -1.2805e-01],
        [ 5.7457e-02, -1.3831e-02, -7.3229e-02],
        [ 1.3675e-02,  3.7079e-02, -1.2640e-01],
        [ 3.1204e-02, -8.9339e-04, -1.0671e-01],
        [ 1.8416e-02,  3.3333e-02, -1.1851e-01],
        [ 5.1344e-02, -2.6226e-02, -7.7851e-02],
        [ 3.0396e-02, -8.0896e-02,  8.6524e-02],
        [ 4.5818e-02, -9.5248e-03, -5.2624e-02],
        [ 6.9463e-02, -4.8120e-02, -3.4106e-02],
        [ 3.9657e-02,  2.3203e-02, -1.0686e-01],
        [ 5.0988e-02,  2.4477e-03, -8.1905e-02],
        [ 3.8687e-02, -7.7637e-02,  1.3720e-01],
        [ 3.6964e-02, -1.0043e-01,  1.0811e-01],
        [ 7.6904e-02, -8.9439e-03, -1.0108e-01],
        [ 4.0369e-02, -4.6854e-02, -3.2242e-02],
        [ 3.1783e-02, -5.0111e-02, -9.7872e-03],
        [ 1.4925e-02,  2.9870e-02, -1.2536e-01],
        [ 1.5221e-02,  4.0205e-02, -1.3962e-01],
        [ 9.6341e-03,  1.9158e-02, -1.1117e-01],
        [ 5.7943e-02, -1.1780e-01,  1.5326e-01],
        [ 3.8683e-02, -1.0057e-01,  1.6296e-01],
        [ 4.7323e-02,  8.3696e-03, -1.0988e-01],
        [ 2.5455e-02,  3.6799e-02, -1.1836e-01],
        [ 3.0468e-02,  3.1763e-03, -1.1223e-01],
        [ 1.2986e-02,  3.4438e-02, -1.2717e-01],
        [ 2.7406e-02, -9.4300e-02,  1.0160e-01],
        [ 9.8715e-03, -7.2857e-02,  1.4004e-01],
        [ 2.2780e-02,  3.0038e-02, -1.1939e-01],
        [ 1.1602e-02,  4.1938e-02, -1.1646e-01],
        [ 1.1439e-02,  3.9946e-02, -1.1475e-01],
        [ 2.4518e-02,  3.0916e-03, -1.2828e-01],
        [ 1.1439e-02,  3.9946e-02, -1.1475e-01],
        [-1.4655e-03, -1.0258e-01,  1.6508e-01],
        [ 1.9988e-02, -9.9299e-02,  9.2781e-02],
        [ 1.4562e-02,  3.5288e-02, -1.1347e-01],
        [ 2.3153e-02,  3.0567e-02, -1.0762e-01],
        [ 3.0714e-02,  4.0289e-03, -1.1982e-01],
        [ 6.3086e-02,  2.6483e-02, -1.0624e-01],
        [ 5.6971e-02, -1.0797e-01,  1.3023e-01],
        [ 1.3673e-02,  4.3847e-02, -1.1669e-01],
        [ 1.0265e-02,  4.1243e-02, -1.2107e-01],
        [ 7.2559e-02, -7.7313e-02,  5.6035e-02],
        [ 6.4288e-02,  7.4550e-03, -6.6057e-02],
        [ 4.0882e-02, -3.5042e-02, -5.1774e-02],
        [ 3.3808e-02, -5.8251e-02, -2.8481e-02],
        [ 1.7048e-02, -9.0661e-02,  1.4043e-01],
        [ 1.1439e-02,  3.9946e-02, -1.1475e-01],
        [ 3.5496e-02,  1.9797e-02, -1.2831e-01],
        [ 6.5151e-02, -9.5270e-02,  5.7466e-02],
        [ 5.0592e-02, -1.6403e-02, -5.0249e-02],
        [ 2.3361e-02, -1.7190e-02, -5.0456e-02],
        [ 2.2420e-02, -1.7682e-02, -5.7015e-02],
        [ 1.6669e-02, -6.8039e-02,  1.1348e-01],
        [ 6.2074e-02, -7.1035e-02, -7.9670e-03],
        [ 6.3893e-03,  3.5014e-02, -1.3722e-01],
        [ 1.2996e-02,  4.0501e-02, -1.1780e-01],
        [ 1.6054e-02, -7.2887e-02,  1.5895e-01],
        [ 1.5683e-02, -1.0023e-01,  1.1044e-01],
        [ 1.2683e-02,  4.1526e-02, -1.2677e-01],
        [ 2.8721e-02,  3.4112e-02, -1.2731e-01],
        [ 5.9332e-02, -6.8085e-02, -3.7925e-02],
        [ 1.8241e-02, -7.3656e-02,  1.4209e-01],
        [ 5.1994e-02, -7.8643e-03, -1.0535e-01],
        [ 6.2800e-02, -1.7729e-04, -7.3359e-02],
        [ 1.4012e-02,  1.9162e-02, -1.0657e-01],
        [ 3.9647e-03,  4.0588e-02, -1.2002e-01],
        [ 4.4952e-02, -1.5073e-03, -1.1539e-01],
        [ 1.2664e-02, -9.4052e-02,  1.6657e-01],
        [ 1.0994e-02, -1.0958e-01,  1.0717e-01],
        [ 9.0233e-03,  3.9315e-02, -1.2929e-01],
        [ 1.1439e-02,  3.9946e-02, -1.1475e-01],
        [ 4.0939e-02, -1.0064e-01,  8.8119e-02],
        [ 3.7120e-02, -5.4431e-02,  3.0868e-02],
        [ 1.4104e-02, -9.9602e-02,  3.4433e-02],
        [ 1.1439e-02,  3.9946e-02, -1.1475e-01],
        [ 5.5117e-03,  5.2488e-02, -1.0155e-01],
        [ 6.5727e-02, -7.8387e-02,  8.4877e-02],
        [ 5.8184e-02, -5.3173e-02,  1.9855e-02],
        [ 1.2137e-02,  4.4720e-02, -1.1131e-01],
        [ 1.5590e-02,  2.9189e-02, -1.2519e-01],
        [ 3.4214e-02, -8.5339e-02,  9.1325e-02],
        [ 5.5379e-02, -2.2934e-02, -5.0288e-02],
        [ 9.4032e-03,  4.8293e-03, -9.5467e-02],
        [ 6.7560e-02, -4.5990e-02, -4.0332e-02],
        [ 4.9361e-02, -1.1166e-01,  4.4971e-02],
        [ 4.5830e-02, -4.0919e-02, -4.5295e-02],
        [ 5.2760e-02, -1.1768e-01,  9.9194e-02],
        [ 9.5771e-03, -9.9542e-02,  8.7481e-02],
        [ 4.2811e-02, -8.4357e-02,  6.2942e-02],
        [ 4.0515e-02, -5.0188e-02, -7.2046e-02],
        [ 2.6021e-02,  4.4957e-02, -1.2465e-01],
        [ 3.6750e-02,  1.1802e-02, -1.0181e-01],
        [ 7.3359e-02, -9.2217e-02, -6.2922e-02],
        [ 4.6363e-03, -7.6877e-02,  8.4946e-02],
        [ 1.9458e-02,  3.1348e-02, -1.2269e-01],
        [ 1.2648e-02, -9.3599e-02,  1.6163e-01],
        [ 1.6420e-02,  2.2117e-02, -1.2413e-01]])
task_pred: tensor([1, 0, 1, 0, 2, 0, 1, 0, 1, 0, 1, 2, 2, 0, 2, 2, 2, 2, 1, 0, 0, 0, 2, 0,
        0, 0, 1, 2, 1, 2, 2, 1, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1,
        1, 1, 2, 2, 0, 1, 0, 1, 2, 2, 1, 1, 1, 0, 1, 2, 2, 1, 1, 0, 0, 2, 1, 1,
        0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 1, 1, 2, 2, 1, 1, 0, 2, 0, 0, 1,
        1, 0, 2, 2, 1, 1, 2, 0, 2, 1, 1, 2, 0, 1, 1, 2, 0, 0, 0, 0, 0, 2, 2, 2,
        0, 1, 0, 0, 2, 1, 2, 1]), task_id: tensor([1, 0, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 2, 1, 1,
        1, 2, 0, 2, 1, 0, 1, 0, 2, 1, 0, 2, 2, 0, 2, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1,
        0, 0, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 0, 0, 0, 2, 0, 2, 2, 0, 2,
        1, 0, 1, 2, 0, 2, 1, 1, 1, 2, 1, 0, 1, 0, 0, 0, 0, 0, 1, 2, 1, 2, 0, 0,
        1, 0, 0, 0, 2, 1, 2, 0])
[correct] no_actions is False
class_weights:  [1.0833334 0.8666667 1.0833334]
task_preds:  tensor([[-0.0014, -0.0895,  0.1176],
        [ 0.0398, -0.1257,  0.1045]])
task_pred: tensor([2, 2]), task_id: tensor([0, 0])
==== [Test Summary] ====
rtg loss: 0.0001
Task loss: 1.1097
Task accuracy: 0.3846
- Task 0: 0.4167 (50/120)
- Task 1: 0.4267 (64/150)

100%|██████████| 3/3 [00:01<00:00,  1.76it/s]
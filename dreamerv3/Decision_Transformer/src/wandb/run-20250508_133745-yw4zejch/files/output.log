Class Weights: [1.08333333 0.86666667 1.08333333]
✅ Will update: action_embedding.0.weight
✅ Will update: time_embedding.weight
✅ Will update: state_embedding.weight
✅ Will update: transformer.pos_embed.W_pos
✅ Will update: transformer.blocks.0.attn.W_Q
✅ Will update: transformer.blocks.0.attn.W_K
✅ Will update: transformer.blocks.0.attn.W_V
✅ Will update: transformer.blocks.0.attn.W_O
✅ Will update: transformer.blocks.0.attn.b_Q
✅ Will update: transformer.blocks.0.attn.b_K
✅ Will update: transformer.blocks.0.attn.b_V
✅ Will update: transformer.blocks.0.attn.b_O
✅ Will update: transformer.blocks.0.mlp.W_in
✅ Will update: transformer.blocks.0.mlp.b_in
✅ Will update: transformer.blocks.0.mlp.W_out
✅ Will update: transformer.blocks.0.mlp.b_out
✅ Will update: transformer.blocks.1.attn.W_Q
✅ Will update: transformer.blocks.1.attn.W_K
✅ Will update: transformer.blocks.1.attn.W_V
✅ Will update: transformer.blocks.1.attn.W_O
✅ Will update: transformer.blocks.1.attn.b_Q
✅ Will update: transformer.blocks.1.attn.b_K
✅ Will update: transformer.blocks.1.attn.b_V
✅ Will update: transformer.blocks.1.attn.b_O
✅ Will update: transformer.blocks.1.mlp.W_in
✅ Will update: transformer.blocks.1.mlp.b_in
✅ Will update: transformer.blocks.1.mlp.W_out
✅ Will update: transformer.blocks.1.mlp.b_out
✅ Will update: action_predictor.weight
✅ Will update: action_predictor.bias
✅ Will update: state_predictor.weight
✅ Will update: state_predictor.bias
✅ Will update: reward_embedding.0.weight
✅ Will update: reward_predictor.weight
✅ Will update: reward_predictor.bias
✅ Will update: penultimate_layer.0.weight
✅ Will update: penultimate_layer.0.bias
✅ Will update: output_layer.0.weight
✅ Will update: output_layer.0.bias
===== 태스크별 데이터셋 크기 =====
Task task_0: 133 샘플
Task task_1: 165 샘플
Task task_2: 131 샘플
===== 원본 데이터셋 태스크별 분포 =====
전체 데이터 수: 429
Task 0: 133 샘플 (31.00%)
Task 1: 165 샘플 (38.46%)
Task 2: 131 샘플 (30.54%)
===== 학습/테스트 데이터 분할 (태스크 균형 유지) =====
학습 데이터: 299 샘플
테스트 데이터: 130 샘플
===== 첫 배치에서의 태스크 분포 확인 =====
Task 0: 42 샘플 (32.81%)
Task 1: 44 샘플 (34.38%)
Task 2: 42 샘플 (32.81%)
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
Training DT: 0.0001:   0%|          | 0/1 [00:01<?, ?it/s]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
Training DT: 0.0001:   0%|          | 0/1 [00:02<?, ?it/s]/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning:
[33mWARN: Overwriting existing videos at /home/hail/DT/src/videos/dt_eval_videos_0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)
Evaluating DT:   0%|          | 0/10 [00:00<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [42, 42, 42, 42, 42, 42, 42, 42] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:03<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [86, 86, 86, 86, 86, 86, 86, 86] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:05<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [130, 130, 130, 130, 130, 130, 130, 130] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:07<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [174, 174, 174, 174, 174, 174, 174, 174] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:09<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [200, 200, 200, 200, 200, 200, 200, 200] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:10<?, ?it/s]
Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [200, 200, 200, 200, 200, 200, 200, 200] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|█         | 1/10 [00:10<01:36, 10.73s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [40, 40, 40, 40, 40, 40, 40, 40] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|█         | 1/10 [00:13<01:36, 10.73s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [108, 108, 108, 108, 108, 108, 108, 108] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|█         | 1/10 [00:15<01:36, 10.73s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [176, 176, 176, 176, 176, 176, 176, 176] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|█         | 1/10 [00:17<01:36, 10.73s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
🔒 Freezing all layers except MLP (penultimate_layer, output_layer)
❌ action_embedding.0.weight is frozen.
❌ time_embedding.weight is frozen.
❌ state_embedding.weight is frozen.
❌ transformer.pos_embed.W_pos is frozen.
❌ transformer.blocks.0.attn.W_Q is frozen.
❌ transformer.blocks.0.attn.W_K is frozen.
❌ transformer.blocks.0.attn.W_V is frozen.
❌ transformer.blocks.0.attn.W_O is frozen.
❌ transformer.blocks.0.attn.b_Q is frozen.
❌ transformer.blocks.0.attn.b_K is frozen.
❌ transformer.blocks.0.attn.b_V is frozen.
❌ transformer.blocks.0.attn.b_O is frozen.
❌ transformer.blocks.0.mlp.W_in is frozen.
❌ transformer.blocks.0.mlp.b_in is frozen.
❌ transformer.blocks.0.mlp.W_out is frozen.
❌ transformer.blocks.0.mlp.b_out is frozen.
❌ transformer.blocks.1.attn.W_Q is frozen.
❌ transformer.blocks.1.attn.W_K is frozen.
❌ transformer.blocks.1.attn.W_V is frozen.
❌ transformer.blocks.1.attn.W_O is frozen.
❌ transformer.blocks.1.attn.b_Q is frozen.
❌ transformer.blocks.1.attn.b_K is frozen.
❌ transformer.blocks.1.attn.b_V is frozen.
❌ transformer.blocks.1.attn.b_O is frozen.
❌ transformer.blocks.1.mlp.W_in is frozen.
❌ transformer.blocks.1.mlp.b_in is frozen.
❌ transformer.blocks.1.mlp.W_out is frozen.
❌ transformer.blocks.1.mlp.b_out is frozen.
❌ action_predictor.weight is frozen.
❌ action_predictor.bias is frozen.
❌ state_predictor.weight is frozen.
❌ state_predictor.bias is frozen.
❌ reward_embedding.0.weight is frozen.
❌ reward_predictor.weight is frozen.
❌ reward_predictor.bias is frozen.
✅ penultimate_layer.0.weight will be updated.
✅ penultimate_layer.0.bias will be updated.
✅ output_layer.0.weight will be updated.
Training DT: 0.0001: 100%|██████████| 1/1 [00:20<00:00, 20.98s/it]t timestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|████████  | 8/10 [00:18<00:03,  1.98s/it]
MLP Fine-Tuning:   0%|          | 0/15 [00:00<?, ?it/s]
MLP Fine-Tuning:   0%|          | 0/15 [00:05<?, ?it/s]
Traceback (most recent call last):
  File "/snap/pycharm-community/471/plugins/python-ce/helpers/pydev/pydevd.py", line 1570, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/snap/pycharm-community/471/plugins/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/hail/DT/src/run_decision_transformer_local.py", line 67, in <module>
    run_decision_transformer(
  File "/home/hail/DT/src/decision_transformer/runner.py", line 121, in run_decision_transformer
    model = train(
  File "/home/hail/DT/src/decision_transformer/train.py", line 243, in train
    state_preds, action_preds, reward_preds, task_preds = model(
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hail/DT/src/models/trajectory_transformer.py", line 589, in forward
    penultimate_out = self.penultimate_layer(pooled)
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)

Class Weights: [1.08333333 0.86666667 1.08333333]
✅ Will update: action_embedding.0.weight
✅ Will update: time_embedding.weight
✅ Will update: state_embedding.weight
✅ Will update: transformer.pos_embed.W_pos
✅ Will update: transformer.blocks.0.attn.W_Q
✅ Will update: transformer.blocks.0.attn.W_K
✅ Will update: transformer.blocks.0.attn.W_V
✅ Will update: transformer.blocks.0.attn.W_O
✅ Will update: transformer.blocks.0.attn.b_Q
✅ Will update: transformer.blocks.0.attn.b_K
✅ Will update: transformer.blocks.0.attn.b_V
✅ Will update: transformer.blocks.0.attn.b_O
✅ Will update: transformer.blocks.0.mlp.W_in
✅ Will update: transformer.blocks.0.mlp.b_in
✅ Will update: transformer.blocks.0.mlp.W_out
✅ Will update: transformer.blocks.0.mlp.b_out
✅ Will update: transformer.blocks.1.attn.W_Q
✅ Will update: transformer.blocks.1.attn.W_K
✅ Will update: transformer.blocks.1.attn.W_V
✅ Will update: transformer.blocks.1.attn.W_O
✅ Will update: transformer.blocks.1.attn.b_Q
✅ Will update: transformer.blocks.1.attn.b_K
✅ Will update: transformer.blocks.1.attn.b_V
✅ Will update: transformer.blocks.1.attn.b_O
✅ Will update: transformer.blocks.1.mlp.W_in
✅ Will update: transformer.blocks.1.mlp.b_in
✅ Will update: transformer.blocks.1.mlp.W_out
✅ Will update: transformer.blocks.1.mlp.b_out
✅ Will update: action_predictor.weight
✅ Will update: action_predictor.bias
✅ Will update: state_predictor.weight
✅ Will update: state_predictor.bias
✅ Will update: reward_embedding.0.weight
✅ Will update: reward_predictor.weight
✅ Will update: reward_predictor.bias
✅ Will update: penultimate_layer.0.weight
✅ Will update: penultimate_layer.0.bias
✅ Will update: penultimate_layer.1.weight
✅ Will update: penultimate_layer.1.bias
✅ Will update: penultimate_layer.3.weight
✅ Will update: penultimate_layer.3.bias
✅ Will update: penultimate_layer.5.weight
✅ Will update: penultimate_layer.5.bias
✅ Will update: penultimate_layer.6.weight
✅ Will update: penultimate_layer.6.bias
✅ Will update: penultimate_layer.8.weight
✅ Will update: penultimate_layer.8.bias
✅ Will update: output_layer.0.weight
✅ Will update: output_layer.0.bias
===== 태스크별 데이터셋 크기 =====
Task task_0: 133 샘플
Task task_1: 165 샘플
Task task_2: 131 샘플
===== 원본 데이터셋 태스크별 분포 =====
전체 데이터 수: 429
Task 0: 133 샘플 (31.00%)
Task 1: 165 샘플 (38.46%)
Task 2: 131 샘플 (30.54%)
===== 학습/테스트 데이터 분할 (태스크 균형 유지) =====
학습 데이터: 299 샘플
테스트 데이터: 130 샘플
===== 첫 배치에서의 태스크 분포 확인 =====
Task 0: 35 샘플 (27.34%)
Task 1: 48 샘플 (37.50%)
Task 2: 45 샘플 (35.16%)
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
Training DT: 0.0002:   0%|          | 0/1 [00:01<?, ?it/s]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
Training DT: 0.0002:   0%|          | 0/1 [00:02<?, ?it/s]/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning:
[33mWARN: Overwriting existing videos at /home/hail/DT/src/videos/dt_eval_videos_0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)
Evaluating DT:   0%|          | 0/10 [00:00<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [45, 45, 45, 45, 45, 45, 45, 45] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:03<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [90, 90, 90, 90, 90, 90, 90, 90] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:05<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [138, 138, 138, 138, 138, 138, 138, 138] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:07<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [184, 184, 184, 184, 184, 184, 184, 184] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:09<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [200, 200, 200, 200, 200, 200, 200, 200] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:10<?, ?it/s]
Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [9, 9, 9, 9, 9, 9, 9, 9] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|█         | 1/10 [00:11<01:31, 10.15s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [68, 68, 68, 68, 68, 68, 68, 68] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|█         | 1/10 [00:13<01:31, 10.15s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False


Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [188, 188, 188, 188, 188, 188, 188, 188] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|█         | 1/10 [00:17<01:31, 10.15s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
Training DT: 0.0002: 100%|██████████| 1/1 [00:19<00:00, 19.97s/it]t timestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|████████  | 8/10 [00:17<00:03,  1.87s/it]
MLP Fine-Tuning:   0%|          | 0/15 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/snap/pycharm-community/471/plugins/python-ce/helpers/pydev/pydevd.py", line 1570, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/snap/pycharm-community/471/plugins/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/hail/DT/src/run_decision_transformer_local.py", line 67, in <module>
    run_decision_transformer(
  File "/home/hail/DT/src/decision_transformer/runner.py", line 121, in run_decision_transformer
    model = train(
  File "/home/hail/DT/src/decision_transformer/train.py", line 243, in train
    state_preds, action_preds, reward_preds, task_preds = model(
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hail/DT/src/models/trajectory_transformer.py", line 576, in forward
    penultimate_out = self.penultimate_layer(pooled)
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 190, in forward
    return F.layer_norm(
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper__native_layer_norm)
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
🔒 Freezing all layers except MLP (penultimate_layer, output_layer)
❌ action_embedding.0.weight is frozen.
❌ time_embedding.weight is frozen.
❌ state_embedding.weight is frozen.
❌ transformer.pos_embed.W_pos is frozen.
❌ transformer.blocks.0.attn.W_Q is frozen.
❌ transformer.blocks.0.attn.W_K is frozen.
❌ transformer.blocks.0.attn.W_V is frozen.
❌ transformer.blocks.0.attn.W_O is frozen.
❌ transformer.blocks.0.attn.b_Q is frozen.
❌ transformer.blocks.0.attn.b_K is frozen.
❌ transformer.blocks.0.attn.b_V is frozen.
❌ transformer.blocks.0.attn.b_O is frozen.
❌ transformer.blocks.0.mlp.W_in is frozen.
❌ transformer.blocks.0.mlp.b_in is frozen.
❌ transformer.blocks.0.mlp.W_out is frozen.
❌ transformer.blocks.0.mlp.b_out is frozen.
❌ transformer.blocks.1.attn.W_Q is frozen.
❌ transformer.blocks.1.attn.W_K is frozen.
❌ transformer.blocks.1.attn.W_V is frozen.
❌ transformer.blocks.1.attn.W_O is frozen.
❌ transformer.blocks.1.attn.b_Q is frozen.
❌ transformer.blocks.1.attn.b_K is frozen.
❌ transformer.blocks.1.attn.b_V is frozen.
❌ transformer.blocks.1.attn.b_O is frozen.
❌ transformer.blocks.1.mlp.W_in is frozen.
❌ transformer.blocks.1.mlp.b_in is frozen.
❌ transformer.blocks.1.mlp.W_out is frozen.
❌ transformer.blocks.1.mlp.b_out is frozen.
❌ action_predictor.weight is frozen.
❌ action_predictor.bias is frozen.
❌ state_predictor.weight is frozen.
❌ state_predictor.bias is frozen.
❌ reward_embedding.0.weight is frozen.
❌ reward_predictor.weight is frozen.
❌ reward_predictor.bias is frozen.
✅ penultimate_layer.0.weight will be updated.
✅ penultimate_layer.0.bias will be updated.
✅ penultimate_layer.1.weight will be updated.
✅ penultimate_layer.1.bias will be updated.
✅ penultimate_layer.3.weight will be updated.
✅ penultimate_layer.3.bias will be updated.
✅ penultimate_layer.5.weight will be updated.
✅ penultimate_layer.5.bias will be updated.
✅ penultimate_layer.6.weight will be updated.
✅ penultimate_layer.6.bias will be updated.
✅ penultimate_layer.8.weight will be updated.
✅ penultimate_layer.8.bias will be updated.
✅ output_layer.0.weight will be updated.
✅ output_layer.0.bias will be updated.

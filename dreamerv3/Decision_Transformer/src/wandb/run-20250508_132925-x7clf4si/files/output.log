Class Weights: [1.08333333 0.86666667 1.08333333]
âœ… Will update: action_embedding.0.weight
âœ… Will update: time_embedding.weight
âœ… Will update: state_embedding.weight
âœ… Will update: transformer.pos_embed.W_pos
âœ… Will update: transformer.blocks.0.attn.W_Q
âœ… Will update: transformer.blocks.0.attn.W_K
âœ… Will update: transformer.blocks.0.attn.W_V
âœ… Will update: transformer.blocks.0.attn.W_O
âœ… Will update: transformer.blocks.0.attn.b_Q
âœ… Will update: transformer.blocks.0.attn.b_K
âœ… Will update: transformer.blocks.0.attn.b_V
âœ… Will update: transformer.blocks.0.attn.b_O
âœ… Will update: transformer.blocks.0.mlp.W_in
âœ… Will update: transformer.blocks.0.mlp.b_in
âœ… Will update: transformer.blocks.0.mlp.W_out
âœ… Will update: transformer.blocks.0.mlp.b_out
âœ… Will update: transformer.blocks.1.attn.W_Q
âœ… Will update: transformer.blocks.1.attn.W_K
âœ… Will update: transformer.blocks.1.attn.W_V
âœ… Will update: transformer.blocks.1.attn.W_O
âœ… Will update: transformer.blocks.1.attn.b_Q
âœ… Will update: transformer.blocks.1.attn.b_K
âœ… Will update: transformer.blocks.1.attn.b_V
âœ… Will update: transformer.blocks.1.attn.b_O
âœ… Will update: transformer.blocks.1.mlp.W_in
âœ… Will update: transformer.blocks.1.mlp.b_in
âœ… Will update: transformer.blocks.1.mlp.W_out
âœ… Will update: transformer.blocks.1.mlp.b_out
âœ… Will update: action_predictor.weight
âœ… Will update: action_predictor.bias
âœ… Will update: state_predictor.weight
âœ… Will update: state_predictor.bias
âœ… Will update: reward_embedding.0.weight
âœ… Will update: reward_predictor.weight
âœ… Will update: reward_predictor.bias
âœ… Will update: penultimate_layer.0.weight
âœ… Will update: penultimate_layer.0.bias
âœ… Will update: penultimate_layer.1.weight
âœ… Will update: penultimate_layer.1.bias
âœ… Will update: penultimate_layer.3.weight
âœ… Will update: penultimate_layer.3.bias
âœ… Will update: penultimate_layer.5.weight
âœ… Will update: penultimate_layer.5.bias
âœ… Will update: penultimate_layer.6.weight
âœ… Will update: penultimate_layer.6.bias
âœ… Will update: penultimate_layer.8.weight
âœ… Will update: penultimate_layer.8.bias
âœ… Will update: output_layer.0.weight
âœ… Will update: output_layer.0.bias
===== íƒœìŠ¤í¬ë³„ ë°ì´í„°ì…‹ í¬ê¸° =====
Task task_0: 133 ìƒ˜í”Œ
Task task_1: 165 ìƒ˜í”Œ
Task task_2: 131 ìƒ˜í”Œ
===== ì›ë³¸ ë°ì´í„°ì…‹ íƒœìŠ¤í¬ë³„ ë¶„í¬ =====
ì „ì²´ ë°ì´í„° ìˆ˜: 429
Task 0: 133 ìƒ˜í”Œ (31.00%)
Task 1: 165 ìƒ˜í”Œ (38.46%)
Task 2: 131 ìƒ˜í”Œ (30.54%)
===== í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (íƒœìŠ¤í¬ ê· í˜• ìœ ì§€) =====
í•™ìŠµ ë°ì´í„°: 299 ìƒ˜í”Œ
í…ŒìŠ¤íŠ¸ ë°ì´í„°: 130 ìƒ˜í”Œ
===== ì²« ë°°ì¹˜ì—ì„œì˜ íƒœìŠ¤í¬ ë¶„í¬ í™•ì¸ =====
Task 0: 35 ìƒ˜í”Œ (27.34%)
Task 1: 48 ìƒ˜í”Œ (37.50%)
Task 2: 45 ìƒ˜í”Œ (35.16%)
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
Training DT: 0.0002:   0%|          | 0/1 [00:01<?, ?it/s]
[correct] no_actions is False
r.shape: torch.Size([128, 100, 1])
reward_preds.shape: torch.Size([12800, 1])
s[1:].shape (GT): torch.Size([128, 100, 7, 7, 20])
state_preds.shape (pred): torch.Size([128, 101, 980])
r[1:].shape (GT): torch.Size([128, 99, 1])
reward_preds.shape (pred): torch.Size([12800, 1])
Training DT: 0.0002:   0%|          | 0/1 [00:02<?, ?it/s]/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning:
[33mWARN: Overwriting existing videos at /home/hail/DT/src/videos/dt_eval_videos_0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)
Evaluating DT:   0%|          | 0/10 [00:00<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [45, 45, 45, 45, 45, 45, 45, 45] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:03<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [90, 90, 90, 90, 90, 90, 90, 90] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:05<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [138, 138, 138, 138, 138, 138, 138, 138] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:07<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 0 episodes.Current episodes are at timestep [184, 184, 184, 184, 184, 184, 184, 184] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:09<?, ?it/s]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [200, 200, 200, 200, 200, 200, 200, 200] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:   0%|          | 0/10 [00:10<?, ?it/s]
Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [9, 9, 9, 9, 9, 9, 9, 9] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|â–ˆ         | 1/10 [00:11<01:31, 10.15s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False

Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [68, 68, 68, 68, 68, 68, 68, 68] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|â–ˆ         | 1/10 [00:13<01:31, 10.15s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False


Evaluating DT: Finished running 8 episodes.Current episodes are at timestep [188, 188, 188, 188, 188, 188, 188, 188] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  10%|â–ˆ         | 1/10 [00:17<01:31, 10.15s/it]
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
Training DT: 0.0002: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.97s/it]t timestep [201, 201, 201, 201, 201, 201, 201, 201] for reward [0. 0. 0. 0. 0. 0. 0. 0.]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.87s/it]
MLP Fine-Tuning:   0%|          | 0/15 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/snap/pycharm-community/471/plugins/python-ce/helpers/pydev/pydevd.py", line 1570, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "/snap/pycharm-community/471/plugins/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "/home/hail/DT/src/run_decision_transformer_local.py", line 67, in <module>
    run_decision_transformer(
  File "/home/hail/DT/src/decision_transformer/runner.py", line 121, in run_decision_transformer
    model = train(
  File "/home/hail/DT/src/decision_transformer/train.py", line 243, in train
    state_preds, action_preds, reward_preds, task_preds = model(
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hail/DT/src/models/trajectory_transformer.py", line 576, in forward
    penultimate_out = self.penultimate_layer(pooled)
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 190, in forward
    return F.layer_norm(
  File "/home/hail/anaconda3/envs/dt/lib/python3.9/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper__native_layer_norm)
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
[correct] no_actions is False
ğŸ”’ Freezing all layers except MLP (penultimate_layer, output_layer)
âŒ action_embedding.0.weight is frozen.
âŒ time_embedding.weight is frozen.
âŒ state_embedding.weight is frozen.
âŒ transformer.pos_embed.W_pos is frozen.
âŒ transformer.blocks.0.attn.W_Q is frozen.
âŒ transformer.blocks.0.attn.W_K is frozen.
âŒ transformer.blocks.0.attn.W_V is frozen.
âŒ transformer.blocks.0.attn.W_O is frozen.
âŒ transformer.blocks.0.attn.b_Q is frozen.
âŒ transformer.blocks.0.attn.b_K is frozen.
âŒ transformer.blocks.0.attn.b_V is frozen.
âŒ transformer.blocks.0.attn.b_O is frozen.
âŒ transformer.blocks.0.mlp.W_in is frozen.
âŒ transformer.blocks.0.mlp.b_in is frozen.
âŒ transformer.blocks.0.mlp.W_out is frozen.
âŒ transformer.blocks.0.mlp.b_out is frozen.
âŒ transformer.blocks.1.attn.W_Q is frozen.
âŒ transformer.blocks.1.attn.W_K is frozen.
âŒ transformer.blocks.1.attn.W_V is frozen.
âŒ transformer.blocks.1.attn.W_O is frozen.
âŒ transformer.blocks.1.attn.b_Q is frozen.
âŒ transformer.blocks.1.attn.b_K is frozen.
âŒ transformer.blocks.1.attn.b_V is frozen.
âŒ transformer.blocks.1.attn.b_O is frozen.
âŒ transformer.blocks.1.mlp.W_in is frozen.
âŒ transformer.blocks.1.mlp.b_in is frozen.
âŒ transformer.blocks.1.mlp.W_out is frozen.
âŒ transformer.blocks.1.mlp.b_out is frozen.
âŒ action_predictor.weight is frozen.
âŒ action_predictor.bias is frozen.
âŒ state_predictor.weight is frozen.
âŒ state_predictor.bias is frozen.
âŒ reward_embedding.0.weight is frozen.
âŒ reward_predictor.weight is frozen.
âŒ reward_predictor.bias is frozen.
âœ… penultimate_layer.0.weight will be updated.
âœ… penultimate_layer.0.bias will be updated.
âœ… penultimate_layer.1.weight will be updated.
âœ… penultimate_layer.1.bias will be updated.
âœ… penultimate_layer.3.weight will be updated.
âœ… penultimate_layer.3.bias will be updated.
âœ… penultimate_layer.5.weight will be updated.
âœ… penultimate_layer.5.bias will be updated.
âœ… penultimate_layer.6.weight will be updated.
âœ… penultimate_layer.6.bias will be updated.
âœ… penultimate_layer.8.weight will be updated.
âœ… penultimate_layer.8.bias will be updated.
âœ… output_layer.0.weight will be updated.
âœ… output_layer.0.bias will be updated.
